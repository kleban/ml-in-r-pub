[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Математичне моделювання в R",
    "section": "",
    "text": "Матеріали підготовлені для читання курсу “Вступ до прикладного математичного моделювання в R” [08.435] для студентів 3-го курсу, спеціальності економічна кібернетика.\n\n\nНавчальна дисципліна спрямована на вивчення основ прикладного математичного моделювання з використаням різних видів моделей та підходів до їх побудови, а також удосокналенню процесів роботи з даними та моделями на основі мови R.\nМісце навчальної дисципліни у підготовці здобувачів: програмні результати дисципліни використовуються під час вивчення таких навчальних дисциплін: “Математичні методи та моделі в аналізі великих даних”, “Економічна кібернетика”, “Дослідження операцій”, “Інтелектуальні технології моделювання у прийнятті рішень”. Закріплення на практиці здобутих програмних результатів відбувається під час проходження Навчальної практики з курсу «Економіко-математичне моделювання».\n\n\n\nМета навчальної дисципліни – формування у студентів теоретичних знань та практичних навичок використання мови програмування R для побудови, навчання та оцінки якості математичних моделей на основі регресії, класифікації, кластеризації та асоціативних правил.\n\n\n\nВикладач та слухач цього курсу, як очікується, повинні дотримуватися Кодексу академічної доброчесності університету:\n\nбудь-яка робота, подана здобувачем протягом курсу, має бути його власною роботою здобувача; не вдаватися до кроків, що можуть нечесно покращити Ваші результати чи погіршити/покращити результати інших здобувачів;\nякщо буде виявлено ознаки плагіату або іншої недобросовісної академічної поведінки, то студент буде позбавлений можливості отримати передбачені бали за завдання;\nне публікувати у відкритому доступі відповіді на запитання, що використовуються в рамках курсу для оцінювання знань здобувачів;\nпід час фінальних видів контролю необхідно працювати самостійно; не дозволяється говорити або обговорювати, а також не можна копіювати документи, використовувати електронні засоби отримання інформації.\n\nПорушення академічної доброчесності під час виконання контрольних завдань призведе до втрати балів або вживання заходів, які передбачені Кодексу академічної доброчесності НаУОА.\n\n\n\n\n\n\n\nМатеріали курсу створені з використанням ряду технологій:\n\nR Language - безкоштована мова програмування для виконання досліджень у сфері статистики, машинного навчання та візуалізацї результатів.\nQuarto Book - система для публікації наукових та технічних текстів з відкритим кодом (R/Python/Julia/Observable).\nJupyterLab - середовище розробки на основі Jupyter Notebook. JupyterLab є розширеним веб-інтерфейсом для роботи з ноутбуками.\nGit/Github - система контролю версій та, відповідно, сервіс для організації зберігання коду, а також публікації статичних сторінок."
  },
  {
    "objectID": "010-intro.html",
    "href": "010-intro.html",
    "title": "ТЕМА 1. ВСТУП ДО КУРСУ",
    "section": "",
    "text": "Мета заняття:\nРозглянути основні поняття, що стосуються математичного моделювання та базові категорії машинного навчання.\n\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "020-feature-engeneering.html",
    "href": "020-feature-engeneering.html",
    "title": "ТЕМА 2. КОНСТРУЮВАННЯ ОЗНАК",
    "section": "",
    "text": "У даній темі розкрито основні принципи інженерії фіч та методів трансформації даних для підвищення якості, швидкості побудови математичних моделей.\n\n\n<p>Це вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>.</p>"
  },
  {
    "objectID": "023-feature-engeneering-woe-binning.html",
    "href": "023-feature-engeneering-woe-binning.html",
    "title": "1  Woe Binning",
    "section": "",
    "text": "Курс: “Матетичне моделювання в R”\n\n\n# install.packages(\"openxlsx\")\n\n\nSys.setlocale(\"LC_CTYPE\", \"ukrainian\")\noptions(warn = -1)\n\n'Ukrainian_Ukraine.1251'\n\n\n\n\nДжерело: https://github.com/gastonstat/CreditScoring/blob/master/CleanCreditScoring.csv\nЗавантажимо дані:\n\nlibrary(openxlsx)\ndata <- openxlsx::read.xlsx(\"data/CreditScoring.xlsx\", sheet = 1, startRow = 1, colNames = TRUE, rowNames = FALSE)\nstr(data)\n\n'data.frame':   4446 obs. of  16 variables:\n $ Status   : chr  \"good\" \"good\" \"bad\" \"good\" ...\n $ Seniority: num  9 17 10 0 0 1 29 9 0 0 ...\n $ Home     : chr  \"rent\" \"rent\" \"owner\" \"rent\" ...\n $ Time     : num  60 60 36 60 36 60 60 12 60 48 ...\n $ Age      : num  30 58 46 24 26 36 44 27 32 41 ...\n $ Marital  : chr  \"married\" \"widow\" \"married\" \"single\" ...\n $ Records  : chr  \"no_rec\" \"no_rec\" \"yes_rec\" \"no_rec\" ...\n $ Job      : chr  \"freelance\" \"fixed\" \"freelance\" \"fixed\" ...\n $ Expenses : num  73 48 90 63 46 75 75 35 90 90 ...\n $ Income   : num  129 131 200 182 107 214 125 80 107 80 ...\n $ Assets   : num  0 0 3000 2500 0 3500 10000 0 15000 0 ...\n $ Debt     : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Amount   : num  800 1000 2000 900 310 650 1600 200 1200 1200 ...\n $ Price    : num  846 1658 2985 1325 910 ...\n $ Finrat   : num  94.6 60.3 67 67.9 34.1 ...\n $ Savings  : num  4.2 4.98 1.98 7.93 7.08 ...\n\n\nОпишемо дані:\n\nStatus - credit status (Target)\nSeniority job seniority (years)\nHome type of home ownership\nTime time of requested loan\nAge client’s age\nMarital marital status\nRecords existance of records\nJob type of job\nExpenses amount of expenses\nIncome amount of income\nAssets amount of assets\nDebt amount of debt\nAmount amount requested of loan\nPrice price of good\n\n\n\n\n\n\n\nСтворимо дата-фрейм для зберігання інформації про групи змінної Home (як приклад біннігу категоріального показника):\n\nhome_groups <- data.frame(Group = unique(data$Home), \n                          Good = c(0), Bad = c(0), \n                          GoodP = c(0), BadP = c(0),\n                          WOE = c(0), IV = c(0))\nhome_groups\n\n\n\nA data.frame: 6 × 7\n\n    GroupGoodBadGoodPBadPWOEIV\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    rent   000000\n    owner  000000\n    parents000000\n    priv   000000\n    other  000000\n    ignore 000000\n\n\n\n\nПереглянемо можливі варіанти показника Status, який є залежною бінарною змінною поточної задачі:\n\nlibrary(gmodels)\nCrossTable(data$Status)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  4446 \n\n \n          |       bad |      good | \n          |-----------|-----------|\n          |      1249 |      3197 | \n          |     0.281 |     0.719 | \n          |-----------|-----------|\n\n\n\n \n\n\nСформуємо групи та обчислимо значення по кожній групі:\n\nfor(i in 1:nrow(home_groups)) {\n  \n  group <- home_groups$Group[i]\n  \n  home_groups$Good[i] <- nrow(data[data$Home == group & data$Status == \"good\", ])\n  home_groups$Bad[i] <- nrow(data[data$Home == group & data$Status == \"bad\", ])\n  \n  home_groups$GoodP[i] <- home_groups$Good[i]/nrow(data[data$Status == \"good\", ])\n  home_groups$BadP[i] <- home_groups$Bad[i]/nrow(data[data$Status == \"bad\", ])\n  \n  home_groups$WOE[i] <- log( home_groups$GoodP[i] / home_groups$BadP[i])\n  home_groups$IV[i] <- (home_groups$GoodP[i] - home_groups$BadP[i])*home_groups$WOE[i]\n}\n\nhome_groups\n\n\n\nA data.frame: 6 × 7\n\n    GroupGoodBadGoodPBadPWOEIV\n    <chr><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    rent    5853880.1829840480.310648519-0.529263130.067568098\n    owner  17163900.5367532060.312249800 0.541734900.121621331\n    parents 5502320.1720362840.185748599-0.076688730.001051580\n    priv    162 840.0506725050.067253803-0.283090100.004694001\n    other   1731460.0541132310.116893515-0.770184670.048352412\n    ignore   11  90.0034407260.007205765-0.739198940.002783113\n\n\n\n\nПереглянемо сумарний IV:\n\nhome_iv <- sum(home_groups$IV)\nhome_iv\n\n0.246070534510342\n\n\nВізуалізуємо групи:\n\nbarplot(home_groups$WOE, \n        col=\"brown\", \n        names.arg=c(as.character(home_groups$Group)), \n        xlab=\"Group\",\n        ylab=\"WOE\"\n)\n\n\n\n\nСтворимо датафрейм для нових WOE-даних:\n\nnew_df <- data.frame(Status = data$Status, Home = data$Home, HomeWoe = c(0))\n\nЗамінимо значення на WOE:\n\nfor(i in 1:nrow(home_groups)) {\n  group <- home_groups$Group[i]\n  woe <- home_groups$WOE[i]  \n  new_df[new_df$Home == group, ]$HomeWoe <- woe\n}\n\ntail(new_df)\n\n\n\nA data.frame: 6 × 3\n\n    StatusHomeHomeWoe\n    <chr><chr><dbl>\n\n\n    4441bad other-0.7701847\n    4442bad rent -0.5292631\n    4443goodowner 0.5417349\n    4444bad owner 0.5417349\n    4445goodrent -0.5292631\n    4446goodowner 0.5417349\n\n\n\n\n\n\n\n\nОбрахуємо приклад числовиго показника (на прикладі Age):\n\nmin_age <- min(data$Age)\nmax_age <- max(data$Age)\n\nstep <- round(max_age - min_age)/10\n\n\nage_groups <- data.frame(Min = seq(min_age, max_age-step, step), \n                         Max = seq(min_age + step, max_age, step), \n                          Good = c(0), Bad = c(0), \n                          GoodP = c(0), BadP = c(0),\n                          WOE = c(0), IV = c(0))\n\nage_groups\n\n\n\nA data.frame: 10 × 8\n\n    MinMaxGoodBadGoodPBadPWOEIV\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1823000000\n    2328000000\n    2833000000\n    3338000000\n    3843000000\n    4348000000\n    4853000000\n    5358000000\n    5863000000\n    6368000000\n\n\n\n\nСформуємо групи:\n\nfor(i in 1:nrow(age_groups)) {\n  \n  min <- age_groups$Min[i]\n  max<- age_groups$Max[i]\n  \n  age_groups$Good[i] <- nrow(data[data$Age >= min & data$Age < max & data$Status == \"good\", ])\n  age_groups$Bad[i] <- nrow(data[data$Age >= min & data$Age < max & data$Status == \"bad\", ])\n  \n  if(i == nrow(age_groups)) {\n    age_groups$Good[i] <- age_groups$Good[i] + nrow(data[data$Age == max & data$Status == \"good\", ])\n    age_groups$Bad[i] <- age_groups$Bad[i] + nrow(data[data$Age == max & data$Status == \"bad\", ])\n  }\n  \n  age_groups$GoodP[i] <- age_groups$Good[i]/nrow(data[data$Status == \"good\", ])\n  age_groups$BadP[i] <- age_groups$Bad[i]/nrow(data[data$Status == \"bad\", ])\n  \n  age_groups$WOE[i] <- log( age_groups$GoodP[i] / age_groups$BadP[i])\n  \n  age_groups$IV[i] <- (age_groups$GoodP[i] - age_groups$BadP[i]) * age_groups$WOE[i]\n}\n\nage_groups\n\n\n\nA data.frame: 10 × 8\n\n    MinMaxGoodBadGoodPBadPWOEIV\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    18231691100.052862060.088070456-0.510451290.0179721723\n    23285052310.157960590.184947958-0.157728920.0042566888\n    28335642130.176415390.170536429 0.033892450.0001992523\n    33384912120.153581480.169735789-0.100011790.0016156210\n    38434431830.138567410.146517214-0.055786020.0004434881\n    43483201230.100093840.098478783 0.016267000.0000262721\n    4853317 790.099155460.063250600 0.449584280.0161422597\n    5358214 580.066937750.046437150 0.365663370.0074963200\n    5863124 310.038786360.024819856 0.446424720.0062349937\n    6368 50  90.015639660.007205765 0.774928790.0065356700\n\n\n\n\nСумарний IV:\n\nage_iv <- sum(age_groups$IV)\nage_iv\n\n0.0609227380464588\n\n\nВізуалізуємо значення груп WOE:\n\nbarplot(age_groups$WOE, \n        col=\"brown\", \n        names.arg=c(age_groups$Min), \n        xlab=\"Min Age\",\n        ylab=\"WOE\"\n)\n\n\n\n\nЗамінимо значення на WOE:\n\nnew_df$Age <- data$Age\nnew_df$AgeWoe <- c(0)\n\nfor(i in 1:nrow(age_groups)) {\n  \n  min <- age_groups$Min[i]\n  max <- age_groups$Max[i]\n  woe <- age_groups$WOE[i]\n  \n  new_df[new_df$Age >= min & new_df$Age < max, ]$AgeWoe <- woe\n  \n  if(i == nrow(age_groups)) {\n    new_df$AgeWoe[i] <- woe\n  }\n  \n}\n\nhead(new_df)\n\n\n\nA data.frame: 6 × 5\n\n    StatusHomeHomeWoeAgeAgeWoe\n    <chr><chr><dbl><dbl><dbl>\n\n\n    1goodrent -0.529263130 0.03389245\n    2goodrent -0.529263158 0.44642472\n    3bad owner 0.541734946 0.01626700\n    4goodrent -0.529263124-0.15772892\n    5goodrent -0.529263126-0.15772892\n    6goodowner 0.541734936-0.10001179\n\n\n\n\nВидалимо оригінальні значення з набору даних:\n\nnew_df$Home <- NULL\nnew_df$Age <- NULL\nhead(new_df)\n\n\n\nA data.frame: 6 × 3\n\n    StatusHomeWoeAgeWoe\n    <chr><dbl><dbl>\n\n\n    1good-0.5292631 0.03389245\n    2good-0.5292631 0.44642472\n    3bad  0.5417349 0.01626700\n    4good-0.5292631-0.15772892\n    5good-0.5292631-0.15772892\n    6good 0.5417349-0.10001179"
  },
  {
    "objectID": "030-supervised-learning.html",
    "href": "030-supervised-learning.html",
    "title": "ТЕМА 3. НАВЧАННЯ “З УЧИТЕЛЕМ”",
    "section": "",
    "text": "У даній темі розкрито основні принципи побудови та використання регресійних моделей, а також моделей для розв’язання задачк ласифікації.\nПід час вивчення матеріалів розглядаються алгоритми на основі лінійної та логістичної регресії, нейронних мереж, дерев рішень та градієнтного бустінгу.\nДля практичного закріплення знань використовують можливості бібліотек R: caret, neuralnet, gmodels, rpart та інші.\n\n\nЦе вбудований документ <a target=\"_blank\" href=\"https://office.com\">Microsoft Office</a> на платформі <a target=\"_blank\" href=\"https://office.com/webapps\">Office</a>."
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html",
    "href": "031-supervised-learning-linear-regression-1.html",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "",
    "text": "Курс: Математичне моделювання в R"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#dataset-overview",
    "href": "031-supervised-learning-linear-regression-1.html#dataset-overview",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.1 Dataset overview",
    "text": "2.1 Dataset overview\nУ даному навчальному матеріалі використано класичний приклад даних з інформацією про престижність професій у Канаді 1971 року. Джерело: carData::Prestige.\n\nlibrary(carData)\ndata <- carData::Prestige\nhead(data)\n\n\n\nA data.frame: 6 × 6\n\n    educationincomewomenprestigecensustype\n    <dbl><int><dbl><dbl><int><fct>\n\n\n    gov.administrators13.111235111.1668.81113prof\n    general.managers12.2625879 4.0269.11130prof\n    accountants12.77 927115.7063.41171prof\n    purchasing.officers11.42 8865 9.1156.81175prof\n    chemists14.62 840311.6873.52111prof\n    physicists15.6411030 5.1377.62113prof\n\n\n\n\nПереглянемо структуру даних:\n\nstr(data)\n\n'data.frame':   102 obs. of  6 variables:\n $ education: num  13.1 12.3 12.8 11.4 14.6 ...\n $ income   : int  12351 25879 9271 8865 8403 11030 8258 14163 11377 11023 ...\n $ women    : num  11.16 4.02 15.7 9.11 11.68 ...\n $ prestige : num  68.8 69.1 63.4 56.8 73.5 77.6 72.6 78.1 73.1 68.8 ...\n $ census   : int  1113 1130 1171 1175 2111 2113 2133 2141 2143 2153 ...\n $ type     : Factor w/ 3 levels \"bc\",\"prof\",\"wc\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nЗначення показників вибірки:\n\nprestige – престиж професії за Pineo-Porter score, на основі дослідження середини 1960-х. It is target!!!\neducation - середня кількість років освіти.\nincome – середній дохід респондентів, дол.\nwomen – частка жінок у галузі\ncensus – канадський код професії.\ntype – тип професії: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar.\n\nОглянемо описову статистику факторів:\n\nsummary(data)\n\n   education          income          women           prestige    \n Min.   : 6.380   Min.   :  611   Min.   : 0.000   Min.   :14.80  \n 1st Qu.: 8.445   1st Qu.: 4106   1st Qu.: 3.592   1st Qu.:35.23  \n Median :10.540   Median : 5930   Median :13.600   Median :43.60  \n Mean   :10.738   Mean   : 6798   Mean   :28.979   Mean   :46.83  \n 3rd Qu.:12.648   3rd Qu.: 8187   3rd Qu.:52.203   3rd Qu.:59.27  \n Max.   :15.970   Max.   :25879   Max.   :97.510   Max.   :87.20  \n     census       type   \n Min.   :1113   bc  :44  \n 1st Qu.:3120   prof:31  \n Median :5135   wc  :23  \n Mean   :5402   NA's: 4  \n 3rd Qu.:8312            \n Max.   :9517"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#traintest-split",
    "href": "031-supervised-learning-linear-regression-1.html#traintest-split",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.2 Train/Test split",
    "text": "2.2 Train/Test split\nРозділимо загальну вибірку на 2 частини: * тренувальна, 70% вибірки, для побудови регресії; * тестова, 30% вибірки, для перевірки точності моделі.\n\nset.seed(10) #довільне число як точка \"відправки\" для генератора випадкових чисел\n\n# Згенеруємо набір чисел від 1 до кількості спостережень у вибірці і відберемо випадквоим чином 70% із них\ntrain_index <- sample(nrow(data), size = 0.7*nrow(data))\n\n#Виведемо індекси (номери рядків) обраних для тренування даних\nprint(train_index)\n\n [1]   9  74  76  55  72  54  39  83  88  15  93  42  71 101  34  24  13   8   7\n[20]  27  82  29  81  50  26  33  84  78  79  30  68  51  97  59  32  11  77  91\n[39]  28  95  65  14  86  66  41  25  85  16  53  75  57  17  48  23  92  46  87\n[58]  94   4  35  61  69  43  10  96  99  89  31  38  52  18\n\n\n\n#Запишемо по номерах відібраних рядків тренувальний набір даних\ntrain_data <- data[train_index, ]\nhead(train_data)\n\n#Всі інші значення, що не увійшли в тренувальну вибірку запишемо у тестову\ntest_data <- data[-train_index, ]\nhead(test_data)\n\n\n\nA data.frame: 6 × 6\n\n    educationincomewomenprestigecensustype\n    <dbl><int><dbl><dbl><int><fct>\n\n\n    civil.engineers14.5211377 1.0373.12143prof\n    textile.weavers 6.69 444331.3633.38267bc  \n    tool.die.makers10.09 8043 1.5042.58311bc  \n    insurance.agents11.60 813113.0947.35171wc  \n    slaughterers.2 7.64 513417.2634.88215bc  \n    service.station.attendant 9.93 2370 3.6923.35145bc  \n\n\n\n\n\n\nA data.frame: 6 × 6\n\n    educationincomewomenprestigecensustype\n    <dbl><int><dbl><dbl><int><fct>\n\n\n    gov.administrators13.111235111.1668.81113prof\n    general.managers12.2625879 4.0269.11130prof\n    accountants12.77 927115.7063.41171prof\n    chemists14.62 840311.6873.52111prof\n    physicists15.6411030 5.1377.62113prof\n    draughtsmen12.30 7059 7.8360.02163prof"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#short-eda-exploratory-data-analysis",
    "href": "031-supervised-learning-linear-regression-1.html#short-eda-exploratory-data-analysis",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.3 Short EDA (Exploratory data analysis)",
    "text": "2.3 Short EDA (Exploratory data analysis)\n\n2.3.1 Correlation\nПереглянемо наявність зв’язків між параметрами за допоомгою матриці попарних кореляцій. Дані на перетині рядків вказують на рівень кореляції між вибраними показниками.\nВиключимо змінну type, оскільки вона не має числового представлення:\n\ncor(train_data[ ,-6])\n\n\n\nA matrix: 5 × 5 of type dbl\n\n    educationincomewomenprestigecensus\n\n\n    education 1.0000000 0.6243784 0.1507510 0.8448752-0.8212488\n    income 0.6243784 1.0000000-0.3910854 0.7425534-0.3037354\n    women 0.1507510-0.3910854 1.0000000 0.0227754-0.3431387\n    prestige 0.8448752 0.7425534 0.0227754 1.0000000-0.5968525\n    census-0.8212488-0.3037354-0.3431387-0.5968525 1.0000000\n\n\n\n\nLets view correlation matrix with corrplot() function:\n\nsuppressMessages(library(corrplot))\ncorrplot(cor(train_data[,-6]) , method = \"number\") \n\n\n\n\nYou can see hight correlation between education ~ prestige, education ~ census, prestige ~ income.\n\n\n2.3.2 Visual analysis\nLets check data distribution of some variables\nTarget/Output variable prestige:\n\nlibrary(ggplot2)\n\nggplot(train_data, aes(prestige)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nInput variable income:\n\n#ggplot(train_data, aes(log(income))) + \nggplot(train_data, aes(income)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nInput variable women:\n\n#ggplot(train_data, aes(log(women))) + \nggplot(train_data, aes(women)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#model-building",
    "href": "031-supervised-learning-linear-regression-1.html#model-building",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.4 Model building",
    "text": "2.4 Model building\nФункція lm() використовується для побудови лінійної регресії.\nSyntax: lm(formula, data = train_data)\nformula дозволяє вказати на залежність між вхідними та вихідним параметром. У даному випадку prestige - залежна змінна (Y), а усі, що після знаку ~ - незалежні (X).\n\nlm_mod <- lm(formula = prestige ~ income + education, data = train_data)\n\nДля детальнішого опису параметрів побудованої моделі варто скористатися функцією summary():\n\nsummary(lm_mod)\n\n\nCall:\nlm(formula = prestige ~ income + education, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.2951  -4.7988   0.1218   5.1932  17.1181 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -5.9237106  3.9637472  -1.494     0.14    \nincome       0.0014932  0.0003017   4.949 5.18e-06 ***\neducation    3.9135175  0.4460247   8.774 8.64e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.035 on 68 degrees of freedom\nMultiple R-squared:  0.7896,    Adjusted R-squared:  0.7834 \nF-statistic: 127.6 on 2 and 68 DF,  p-value: < 2.2e-16\n\n\n\n\n2.4.1 Demo task: generating final formula for regression\nСпробуємо створити рядковий вигляд для побудованої регресії (шляхом звичайного “склеювання” рядків). Це завдання для закріплення знань з алгоритмічного програмування в R.\n\nlm_mod$coefficients # coefficients of the model\n\n(Intercept)-5.92371064968481income0.00149322824686838education3.91351754603805\n\n\n\ngetLmFormula <- function(lm_model) {\n  \n  str_formula <- paste(lm_model$terms[[2]], \" = \", sep=\"\")\n  str_formula <- paste(str_formula, + round(lm_model$coefficients[1], 4), sep=\" \")\n  \n  for(i in 2:length(lm_mod$coefficients)) {\n    \n    znak <- \"+\"\n    if(lm_model$coefficients[i] < 0)\n      znak <- \"-\"\n    str_formula <- paste(str_formula, \" \", znak, \" \", round(lm_mod$coefficients[i], 4), \"*\", names(lm_mod$coefficients)[i] ,sep = \"\")\n  }\n  \n  return(str_formula)\n}\n\nТоді формулу можна отримати наступним чином:\n\nstr_formula <- getLmFormula(lm_mod)\nprint(str_formula)\n\n[1] \"prestige =  -5.9237 + 0.0015*income + 3.9135*education\"\n\n\nEND OF DEMO TASK\n\nOne more way to preview model info is using package broom and function tidy():\n\n#install.packages(\"broom\")\nlibrary(broom)\n\nlm_mod_view <- tidy(lm_mod)\nlm_mod_view\n\n\n\nA tibble: 3 × 5\n\n    termestimatestd.errorstatisticp.value\n    <chr><dbl><dbl><dbl><dbl>\n\n\n    (Intercept)-5.9237106503.9637471616-1.4944721.396785e-01\n    income      0.0014932280.0003017277 4.9489275.183124e-06\n    education   3.9135175460.4460246832 8.7742178.636723e-13\n\n\n\n\n\n\n2.4.2 Зміна форми залежності\nСпробуємо змінити форму залежності і побудуємо модель на основі трансформованих показників. Скористаємося логарифмуванням незалежних змінних:\n\nlm_mod2 <- lm(formula = prestige ~ log(income) + log(education), data = train_data)\nsummary(lm_mod2)\n\n\nCall:\nlm(formula = prestige ~ log(income) + log(education), data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.6827  -6.0052   0.3465   4.1241  17.4622 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -165.835     14.142 -11.727  < 2e-16 ***\nlog(income)      14.016      1.923   7.290 4.25e-10 ***\nlog(education)   38.615      4.094   9.433 5.62e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.496 on 68 degrees of freedom\nMultiple R-squared:  0.8169,    Adjusted R-squared:  0.8115 \nF-statistic: 151.7 on 2 and 68 DF,  p-value: < 2.2e-16\n\n\n\\(R^2\\) зріс. Тобто зміна форми залежності може впливати на якість моделі.\nAlert! Its depends on our seed parameter, because of changing train and test sets.\nПовернемося до попередньої моделі:\n\nlm_mod <- lm(formula = prestige ~ income + education, data = train_data)\n\nПереглянемо графік реальних даних, прогнозованих та похибок. Для початку створимо тимчасовий data.frame для генерації графіка зі значень\n\nфактори;\nреальні значення;\nпрогнозовані значення;\nпохибки.\n\nУвага! Даний графік будуватимемо на тренувальній вибірці!\n\n# data frame for storing data\ntmp_data <- data.frame(education = train_data$education,\n                       income = train_data$income,\n                       prestige = train_data$prestige,\n                       predicted = lm_mod$fitted.values,\n                       residuals = lm_mod$residuals) \n\nПереглянемо залежності між параметрами моделі:\n\n# prestige vs education\nggplot(tmp_data, aes(x = education, y = prestige)) +\n  geom_segment(aes(xend = education, yend = predicted), alpha = .2) +\n  geom_point(aes(), color = 'blue') +\n  #scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  #guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 2) +\n  theme_bw()\n\n\n\n\n\n# prestige vs income\nggplot(tmp_data, aes(x = income, y = prestige)) +\n  geom_segment(aes(xend = income, yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals)) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 2) +\n  theme_bw()"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#prediction-on-test-data-and-accuracy",
    "href": "031-supervised-learning-linear-regression-1.html#prediction-on-test-data-and-accuracy",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.5 Prediction on test data and accuracy",
    "text": "2.5 Prediction on test data and accuracy\nЗдійснимо прогноз за допомогою функції predict().\n\nsuppressMessages(library(dplyr))\n\ntest_predicted <- predict(lm_mod, test_data)\n\nsummary(test_predicted)\nas.data.frame(sort(test_predicted) %>% head())\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.94   33.43   43.74   45.96   56.82   80.70 \n\n\n\n\nA data.frame: 6 × 1\n\n    sort(test_predicted) %>% head()\n    <dbl>\n\n\n    canners25.93679\n    farmers26.28458\n    launderers27.24206\n    cooks29.01981\n    bakers29.85428\n    babysitters32.01053\n\n\n\n\nMSE (Mean Squared Error) – середньоквадратичне відхилення; середнє значення квадратів відхилень прогнозованих даних від реальних.\n\nmse <- mean((test_data$prestige - test_predicted)^2)\nmse\n\n55.2458474568378\n\n\nMAPE. MAPE (Mean Absolute Percentage Error) – середнє абсолютне відхилення прогнозованого показника від реального:\n\nmape <- mean(abs(test_data$prestige - test_predicted)/test_data$prestige)\nmape\n\n0.134424776204161\n\n\nОбчислимо метрики моделі за допомогою пакету modelr та функцій з нього:\n\nsuppressMessages(library(modelr))\ndata.frame(\n  R2 = rsquare(lm_mod, data = test_data),\n  MSE = mse(lm_mod, data = test_data),\n  RMSE = rmse(lm_mod, data = test_data),\n  MAE = mae(lm_mod, data = test_data),\n  MAPE = mape(lm_mod, data = test_data)\n)\n\n\n\nA data.frame: 1 × 5\n\n    R2MSERMSEMAEMAPE\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n    0.821986555.245857.4327556.1394070.1344248\n\n\n\n\nMae - Mean Absolute Error. Rmse - Root mean squared error. Детальніше: https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d.\n\n2.5.1 Перевірка на мультиколінераність\nЗафіксуємо значення R^2 для моделі у деякій змінній:\n\nr_sq = rsquare(lm_mod, data = test_data)\nr_sq \n\n0.821986517080715\n\n\n\nsuppressMessages(library(car))\n\n\nvif(lm_mod)\n\nincome1.63893675540857education1.63893675540857\n\n\nЯк видно вище тут мультиколінеарність відсутня.\nВведемо в модель додатковий показник, що буде явно залежати від одного з факторів:\n\n#lm_mod2 <- lm(formula = prestige ~ income + education + income_2, data = train_data)\n#summary(lm_mod2)\n#rsquare(lm_mod2, data = train_data)\n\n\n#r_sq2 <- rsquare(lm_mod2, data = test_data)\n#r_sq\n#r_sq2\n\nVIF-тест для моделі із корельованими змінними:\n\n#vif(lm_mod2)\n\nThe best way is to exclude the biggest and rebuild model in this case.\n\n\n2.5.2 Extending model\nДодамо до моделі категоріальний параметр type. Переглянемо можливі варіанти значень:\n\nunique(train_data$type)\n\n\nprofbcwc<NA>\n\n\n    \n        Levels:\n    \n    \n    'bc''prof''wc'\n\n\n\nSo, its a factor. with 3 values and missing data (NA).\nLets build new model with type:\n\nlm_mod <- lm(formula = prestige ~ income + education + type, data = train_data)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = prestige ~ income + education + type, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7801  -4.1603   0.0212   5.2034  20.3020 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.136361   6.409402   0.645 0.521002    \nincome       0.001185   0.000303   3.912 0.000224 ***\neducation    2.962932   0.813449   3.642 0.000542 ***\ntypeprof     8.953012   4.713668   1.899 0.062024 .  \ntypewc      -0.974902   3.250061  -0.300 0.765177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.434 on 64 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8214,    Adjusted R-squared:  0.8102 \nF-statistic: 73.59 on 4 and 64 DF,  p-value: < 2.2e-16\n\n\nЗ’явилися нові показники typeprof та typewc. Вони згенеровані автоматично як dummy-змінні функцією lm(). Якщо Ви хочете переглянути усі показники, що приймали участь у побудові моделі, то можна викликати $model:\n\nlm_mod$model %>% head()\n\n\n\nA data.frame: 6 × 4\n\n    prestigeincomeeducationtype\n    <dbl><int><dbl><fct>\n\n\n    civil.engineers73.11137714.52prof\n    textile.weavers33.3 4443 6.69bc  \n    tool.die.makers42.5 804310.09bc  \n    insurance.agents47.3 813111.60wc  \n    slaughterers.234.8 5134 7.64bc  \n    service.station.attendant23.3 2370 9.93bc  \n\n\n\n\n\nlm_mod$model\n\n\n\nA data.frame: 69 × 4\n\n    prestigeincomeeducationtype\n    <dbl><int><dbl><fct>\n\n\n    civil.engineers73.11137714.52prof\n    textile.weavers33.3 4443 6.69bc  \n    tool.die.makers42.5 804310.09bc  \n    insurance.agents47.3 813111.60wc  \n    slaughterers.234.8 5134 7.64bc  \n    service.station.attendant23.3 2370 9.93bc  \n    computer.operators47.7 433011.36wc  \n    radio.tv.repairmen37.2 544910.29bc  \n    electrical.linemen40.9 8316 9.05bc  \n    psychologists74.9 740514.36prof\n    house.painters29.9 4549 7.81bc  \n    receptionsts38.7 290111.04wc  \n    slaughterers.125.2 5134 7.64bc  \n    typesetters42.2 646210.00bc  \n    physicians87.22530815.96prof\n    computer.programers53.8 842513.83prof\n    architects78.11416315.44prof\n    biologists72.6 825815.09prof\n    nurses64.7 461412.46prof\n    electronic.workers50.8 3942 8.76bc  \n    physio.therapsts72.1 509213.62prof\n    aircraft.workers43.7 6573 8.78bc  \n    sales.supervisors41.5 7482 9.84wc  \n    osteopaths.chiropractors68.41749814.71prof\n    radio.tv.announcers57.6 756212.71wc  \n    sewing.mach.operators28.2 2847 6.38bc  \n    sheet.metal.workers35.9 6565 8.40bc  \n    welders41.8 6477 7.92bc  \n    pharmacists69.31043215.21prof\n    farm.workers21.5 1656 8.60bc  \n    ⋮⋮⋮⋮⋮\n    janitors17.3 3472 7.11bc  \n    economists62.2 804914.44prof\n    aircraft.repairmen50.3 771610.10bc  \n    elevator.operators20.1 3582 7.58bc  \n    file.clerks32.7 301612.09wc  \n    veterinarians66.71455815.94prof\n    auto.repairmen38.1 5795 8.10bc  \n    social.workers55.1 633614.21prof\n    textile.labourers28.8 3485 6.74bc  \n    buyers51.1 795611.03wc  \n    lawyers82.31926315.77prof\n    travel.clerks35.7 625911.43wc  \n    secondary.school.teachers66.1 803415.08prof\n    masons36.2 5959 6.60bc  \n    collectors29.4 474111.20wc  \n    railway.sectionmen27.3 4696 6.67bc  \n    plumbers42.9 6928 8.33bc  \n    purchasing.officers56.8 886511.42prof\n    secretaries46.0 403611.59wc  \n    bartenders20.2 3930 8.50bc  \n    rotary.well.drillers35.3 6860 8.88bc  \n    mail.carriers36.1 5511 9.22wc  \n    mining.engineers68.81102314.64prof\n    pilots66.11403212.27prof\n    taxi.drivers25.1 4224 7.93bc  \n    electricians50.2 7147 9.93bc  \n    medical.technicians67.5 518012.79wc  \n    tellers.cashiers42.3 244810.64wc  \n    sales.clerks26.5 259410.05wc  \n    librarians58.1 611214.15prof"
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#model-errors",
    "href": "031-supervised-learning-linear-regression-1.html#model-errors",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.6 Model errors",
    "text": "2.6 Model errors\nПереглянемо характеристики моделі:\n\nlm_mod <- lm(formula = prestige ~ income + education + type, data = train_data)\nsummary(lm_mod)\n\n\nCall:\nlm(formula = prestige ~ income + education + type, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.7801  -4.1603   0.0212   5.2034  20.3020 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.136361   6.409402   0.645 0.521002    \nincome       0.001185   0.000303   3.912 0.000224 ***\neducation    2.962932   0.813449   3.642 0.000542 ***\ntypeprof     8.953012   4.713668   1.899 0.062024 .  \ntypewc      -0.974902   3.250061  -0.300 0.765177    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.434 on 64 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8214,    Adjusted R-squared:  0.8102 \nF-statistic: 73.59 on 4 and 64 DF,  p-value: < 2.2e-16\n\n\nСформуємо розмітку для виведення графіків 4-х графіків одразу (2*2):\n\n#par(mfrow=c(2,2))\nplot(lm_mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\nНа двох графіках зліва червона лінія показує середнє з відхилень. Якщо варіація похибок зростає разом із збільшенням значень прогнозу - це називається гетероскедастичніть. Прогнозування за таких умов буде давати спотворені результати. Переконаємося у відсутності гетероскедастичності перевіривши p-value:\n\nsuppressMessages(library(lmtest))\nbptest(lm_mod)\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm_mod\nBP = 9.0675, df = 4, p-value = 0.05943\n\n\np-value > 0.05, отже ми відхиляємо гіпотезу про те, що гетероскедастичність відсутня (залишки гомоскедастичні - мають однакову дисперсію).\nNormal QQ-графік дозволяє перевірити чи похибки розподілені за нормальним законом розподілу. Ідеальний варіант коли вони розміщені чітко по діагональній лінії.\nНа останньому графіку показується як кожне значення впливає на регресію. У статистиці відстань Кука є загальноприйнятою оцінкою впливу спостереження під час застосування методу найменших квадратів у регресійному аналізі.На практиці, при застосуванні методу найменших квадратів, відстань Кука може використовуватися для наступних цілей: визначити впливові спостереження даних, які потрібно перевірити на валідність; визначення областей простору, у яких непогано було б отримати більше результатів спостереження. Джерело: Detection of Influential Observation in Linear Regression / R. Dennis Cook (https://www.jstor.org/stable/1268249?origin=crossref&seq=1)\nПовернемо розмітку для 1-го графіка у вікні RStudio:\n\npar(mfrow=c(1,1))\n\nПереглянемо розподіл похибок:\n\nplot(residuals(lm_mod)) \nabline(a=0,b=0,col='blue')\n\n\n\n\nВізуальних закономірностей у похибках немає. Схоже, що вони мають випадковий характер."
  },
  {
    "objectID": "031-supervised-learning-linear-regression-1.html#features-selection",
    "href": "031-supervised-learning-linear-regression-1.html#features-selection",
    "title": "2  Лінійна регресія. Престиж професій",
    "section": "2.7 Features selection",
    "text": "2.7 Features selection\nРучний відбір параметрів моделі має ряд недоліків, що пов’язані із якістю моделі та затратами часу на її побудову. Вивчимо окремі алгоритми автоматизованого відбору параметрів у модель.\n\n2.7.1 BestSubsets method\nМетод BestSubsets також відомий як all possible regressions. Цей метод відповідно до назв будує всі можливі варіанти моделей на незалежних змінних. Кількість моделей становить \\(2^p\\), де \\(p\\) - кількість неазелжних змінних, так для 10 вхідних параметрів буде побудовано \\(2^10 =1024\\) моделей.\n\nlibrary(leaps)\nbest_subsets_mod <- regsubsets(prestige ~ income + education + type + women, data = train_data)\n\nРозгялнемо відібрані параметри моделі BestSubsets. По осі Y розміщені значення BIC, який обраний як якісний показник регресії, по X - параметри моделі:\n\nplot(best_subsets_mod)\n\n\n\n\nНайтемніша лінія вказує на найкращу модель.\nПеребудуємо модель для усіх можливих варіантів комбінацій параметрів:\n\nbest_subsets_mod <- regsubsets(prestige ~ income + education + type + women, data = train_data)\n\nТакож можемо обрати кращу модель окремо за критеріями, наприклад скоригований \\(R^2\\):\n\nplot(best_subsets_mod, scale = \"adjr2\")\n\n\n\n\nМоделі з найвищим \\(R^2\\) обираються у такому випадку за цим критерієм, проте не варто забувати про появу мультиколінеарності.\n\nresults <- summary(best_subsets_mod)\nround(results$adjr2, 4)\n\n\n0.7290.79030.81290.81510.8136\n\n\n\nmodel_index <- which.max(results$adjr2)\nt <- results$which[model_index, ]\nas.data.frame(t)\n\n\n\nA data.frame: 6 × 1\n\n    t\n    <lgl>\n\n\n    (Intercept) TRUE\n    income TRUE\n    education TRUE\n    typeprof TRUE\n    typewcFALSE\n    women TRUE\n\n\n\n\nSo, the best regression by Adjusted R-Squared is\na0 + a1*income + a2*typeprof + a3*income:typeprof + a4*education:women + a5*income:education:typeprof + a6*income:typeprof:women + a7*income:typewc:women + a8*income:education:typewc:women\n\n\n\n2.7.2 Stepwise method\nStepwise - метод, що перебирає можливі варіанти та повертає найкращу модель з найнижчим показником AIC. Перебір моделі може бути з виключенням або з включенням показників у модель - у обох напрямках.\nДля початку очистимо дані, що мають пропуски у полі type (також ці дані можна заповнити).\n\nnrow(train_data)\ntrain_data <- train_data[!is.na(train_data$type), ]\nnrow(train_data)\n\n71\n\n\n69\n\n\nНайпростіша модель з одним параметром (першим по списку) матиме вигляд:\n\nstart_mod <- lm(prestige ~ 1, data = train_data)\n\nМодель з усіма параметрами:\n\nend_mod <- lm(prestige ~ ., data = train_data)\n\nЗапустимо алгоритм функцією step():\n\nstepwise_mod <- step(start_mod, \n                     # set minimum and maximum parameters\n                     scope = list(lower = start_mod, upper = end_mod),\n                     # direction of model building\n                     direction = \"both\", trace = 1, steps = 1000)\n                     # c(\"both\", \"backward\", \"forward\") - possible directions\n\nStart:  AIC=392.5\nprestige ~ 1\n\n            Df Sum of Sq     RSS    AIC\n+ education  1   14514.4  5286.9 303.38\n+ type       2   13466.4  6334.9 317.86\n+ income     1   10623.3  9178.0 341.44\n+ census     1    7498.0 12303.3 361.66\n<none>                   19801.3 392.50\n+ women      1       2.1 19799.2 394.49\n\nStep:  AIC=303.38\nprestige ~ education\n\n            Df Sum of Sq     RSS    AIC\n+ income     1    1257.5  4029.4 286.64\n+ type       2     904.7  4382.2 294.43\n+ census     1     512.7  4774.3 298.35\n+ women      1     284.3  5002.6 301.57\n<none>                    5286.9 303.38\n- education  1   14514.4 19801.3 392.50\n\nStep:  AIC=286.64\nprestige ~ education + income\n\n            Df Sum of Sq    RSS    AIC\n+ type       2     492.9 3536.5 281.64\n<none>                   4029.4 286.64\n+ women      1      70.7 3958.8 287.42\n+ census     1      47.1 3982.3 287.83\n- income     1    1257.5 5286.9 303.38\n- education  1    5148.6 9178.0 341.44\n\nStep:  AIC=281.64\nprestige ~ education + income + type\n\n            Df Sum of Sq    RSS    AIC\n+ census     1    144.45 3392.1 280.76\n+ women      1    116.49 3420.0 281.33\n<none>                   3536.5 281.64\n- type       2    492.89 4029.4 286.64\n- education  1    733.13 4269.7 292.64\n- income     1    845.72 4382.2 294.43\n\nStep:  AIC=280.76\nprestige ~ education + income + type + census\n\n            Df Sum of Sq    RSS    AIC\n+ women      1    131.49 3260.6 280.03\n<none>                   3392.1 280.76\n- census     1    144.45 3536.5 281.64\n- income     1    470.45 3862.5 287.72\n- type       2    590.20 3982.3 287.83\n- education  1    877.55 4269.6 294.64\n\nStep:  AIC=280.03\nprestige ~ education + income + type + census + women\n\n            Df Sum of Sq    RSS    AIC\n<none>                   3260.6 280.03\n- women      1    131.49 3392.1 280.76\n- census     1    159.46 3420.0 281.33\n- type       2    628.53 3889.1 288.20\n- income     1    596.05 3856.6 289.62\n- education  1    748.59 4009.2 292.29\n\n\n\n# lets see the summary\nsummary(stepwise_mod)\n\n\nCall:\nlm(formula = prestige ~ education + income + type + census + \n    women, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.0817  -4.9121   0.0985   5.8624  19.7564 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.136e+01  1.058e+01  -1.073 0.287294    \neducation    3.328e+00  8.821e-01   3.773 0.000363 ***\nincome       1.307e-03  3.882e-04   3.367 0.001311 ** \ntypeprof     1.241e+01  5.318e+00   2.333 0.022930 *  \ntypewc       7.615e-01  3.732e+00   0.204 0.839003    \ncensus       1.356e-03  7.788e-04   1.741 0.086595 .  \nwomen        6.212e-02  3.928e-02   1.581 0.118911    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.252 on 62 degrees of freedom\nMultiple R-squared:  0.8353,    Adjusted R-squared:  0.8194 \nF-statistic: 52.42 on 6 and 62 DF,  p-value: < 2.2e-16\n\n\n\nЗ підсумків моделі видно, що детермінація зросла, тобто модель стала описувати явище ще краще."
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html",
    "href": "032-supervised-learning-linear-regression-2.html",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "",
    "text": "Курс: “Математичне моделювання в R”"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#dataset-overview",
    "href": "032-supervised-learning-linear-regression-2.html#dataset-overview",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.1 Dataset overview",
    "text": "3.1 Dataset overview\nSource: https://www.kaggle.com/c/bike-sharing-demand/data\nDataset description:\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\nData Fields:\n\ndatetime - hourly date + timestamp\nseason -\n1 = spring\n2 = summer\n3 = fall\n4 = winter\n\nholiday - whether the day is considered a holiday\nworkingday - whether the day is neither a weekend nor holiday\nweather\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp - temperature in Celsius\natemp - “feels like” temperature in Celsius\nhumidity - relative humidity\nwindspeed - wind speed\ncasual - number of non-registered user rentals initiated\nregistered - number of registered user rentals initiated\ncount - number of total rentals (It is target variable. We will predict it!)\n\n\ndata <- read.csv(\"data/bikes.csv\")\nhead(data)\n\n\n\nA data.frame: 6 × 12\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcasualregisteredcount\n    <chr><int><int><int><int><dbl><dbl><int><dbl><int><int><int>\n\n\n    12011-01-01 00:00:0010019.8414.395810.000031316\n    22011-01-01 01:00:0010019.0213.635800.000083240\n    32011-01-01 02:00:0010019.0213.635800.000052732\n    42011-01-01 03:00:0010019.8414.395750.000031013\n    52011-01-01 04:00:0010019.8414.395750.00000 1 1\n    62011-01-01 05:00:0010029.8412.880756.00320 1 1\n\n\n\n\nОписова статистика факторів:\n\nsummary(data)\n\n   datetime             season         holiday          workingday    \n Length:10886       Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :3.000   Median :0.00000   Median :1.0000  \n                    Mean   :2.507   Mean   :0.02857   Mean   :0.6809  \n                    3rd Qu.:4.000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :4.000   Max.   :1.00000   Max.   :1.0000  \n    weather           temp           atemp          humidity     \n Min.   :1.000   Min.   : 0.82   Min.   : 0.76   Min.   :  0.00  \n 1st Qu.:1.000   1st Qu.:13.94   1st Qu.:16.66   1st Qu.: 47.00  \n Median :1.000   Median :20.50   Median :24.24   Median : 62.00  \n Mean   :1.418   Mean   :20.23   Mean   :23.66   Mean   : 61.89  \n 3rd Qu.:2.000   3rd Qu.:26.24   3rd Qu.:31.06   3rd Qu.: 77.00  \n Max.   :4.000   Max.   :41.00   Max.   :45.45   Max.   :100.00  \n   windspeed          casual         registered        count      \n Min.   : 0.000   Min.   :  0.00   Min.   :  0.0   Min.   :  1.0  \n 1st Qu.: 7.002   1st Qu.:  4.00   1st Qu.: 36.0   1st Qu.: 42.0  \n Median :12.998   Median : 17.00   Median :118.0   Median :145.0  \n Mean   :12.799   Mean   : 36.02   Mean   :155.6   Mean   :191.6  \n 3rd Qu.:16.998   3rd Qu.: 49.00   3rd Qu.:222.0   3rd Qu.:284.0  \n Max.   :56.997   Max.   :367.00   Max.   :886.0   Max.   :977.0"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#minimal-data-preprocessing",
    "href": "032-supervised-learning-linear-regression-2.html#minimal-data-preprocessing",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.2 Minimal data preprocessing",
    "text": "3.2 Minimal data preprocessing\nПеревіримо вибірку на наявність пропусків за допомогою функції md.pattern() з пакету mice:\n\nanyNA(data)\n\nFALSE\n\n\n\nsuppressMessages(library(mice))\nmd.pattern(data, F)\n\n /\\     /\\\n{  `---'  }\n{  O   O  }\n==>  V <==  No need for mice. This data set is completely observed.\n \\  \\|/  /\n  `-----'\n\n\n\n\n\nA matrix: 2 × 13 of type dbl\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcasualregisteredcount\n\n\n    108861111111111110\n    0000000000000\n\n\n\n\nThere are no missing data! Great for us!\nПеретворимо категоріальні змінні до факторів:\n\ndata$season <- factor(data$season)\ndata$holiday <- factor(data$holiday)\ndata$workingday <- factor(data$workingday)\ndata$weather <- factor(data$weather)\ndata$holiday <- factor(data$holiday)\n\nhead(data) # preview data\n\n\n\nA data.frame: 6 × 12\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcasualregisteredcount\n    <chr><fct><fct><fct><fct><dbl><dbl><int><dbl><int><int><int>\n\n\n    12011-01-01 00:00:0010019.8414.395810.000031316\n    22011-01-01 01:00:0010019.0213.635800.000083240\n    32011-01-01 02:00:0010019.0213.635800.000052732\n    42011-01-01 03:00:0010019.8414.395750.000031013\n    52011-01-01 04:00:0010019.8414.395750.00000 1 1\n    62011-01-01 05:00:0010029.8412.880756.00320 1 1\n\n\n\n\nУ наборі даних наявні 3 вихідних параметра: casual,registered,count, де $casual + registered = count$. Здійснимо прогнозcount`. Видалимо змінні, які використовуватися нами не будуть:\n\n#data$casual <- NULL\n#data$registered <- NULL\n\n# or with dplyr\nsuppressMessages(library(dplyr))\ndata <- data %>% select(-c(casual, registered))\n\nhead(data)\n\n\n\nA data.frame: 6 × 10\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcount\n    <chr><fct><fct><fct><fct><dbl><dbl><int><dbl><int>\n\n\n    12011-01-01 00:00:0010019.8414.395810.000016\n    22011-01-01 01:00:0010019.0213.635800.000040\n    32011-01-01 02:00:0010019.0213.635800.000032\n    42011-01-01 03:00:0010019.8414.395750.000013\n    52011-01-01 04:00:0010019.8414.395750.0000 1\n    62011-01-01 05:00:0010029.8412.880756.0032 1"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#exploratory-data-analysis",
    "href": "032-supervised-learning-linear-regression-2.html#exploratory-data-analysis",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.3 Exploratory data analysis",
    "text": "3.3 Exploratory data analysis\nОцінимо звязки між числовими змінними та вихідним показником через кореляцію:\n\ndata %>%\n select(temp:count) %>% \n cor() %>%\n as.data.frame()\n\n\n\nA data.frame: 5 × 5\n\n    tempatemphumiditywindspeedcount\n    <dbl><dbl><dbl><dbl><dbl>\n\n\n    temp 1.00000000 0.98494811-0.06494877-0.01785201 0.3944536\n    atemp 0.98494811 1.00000000-0.04353571-0.05747300 0.3897844\n    humidity-0.06494877-0.04353571 1.00000000-0.31860699-0.3173715\n    windspeed-0.01785201-0.05747300-0.31860699 1.00000000 0.1013695\n    count 0.39445364 0.38978444-0.31737148 0.10136947 1.0000000\n\n\n\n\nЩоб оглянути інформацію про частоту спостежень по сезонах варто скористатися записом:\n\ndata %>%\n group_by(season) %>%\n summarize(n = n())\n\n\n\nA tibble: 4 × 2\n\n    seasonn\n    <fct><int>\n\n\n    12686\n    22733\n    32733\n    42734\n\n\n\n\n\nlibrary(ggplot2)\n# llok like we have almost the same count by each season\nggplot(data, aes(season)) + \n    geom_bar(aes(fill = season)) + \n    theme_bw()\n\n\n\n\n\n# Holidays plot\ndata %>%\n group_by(holiday) %>%\n summarize(n = n())\n\nggplot(data, aes(holiday)) + geom_bar(aes(fill = holiday)) + theme_bw()\n\n\n\nA tibble: 2 × 2\n\n    holidayn\n    <fct><int>\n\n\n    010575\n    1  311\n\n\n\n\n\n\n\n\ndata %>%\n group_by(workingday) %>%\n summarize(n = n())\n\n# Working dats plot\nggplot(data, aes(workingday)) + geom_bar(aes(fill = workingday)) + theme_bw()\n\n\n\nA tibble: 2 × 2\n\n    workingdayn\n    <fct><int>\n\n\n    03474\n    17412\n\n\n\n\n\n\n\n\nweather\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n\n\n# weather plot\nggplot(data, aes(weather)) + geom_bar(aes(fill = weather)) + theme_bw()\n\n\n\n\nПереглянемо також залежність між числовими факторами та кількістю прокатів байків. Для температури це матиме запис:\n\n# temp\nplot(x=data$temp, y=data$count)\n\n\n\n\n\nggplot(data, aes(temp, count)) + geom_point() + theme_bw()\n\n\n\n\n\n# atemp\nggplot(data, aes(atemp, count)) + geom_point() + theme_bw()\n\n\n\n\n\n# humidity\nggplot(data, aes(humidity, count)) + geom_point() + theme_bw()\n\n\n\n\n\n# windspeed\nggplot(data, aes(windspeed, count)) + geom_point() + theme_bw()\n\n\n\n\nПереглянемо також загальний розподіл частоти замовлень байків на гістограмі:\n\nggplot(data, aes(count)) + geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + theme_bw()\n\n\n\n\nПри спробі побудувати графік залежності між датою та кількістю замовлень байків ми отримаємо помилку або графік, що не відповідає порядку дат, оскільки на даний момент R не сприймає поле datetime як дату. Перетворимо дату за допомгою функції as.POSIXct():\n\ndata$datetime <- as.POSIXct(data$datetime)\nggplot(data, aes(datetime, count)) + geom_point() + theme_bw()\n\n\n\n\nЧи можна прослідкувати зміну кількості замовлень в залежності від пори року з цього графіку?\nПокращити вигляд графіка можна додавання параметрів у geom_point():\n\nggplot(data, aes(datetime, count)) + geom_point(aes(color=temp)) + theme_bw()\n\n\n\n\nПереглянемо як час оренди байка впливає на кількість замовлень:\n\nggplot(data, aes(format(datetime, \"%H\"), count)) + \n        geom_point(aes(color=temp), alpha = 0.5) + \n        theme_bw()\n\n\n\n\nAlso, we can add new column to our dataset and use it:\n\ndata <- data %>%\n    mutate(hour_rent = as.numeric(format(datetime, \"%H\")))\nhead(data)\n\n\n\nA data.frame: 6 × 11\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rent\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl>\n\n\n    12011-01-01 00:00:0010019.8414.395810.0000160\n    22011-01-01 01:00:0010019.0213.635800.0000401\n    32011-01-01 02:00:0010019.0213.635800.0000322\n    42011-01-01 03:00:0010019.8414.395750.0000133\n    52011-01-01 04:00:0010019.8414.395750.0000 14\n    62011-01-01 05:00:0010029.8412.880756.0032 15\n\n\n\n\n\n# the same chart as before\nggplot(data, aes(hour_rent, count)) + \n        geom_point(aes(color=temp), alpha = 0.5) + \n        theme_bw()"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#train-test-split",
    "href": "032-supervised-learning-linear-regression-2.html#train-test-split",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.4 Train test split",
    "text": "3.4 Train test split\nOne more splitting method is based on date. So, we can use first 70% of dataset as train and 30% as test. This method often used for timeseries forecasting. We will use it for linear regression for now.\n\nset.seed(10) \nnrow(data)\n\ntrain_count = ceiling(0.7 * nrow(data))\ntrain_count\n\ntest_count = nrow(data) - train_count\ntest_count\n\n10886\n\n\n7621\n\n\n3265\n\n\n\ndata <- data %>% arrange(datetime) # sort by datetime\n\n\ntrain_data <- data %>% slice_head(n = train_count)\nnrow(train_data)\ntest_data <- data %>% slice_tail(n = test_count)\nnrow(test_data)\n\n7621\n\n\n3265\n\n\n\n# lets compare last from train and first from test\ntail(train_data)\nhead(test_data) # ok!\n\n\n\nA data.frame: 6 × 11\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rent\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl>\n\n\n    76162012-05-16 16:00:00201129.5232.57545 7.001544616\n    76172012-05-16 17:00:00201129.5233.3355112.998087317\n    76182012-05-16 18:00:00201129.5233.3355115.001384618\n    76192012-05-16 19:00:00201128.7032.5755419.001259019\n    76202012-05-16 20:00:00201127.0631.0606516.997945920\n    76212012-05-16 21:00:00201126.2430.3057312.998039321\n\n\n\n\n\n\nA data.frame: 6 × 11\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rent\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl>\n\n\n    12012-05-16 22:00:00201126.2430.3057311.001428622\n    22012-05-16 23:00:00201125.4229.54578 7.001513323\n    32012-05-17 00:00:00201124.6028.79078 8.9981 79 0\n    42012-05-17 01:00:00201124.6028.0308311.0014 28 1\n    52012-05-17 02:00:00201124.6028.7907811.0014 16 2\n    62012-05-17 03:00:00201124.6029.5457316.9979  3 3"
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#model-building",
    "href": "032-supervised-learning-linear-regression-2.html#model-building",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.5 Model building",
    "text": "3.5 Model building\nПобудуємо напройстішу модель залежності середньої температури за день та кількості орендованих байків:\n\nlm_bike <- lm(count ~ atemp, train_data)\nsummary(lm_bike)\n\n\nCall:\nlm(formula = count ~ atemp, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-254.42  -93.91  -27.43   65.57  648.09 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.0774     4.4973  -0.462    0.644    \natemp         7.2602     0.1895  38.311   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 138.4 on 7619 degrees of freedom\nMultiple R-squared:  0.1615,    Adjusted R-squared:  0.1614 \nF-statistic:  1468 on 1 and 7619 DF,  p-value: < 2.2e-16\n\n\nСпробуємо порахувати кількість орендованих байків для середніх темпертур від 10 до 30 градусів.\nСтворимо датафрейм з послідовністю елементів 10-30 з кроком 1 та порахуємо за формулою з інформації про модель прогнозовані показники:\nЗдійснимо прогноз за допомогою функції predict():\n\n#predict_1030$count_model <- predict(lm_bike, newdata = predict_1030)\n#predict_1030 %>% head()\n\n# our results are very close, minimal difference is a result of rounding numbers in our formula\n\n\n# how it looks like at chart\n#ggplot(predict_1030, aes(atemp, count_model)) + \n #       geom_point() + \n    #    theme_bw()\n\nПобудуємо модель для усіх параметрів моделі окрім дати:\n\nlm_bike <- lm(count~.-datetime, train_data)\nsummary(lm_bike)\n\n\nCall:\nlm(formula = count ~ . - datetime, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-283.67  -78.72  -25.68   46.27  587.17 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  15.67996    8.85332   1.771   0.0766 .  \nseason2       9.39674    4.59288   2.046   0.0408 *  \nseason3     -40.29356    6.46891  -6.229 4.95e-10 ***\nseason4      21.14562    4.40422   4.801 1.61e-06 ***\nholiday1     -8.46508    8.79408  -0.963   0.3358    \nworkingday1  -4.89327    3.12276  -1.567   0.1172    \nweather2      2.76675    3.47283   0.797   0.4257    \nweather3    -31.30849    5.61763  -5.573 2.59e-08 ***\nweather4     85.56066  122.62585   0.698   0.4854    \ntemp          1.34075    1.67470   0.801   0.4234    \natemp         6.30809    1.48446   4.249 2.17e-05 ***\nhumidity     -1.49856    0.08884 -16.868  < 2e-16 ***\nwindspeed     0.35086    0.19131   1.834   0.0667 .  \nhour_rent     6.20760    0.21640  28.685  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 7607 degrees of freedom\nMultiple R-squared:  0.3438,    Adjusted R-squared:  0.3427 \nF-statistic: 306.6 on 13 and 7607 DF,  p-value: < 2.2e-16\n\n\nПеревіримо модель на мультиколінеарність:\n\nsuppressMessages(library(car))\nvif(lm_bike)\n\n\n\nA matrix: 9 × 3 of type dbl\n\n    GVIFDfGVIF^(1/(2*Df))\n\n\n    season 2.94174431.197018\n    holiday 1.07616211.037382\n    workingday 1.07216311.035453\n    weather 1.29120731.043516\n    temp83.07373919.114480\n    atemp78.28136218.847676\n    humidity 1.59412411.262586\n    windspeed 1.33172311.154003\n    hour_rent 1.13543711.065569\n\n\n\n\nВиключимо корельовані показники з моделі за принципом - залишити той, що має вищу кореляцію із залежним фактором. У нашому випадку це temp:\n\nlm_bike <- lm(count~.-datetime-atemp, train_data)\nsummary(lm_bike)\n\n\nCall:\nlm(formula = count ~ . - datetime - atemp, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-297.02  -79.11  -25.60   46.35  585.12 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26.97221    8.45456   3.190  0.00143 ** \nseason2       9.25257    4.59790   2.012  0.04422 *  \nseason3     -45.22396    6.37114  -7.098 1.38e-12 ***\nseason4      22.00064    4.40455   4.995 6.02e-07 ***\nholiday1    -11.34365    8.77778  -1.292  0.19629    \nworkingday1  -4.86349    3.12625  -1.556  0.11982    \nweather2      3.02977    3.47616   0.872  0.38346    \nweather3    -32.19037    5.62008  -5.728 1.06e-08 ***\nweather4     88.33390  122.76150   0.720  0.47182    \ntemp          8.34447    0.29736  28.062  < 2e-16 ***\nhumidity     -1.47674    0.08879 -16.632  < 2e-16 ***\nwindspeed     0.06683    0.17945   0.372  0.70959    \nhour_rent     6.20731    0.21665  28.652  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.7 on 7608 degrees of freedom\nMultiple R-squared:  0.3422,    Adjusted R-squared:  0.3412 \nF-statistic: 329.9 on 12 and 7608 DF,  p-value: < 2.2e-16\n\n\nДалі побудуємо модель на основі алгоритму stepwise та порівняємо з поточними характеристиками моделі.\nСтворимо моделі для проходів у два боки:\n\nstart_mod <- lm(count ~ 1, data = train_data)\nend_mod <- lm(count ~ . - datetime - temp, data = train_data)\n\nЗапустимо алгоритм функцією step():\n\nlm_bike_stepwise <- step(start_mod,\n                     scope = list(lower = start_mod, upper = end_mod),\n                     direction = \"both\", trace = 1, steps = 1000)\n\nStart:  AIC=76488.12\ncount ~ 1\n\n             Df Sum of Sq       RSS   AIC\n+ atemp       1  28114587 145942663 75148\n+ hour_rent   1  27600736 146456514 75174\n+ humidity    1  15503642 158553609 75779\n+ season      3   9131936 164925314 76083\n+ weather     3   3537078 170520172 76338\n+ windspeed   1   2112392 171944858 76397\n<none>                    174057250 76488\n+ holiday     1     12170 174045081 76490\n+ workingday  1      3511 174053739 76490\n\nStep:  AIC=75147.53\ncount ~ atemp\n\n             Df Sum of Sq       RSS   AIC\n+ hour_rent   1  20603748 125338915 73990\n+ humidity    1  15576942 130365721 74289\n+ season      3   5178271 140764392 74878\n+ weather     3   2989985 142952678 74996\n+ windspeed   1   2889374 143053289 74997\n<none>                    145942663 75148\n+ workingday  1     34765 145907898 75148\n+ holiday     1        44 145942619 75150\n- atemp       1  28114587 174057250 76488\n\nStep:  AIC=73989.67\ncount ~ atemp + hour_rent\n\n             Df Sum of Sq       RSS   AIC\n+ humidity    1   8093815 117245100 73483\n+ season      3   2905720 122433195 73817\n+ weather     3   2861080 122477834 73820\n+ windspeed   1   1038961 124299954 73928\n<none>                    125338915 73990\n+ workingday  1     29661 125309254 73990\n+ holiday     1       313 125338602 73992\n- hour_rent   1  20603748 145942663 75148\n- atemp       1  21117599 146456514 75174\n\nStep:  AIC=73482.93\ncount ~ atemp + hour_rent + humidity\n\n             Df Sum of Sq       RSS   AIC\n+ season      3   2375796 114869304 73333\n+ weather     3    534048 116711052 73454\n+ workingday  1     32372 117212728 73483\n<none>                    117245100 73483\n+ windspeed   1     22790 117222310 73483\n+ holiday     1      6750 117238350 73484\n- humidity    1   8093815 125338915 73990\n- hour_rent   1  13120621 130365721 74289\n- atemp       1  22173444 139418544 74801\n\nStep:  AIC=73332.92\ncount ~ atemp + hour_rent + humidity + season\n\n             Df Sum of Sq       RSS   AIC\n+ weather     3    525600 114343704 73304\n+ workingday  1     42319 114826985 73332\n+ windspeed   1     31234 114838071 73333\n<none>                    114869304 73333\n+ holiday     1      2238 114867066 73335\n- season      3   2375796 117245100 73483\n- humidity    1   7563890 122433195 73817\n- atemp       1  12150238 127019542 74097\n- hour_rent   1  12266423 127135727 74104\n\nStep:  AIC=73303.97\ncount ~ atemp + hour_rent + humidity + season + weather\n\n             Df Sum of Sq       RSS   AIC\n+ windspeed   1     77129 114266576 73301\n+ workingday  1     30192 114313512 73304\n<none>                    114343704 73304\n+ holiday     1      3786 114339918 73306\n- weather     3    525600 114869304 73333\n- season      3   2367348 116711052 73454\n- humidity    1   5224113 119567817 73642\n- atemp       1  12055477 126399182 74066\n- hour_rent   1  12568342 126912046 74097\n\nStep:  AIC=73300.82\ncount ~ atemp + hour_rent + humidity + season + weather + windspeed\n\n             Df Sum of Sq       RSS   AIC\n<none>                    114266576 73301\n+ workingday  1     28599 114237977 73301\n+ holiday     1      4334 114262242 73303\n- windspeed   1     77129 114343704 73304\n- weather     3    571495 114838071 73333\n- season      3   2382626 116649202 73452\n- humidity    1   4315759 118582335 73581\n- atemp       1  12116310 126382885 74067\n- hour_rent   1  12411506 126678081 74085\n\n\n\n# check the summary\nsummary(lm_bike_stepwise)\n\n\nCall:\nlm(formula = count ~ atemp + hour_rent + humidity + season + \n    weather + windspeed, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-285.56  -78.70  -25.63   47.20  586.14 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  10.56655    8.35898   1.264   0.2062    \natemp         7.46224    0.26269  28.407  < 2e-16 ***\nhour_rent     6.21766    0.21626  28.750  < 2e-16 ***\nhumidity     -1.50051    0.08851 -16.954  < 2e-16 ***\nseason2       9.95637    4.55753   2.185   0.0289 *  \nseason3     -38.55100    6.14342  -6.275 3.68e-10 ***\nseason4      21.22676    4.40003   4.824 1.43e-06 ***\nweather2      2.68041    3.47136   0.772   0.4400    \nweather3    -31.47986    5.60817  -5.613 2.06e-08 ***\nweather4     83.31112  122.62122   0.679   0.4969    \nwindspeed     0.40641    0.17932   2.266   0.0235 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 7610 degrees of freedom\nMultiple R-squared:  0.3435,    Adjusted R-squared:  0.3426 \nF-statistic: 398.2 on 10 and 7610 DF,  p-value: < 2.2e-16\n\n\n\\(R^2\\) increses!\nОб’єднаємо для наочності коефіцієнти у один датафрейм:\n\nlm_coefs_def <- data.frame(lm_name = names(lm_bike$coefficients),\n                           name = lm_bike$coefficients, \n                           row.names = c() # cleaning rownames\n                          )\nlm_coefs_def\n\n\n\nA data.frame: 13 × 2\n\n    lm_namename\n    <chr><dbl>\n\n\n    (Intercept) 26.97221255\n    season2      9.25256711\n    season3    -45.22395805\n    season4     22.00064078\n    holiday1   -11.34364536\n    workingday1 -4.86348665\n    weather2     3.02977126\n    weather3   -32.19036573\n    weather4    88.33390060\n    temp         8.34447012\n    humidity    -1.47674448\n    windspeed    0.06683245\n    hour_rent    6.20730753\n\n\n\n\n\nlm_coefs_sw <- data.frame(lm_name = names(lm_bike_stepwise$coefficients), name = lm_bike_stepwise$coefficients, row.names = c())\nlm_coefs_sw\n\n\n\nA data.frame: 11 × 2\n\n    lm_namename\n    <chr><dbl>\n\n\n    (Intercept) 10.5665521\n    atemp        7.4622378\n    hour_rent    6.2176579\n    humidity    -1.5005074\n    season2      9.9563690\n    season3    -38.5510028\n    season4     21.2267554\n    weather2     2.6804117\n    weather3   -31.4798561\n    weather4    83.3111193\n    windspeed    0.4064147\n\n\n\n\n\n# lets combine coefficients from both models\nlm_coefs <- merge(lm_coefs_def, lm_coefs_sw, by = \"lm_name\", X.all = T, X.all = T)\ncolnames(lm_coefs) <- c(\"variable\", \"lm\", \"lm_sw\")\nlm_coefs\n\n\n\nA data.frame: 10 × 3\n\n    variablelmlm_sw\n    <chr><dbl><dbl>\n\n\n    (Intercept) 26.97221255 10.5665521\n    hour_rent    6.20730753  6.2176579\n    humidity    -1.47674448 -1.5005074\n    season2      9.25256711  9.9563690\n    season3    -45.22395805-38.5510028\n    season4     22.00064078 21.2267554\n    weather2     3.02977126  2.6804117\n    weather3   -32.19036573-31.4798561\n    weather4    88.33390060 83.3111193\n    windspeed    0.06683245  0.4064147\n\n\n\n\nЯк бачимо моделі мають незначні відмінності."
  },
  {
    "objectID": "032-supervised-learning-linear-regression-2.html#model-errors-analysis",
    "href": "032-supervised-learning-linear-regression-2.html#model-errors-analysis",
    "title": "3  Лінійна регресія. Прокат велосипедів",
    "section": "3.6 Model errors analysis",
    "text": "3.6 Model errors analysis\nЗапишемо у train набір даних модельовані значення count та похибки:\n\ntrain_data$predicted <- lm_bike$fitted.values\ntrain_data$residuals <- lm_bike$residuals\nhead(train_data)\n\n\n\nA data.frame: 6 × 13\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rentpredictedresiduals\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl><dbl><dbl>\n\n\n    12011-01-01 00:00:0010019.8414.395810.0000160-10.534505 26.534505\n    22011-01-01 01:00:0010019.0213.635800.0000401 -9.692918 49.692918\n    32011-01-01 02:00:0010019.0213.635800.0000322 -3.485611 35.485611\n    42011-01-01 03:00:0010019.8414.395750.0000133 16.947885 -3.947885\n    52011-01-01 04:00:0010019.8414.395750.0000 14 23.155192-22.155192\n    62011-01-01 05:00:0010029.8412.880756.0032 15 32.793480-31.793480\n\n\n\n\nВізуалізуємо відхилення моделі:\n\nplot_data <- train_data[1:100,]\n\n\nggplot(plot_data, aes(x = c(1:nrow(plot_data)), y = as.numeric(count))) +\n  geom_segment(aes(xend = c(1:nrow(plot_data)), yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals), size = 2) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 1, size = 2) +\n  geom_hline(yintercept=0.5, linetype=\"dashed\", color = \"red\") +\n  theme_bw()\n\nWarning message:\n\"`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\"\n\n\n\n\n\nLets check the same for test data\nFirst we need to predict values:\n\ntest_data$predicted <- predict(lm_bike, newdata = test_data)\nhead(test_data)\n\n\n\nA data.frame: 6 × 12\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rentpredicted\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl><dbl>\n\n\n    12012-05-16 22:00:00201126.2430.3057311.001428622279.8139\n    22012-05-16 23:00:00201125.4229.54578 7.001513323271.5277\n    32012-05-17 00:00:00201124.6028.79078 8.9981 79 0122.0506\n    42012-05-17 01:00:00201124.6028.0308311.0014 28 1121.0080\n    52012-05-17 02:00:00201124.6028.7907811.0014 16 2134.5991\n    62012-05-17 03:00:00201124.6029.5457316.9979  3 3148.5908\n\n\n\n\n\n# resulduals calculation\ntest_data$residuals <- test_data$count - test_data$predicted\nhead(test_data)\n\n\n\nA data.frame: 6 × 13\n\n    datetimeseasonholidayworkingdayweathertempatemphumiditywindspeedcounthour_rentpredictedresiduals\n    <dttm><fct><fct><fct><fct><dbl><dbl><int><dbl><int><dbl><dbl><dbl>\n\n\n    12012-05-16 22:00:00201126.2430.3057311.001428622279.8139   6.186142\n    22012-05-16 23:00:00201125.4229.54578 7.001513323271.5277-138.527654\n    32012-05-17 00:00:00201124.6028.79078 8.9981 79 0122.0506 -43.050553\n    42012-05-17 01:00:00201124.6028.0308311.0014 28 1121.0080 -93.008024\n    52012-05-17 02:00:00201124.6028.7907811.0014 16 2134.5991-118.599054\n    62012-05-17 03:00:00201124.6029.5457316.9979  3 3148.5908-145.590844\n\n\n\n\n\nplot_data <- test_data[1:100,]\n\n\nggplot(plot_data, aes(x = c(1:nrow(plot_data)), y = as.numeric(count))) +\n  geom_segment(aes(xend = c(1:nrow(plot_data)), yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals), size = 2) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 1, size = 2) +\n  geom_hline(yintercept=0.5, linetype=\"dashed\", color = \"red\") +\n  theme_bw()\n\nWarning message:\n\"`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\"\n\n\n\n\n\n\nlibrary(modelr)\nresids <- data.frame(\n  R2 = c(rsquare(lm_bike, data = train_data), rsquare(lm_bike, data = test_data)),\n  MSE = c(mse(lm_bike, data = train_data),mse(lm_bike, data = test_data)),\n  RMSE = c(rmse(lm_bike, data = train_data), rmse(lm_bike, data = test_data))#,\n  #MAE = c(mae(lm_bike, data = train_data), mae(lm_bike, data = test_data)), # if needed \n  #MAPE = c(mape(lm_bike, data = train_data), mape(lm_bike, data = test_data)) # if needed\n)\n          \nrownames(resids) <- c(\"train\", \"test\") # give names for rows\n\nresids\n\n\n\nA data.frame: 2 × 3\n\n    R2MSERMSE\n    <dbl><dbl><dbl>\n\n\n    train0.342244315022.59122.5667\n    test0.285474140401.94201.0023\n\n\n\n\nConclusion: Test set has less RSquared and Bigger Errors variation. Train dataset is closer, because it built on training data and model include information about data."
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html",
    "href": "033-supervised-learning-desicion-trees-regression.html",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "",
    "text": "Прикладне математичне моделювання в R\nУ даній частині навчального процесу потрібно побудувати математичні моделі регресії клієнтів на основі алгоритму дерева рішень та перевірити їх на тестовій вибірці."
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#підготовка",
    "href": "033-supervised-learning-desicion-trees-regression.html#підготовка",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.1 Підготовка",
    "text": "4.1 Підготовка\n\n# install.packages(\"DMwR\")\n# install.packages(\"gmodels\")\n# install.packages(\"rpart\")\n# install.packages(\"partykit\")\n# install.packages(\"rpart.plot\")\n# install.packages(\"RColorBrewer\")\n# install.packages(\"rattle\")\n\n\nSys.setlocale(\"LC_CTYPE\", \"ukrainian\") \noptions(warn = -1)\n\n'Ukrainian_Ukraine.1251'\n\n\nДжерела:\n\nAn Introduction to Statistical Learning with Applications in R http://www-bcf.usc.edu/~gareth/ISL/data.html\nPredicting Credit Card Balance using Regression https://www.kaggle.com/suzanaiacob/predicting-credit-card-balance-using-regression/notebook\n\nДля виконнання завдань потрібний ряд R-пакетів:\n\nsuppressMessages(library(ggplot2)) # побудова графіків\nsuppressMessages(library(gmodels)) # побудова крос-таблиць\nsuppressMessages(library(rpart)) # дерево рішень\nsuppressMessages(library(partykit)) # дерево рішень\nsuppressMessages(library(rpart.plot)) # візуалізація дерева рішень\nsuppressMessages(library(DMwR)) # оцінка похибок моделі\nsuppressMessages(library(rattle)) # допомога у візуалізації \nsuppressMessages(library(RColorBrewer)) # допомога у візуалізації \n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\nЯкщо ці пакети відсутні, то інсталюйте їх за допомогою команди install.packages(назва_пакету).\nДля очистки сесії від непотрібних даних використайте команду rm():\nrm(list = ls()) #видаляє усі змінні"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#набір-даних",
    "href": "033-supervised-learning-desicion-trees-regression.html#набір-даних",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.2 Набір даних",
    "text": "4.2 Набір даних\nДані складають з 400 спостежень та наступник пкоазників:\n\nID - ідентифікатор;\nIncome - дохід у $10,0000;\nLimit - кредитний ліміт;\nRating - кредитний рейтинг;\nAge - вік, роки;\nEducation - освіта, кількість років навчання;\nGender - стать (Male or Female);\nStudent - флаг чи є студентом (Yes or No);\nMarried - флаг чи одружений (Yes or No);\nEthnicity - етнічна належність (African American, Asian or Caucasian);\nBalance - середній баланс по карті у $$.\n\nЗадача: визначити вплив факторів на середній баланс по карті.\nІмпорт даних:\n\ndata <- read.csv(\"data/credit_card_balance.csv\")\n\nПереглянемо структуру даних:\n\nstr(data)\n\n'data.frame':   400 obs. of  12 variables:\n $ X        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Income   : num  14.9 106 104.6 148.9 55.9 ...\n $ Limit    : int  3606 6645 7075 9504 4897 8047 3388 7114 3300 6819 ...\n $ Rating   : int  283 483 514 681 357 569 259 512 266 491 ...\n $ Cards    : int  2 3 4 3 2 4 2 2 5 3 ...\n $ Age      : int  34 82 71 36 68 77 37 87 66 41 ...\n $ Education: int  11 15 11 11 16 10 12 9 13 19 ...\n $ Gender   : chr  \"Male\" \"Female\" \"Male\" \"Female\" ...\n $ Student  : chr  \"No\" \"Yes\" \"No\" \"No\" ...\n $ Married  : chr  \"Yes\" \"Yes\" \"No\" \"No\" ...\n $ Ethnicity: chr  \"Caucasian\" \"Asian\" \"Asian\" \"Asian\" ...\n $ Balance  : int  333 903 580 964 331 1151 203 872 279 1350 ...\n\n\n\nhead(data)\n\n\n\nA data.frame: 6 × 12\n\n    XIncomeLimitRatingCardsAgeEducationGenderStudentMarriedEthnicityBalance\n    <int><dbl><int><int><int><int><int><chr><chr><chr><chr><int>\n\n\n    11 14.891360628323411Male  No YesCaucasian 333\n    22106.025664548338215FemaleYesYesAsian     903\n    33104.593707551447111Male  No No Asian     580\n    44148.924950468133611FemaleNo No Asian     964\n    55 55.882489735726816Male  No YesCaucasian 331\n    66 80.180804756947710Male  No No Caucasian1151\n\n\n\n\nОглянемо описову статистику факторів:\n\nsummary(data)\n\n       X             Income           Limit           Rating     \n Min.   :  1.0   Min.   : 10.35   Min.   :  855   Min.   : 93.0  \n 1st Qu.:100.8   1st Qu.: 21.01   1st Qu.: 3088   1st Qu.:247.2  \n Median :200.5   Median : 33.12   Median : 4622   Median :344.0  \n Mean   :200.5   Mean   : 45.22   Mean   : 4736   Mean   :354.9  \n 3rd Qu.:300.2   3rd Qu.: 57.47   3rd Qu.: 5873   3rd Qu.:437.2  \n Max.   :400.0   Max.   :186.63   Max.   :13913   Max.   :982.0  \n     Cards            Age          Education        Gender         \n Min.   :1.000   Min.   :23.00   Min.   : 5.00   Length:400        \n 1st Qu.:2.000   1st Qu.:41.75   1st Qu.:11.00   Class :character  \n Median :3.000   Median :56.00   Median :14.00   Mode  :character  \n Mean   :2.958   Mean   :55.67   Mean   :13.45                     \n 3rd Qu.:4.000   3rd Qu.:70.00   3rd Qu.:16.00                     \n Max.   :9.000   Max.   :98.00   Max.   :20.00                     \n   Student            Married           Ethnicity            Balance       \n Length:400         Length:400         Length:400         Min.   :   0.00  \n Class :character   Class :character   Class :character   1st Qu.:  68.75  \n Mode  :character   Mode  :character   Mode  :character   Median : 459.50  \n                                                          Mean   : 520.01  \n                                                          3rd Qu.: 863.00  \n                                                          Max.   :1999.00  \n\n\nПідготуємо дані до моделювання. Перетворимо категоріальні показники до факторів:\n\ndata$X <- NULL\ndata$Gender <-  factor(data$Gender)\ndata$Student <- factor(data$Student)\ndata$Married <- factor(data$Married)\ndata$Ethnicity <- factor(data$Ethnicity)\n\n\n\n\n\n\n\nNote\n\n\n\nСпростіть код, використавши можливості пакету dplyr."
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#тренувальна-та-тестова-вибірки",
    "href": "033-supervised-learning-desicion-trees-regression.html#тренувальна-та-тестова-вибірки",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.3 Тренувальна та тестова вибірки",
    "text": "4.3 Тренувальна та тестова вибірки\nРозділимо загальну вибірку на 2 частини: * тренувальна, 70% вибірки, для побудови регресії; * тестова, 30% вибірки, для перевірки точності моделі.\n\nset.seed(2023) #довільне число як точка \"відправки\" для генератора випадкових чисел\n\n# Згенеруємо набір чисел від 1 до кількості спостережень у вибірці і відберемо  випадквоим чином 70% із них\ntrain_index <- sample(1:nrow(data), size = 0.7*nrow(data))\n\n#Запишемо по номерах відібраних рядків тренувальний набір даних\ntrain_data <- data[train_index,]\n\n#Всі інші значення, що не увійшли в тренувальну вибірку запишемо у тестову\ntest_data <- data[-train_index,]\n\nПереглянемо наявність зв’язків між числовими параметрами для тренувальної вибірки за допомогою матриці попарних кореляцій/ Дані на перетині рядків вказують на кореляцію між вибраними показниками.\n\ncor(train_data[, -c(7:10)])\n\n\n\nA matrix: 7 × 7 of type dbl\n\n    IncomeLimitRatingCardsAgeEducationBalance\n\n\n    Income1.000000000.797412690.79561996 0.017137980.14965311 0.001772920.49497408\n    Limit0.797412691.000000000.99694968 0.045597840.10507642 0.016592740.87729947\n    Rating0.795619960.996949681.00000000 0.089412760.10494558 0.012338530.87873569\n    Cards0.017137980.045597840.08941276 1.000000000.03596854-0.051421400.09550304\n    Age0.149653110.105076420.10494558 0.035968541.00000000 0.071655030.03652931\n    Education0.001772920.016592740.01233853-0.051421400.07165503 1.000000000.04109140\n    Balance0.494974080.877299470.87873569 0.095503040.03652931 0.041091401.00000000"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#оглядовий-аналіз-даних",
    "href": "033-supervised-learning-desicion-trees-regression.html#оглядовий-аналіз-даних",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.4 Оглядовий аналіз даних",
    "text": "4.4 Оглядовий аналіз даних\nДля початку переглянемо категоріальні змінні.\n\n# Потбірні пакети\nlibrary(ggplot2)\nlibrary(gmodels)\n\nСтать (Gender):\n\nggplot(train_data, aes(Gender)) + \n    geom_bar(aes(fill = Gender)) +\n    theme_bw()\n\n\n\n\n\nCrossTable(train_data$Gender)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n          |    Female |      Male | \n          |-----------|-----------|\n          |       149 |       131 | \n          |     0.532 |     0.468 | \n          |-----------|-----------|\n\n\n\n \n\n\nСімейний стан (Married):\n\nggplot(train_data, aes(Married)) + geom_bar(aes(fill = Married)) + theme_bw()\n\n\n\n\n\nCrossTable(train_data$Married)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |       101 |       179 | \n          |     0.361 |     0.639 | \n          |-----------|-----------|\n\n\n\n \n\n\nСтудент:\n\nggplot(train_data, aes(Student)) + geom_bar(aes(fill = Student)) + theme_bw()\n\n\n\n\n\nCrossTable(train_data$Student)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n          |        No |       Yes | \n          |-----------|-----------|\n          |       255 |        25 | \n          |     0.911 |     0.089 | \n          |-----------|-----------|\n\n\n\n \n\n\nЕтнічна належність:\n\nggplot(train_data, aes(Ethnicity)) + geom_bar(aes(fill = Ethnicity)) + theme_bw()\n\n\n\n\n\nCrossTable(train_data$Ethnicity)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  280 \n\n \n                 | African American |            Asian |        Caucasian | \n                 |------------------|------------------|------------------|\n                 |               67 |               65 |              148 | \n                 |            0.239 |            0.232 |            0.529 | \n                 |------------------|------------------|------------------|\n\n\n\n \n\n\nПорівняємо числові змінні з показником середнього балансу.\nГрафік залежності між доходом (Income) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Income, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік залежності між доходом (Rating) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Rating, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік залежності між кількістю карт (Cards) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Cards, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік залежності між віком (Age) та середнім балансом по карті (Balance):\n\nggplot(train_data, aes(Education, Balance)) + geom_point()  + theme_bw()\n\n\n\n\nГрафік розподілу значень балансу:"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-rpart",
    "href": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-rpart",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.5 Моделювання. Робота з пакетом RPart",
    "text": "4.5 Моделювання. Робота з пакетом RPart\n\n4.5.1 Побудова моделі без налаштувань\nДля побудови регресії на основі дерева рішень інсталюємо пакет rpart (install.package(\"rpart\")).\nВикористаємо функцію для побудови моделі rpart():\n\n#library(rpart)\nrpart_model <- rpart(Balance ~ ., train_data)\n\nСтворимо дата-фрейми для запису результатів моделювання на тестовій та тренувальній вибірках:\n\ntrain_res <- data.frame(No = c(1:nrow(train_data)), \n                        Balance = train_data$Balance, \n                        RPartPredictedDef = predict(rpart_model, train_data))\n\ntest_res <- data.frame(No = c(1:nrow(test_data)),\n                       Balance = test_data$Balance, \n                       RPartPredictedDef = predict(rpart_model, test_data))\n\nhead(train_res)\n\n\n\nA data.frame: 6 × 3\n\n    NoBalanceRPartPredictedDef\n    <int><int><dbl>\n\n\n    3731840453.17778\n    3612712891.97561\n    2433 16 64.26136\n    2824  0 64.26136\n    445976891.97561\n    3546425453.17778\n\n\n\n\n\n\n4.5.2 Оцінка метрик\nПереглянемо похибки моделі на обох вибірках:\n\n#library(DMwR)\nregr.eval(train_res$Balance, train_res$RPartPredictedDef)\nregr.eval(test_res$Balance, test_res$RPartPredictedDef)\n\nmae119.689257298022mse26183.8847433001rmse161.814352711062mapeInf\n\n\nmae159.674646317362mse49034.8928845991rmse221.438237178223mapeInf\n\n\nОцінимо точність моделі за допомогою коефіцієнта детермінації:\n\nr_train <- cor(train_res$Balance, train_res$RPartPredictedDef)^2\nr_test <- cor(test_res$Balance, test_res$RPartPredictedDef)^2\nprint(paste(\"R_train = \", r_train, sep = \"\"))\nprint(paste(\"R_test = \", r_test, sep = \"\"))\n\n[1] \"R_train = 0.872679719680806\"\n[1] \"R_test = 0.780363587175022\"\n\n\nДля покращення візуалізації відсортуємо результати:\n\n# відсортуємо за зростанням значення балансів карт\nordered_train_res <- train_res[order(train_res$Balance),]\n# відсортуємо за зростанням значення модельованого значення балансу\nordered_train_res <- train_res[order(train_res$RPartPredictedDef),]\n# \"Перепишемо\" номери по порядку\nordered_train_res$No <- c(1:nrow(train_res))\n\nПобудуємо графік модельованих та реальних значень балансу з відсортованими показниками для наочності:\n\nggplot(ordered_train_res) +\n  geom_point(aes(x = No, y = Balance), colour = \"blue\") +\n  geom_line(aes(x = No, y = RPartPredictedDef), colour = \"red\", size = 1) + theme_bw()\n\n\n\n\nПереглянемо залежність між рейтингом та балансом клієнта, а також прогнозованих значеннях.\n\nggplot(train_data) +\n  geom_point(aes(x = Rating, y = Balance), colour = \"blue\") +\n  geom_line(aes(x = Rating, y = train_res$RPartPredictedDef), colour = \"red\") + theme_bw()\n\n\n\n\n\n\n4.5.3 Візуальне представлення\nДля візуалізації дерева рішень скористаємося пакетом rpart.plot:\n\n#library(rpart.plot)\nprp(rpart_model)\n\n\n\n\n\nprp(rpart_model, extra = 1, type = 2)\n\n\n\n\nДодамо інтерактивності для побудованого дерева рішень:\n\nprp(rpart_model, snip = TRUE) # Працює у RStudio\n\n\n\n\nПобудуємо також “розфарбоване” дерево рішень за допомогою пакетів rattle та RColorBrewer:\n\n#library(rattle)\n#library(RColorBrewer)\nfancyRpartPlot(rpart_model)\n\n\n\n\n\n\n\n4.5.4 Побудова моделі з розділеннями\nПобудуємо модель з вказанням мінімальної кількості розділень даних (minsplit):\n\nrpart_model2 <- rpart(Balance ~ ., train_data, control = rpart.control(minsplit = 10))\n\nОтримаємо прогнозовані значення для обох вибірок на основі другої моделі:\n\ntrain_res$RPartPredicted10 <- predict(rpart_model2, train_data)\ntest_res$RPartPredicted10 <- predict(rpart_model2, test_data)\n\nhead(train_res)\n\n\n\nA data.frame: 6 × 4\n\n    NoBalanceRPartPredictedDefRPartPredicted10\n    <int><int><dbl><dbl>\n\n\n    3731840453.17778836.25000\n    3612712891.97561891.97561\n    2433 16 64.26136 64.26136\n    2824  0 64.26136 64.26136\n    445976891.97561891.97561\n    3546425453.17778415.80488\n\n\n\n\n\n\n4.5.5 Оцінка метрик\nПереглянемо похибки:\n\nregr.eval(train_res$Balance, train_res$RPartPredicted10)\nregr.eval(test_res$Balance, test_res$RPartPredicted10)\n\nmae115.002097019276mse23883.0154977523rmse154.541306768619mapeInf\n\n\nmae152.524889090623mse43219.8819877103rmse207.89392003546mapeInf\n\n\nОцінимо точність моделі за допомогою коефіцієнта детермінації:\n\nr_train <- cor(train_res$Balance, train_res$RPartPredicted10)^2\nr_test <- cor(test_res$Balance, test_res$RPartPredicted10)^2\n\nprint(paste(\"R_train =\", r_train, sep = \"\"))\nprint(paste(\"R_test =\", r_test, sep = \"\"))\n\n[1] \"R_train =0.883867796629392\"\n[1] \"R_test =0.805983358303455\"\n\n\n\n\n4.5.6 Візуалізація результатів\nВізуалізуємо модель:\n\nprp(rpart_model2)"
  },
  {
    "objectID": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-partykit",
    "href": "033-supervised-learning-desicion-trees-regression.html#моделювання.-робота-з-пакетом-partykit",
    "title": "4  Дерева рішень. Регресія. Баланс кредитної карти",
    "section": "4.6 Моделювання. Робота з пакетом partykit",
    "text": "4.6 Моделювання. Робота з пакетом partykit\nПобудуємо дерево рішень на основі partykit та функції ctree() та порівняємо з результатами роботи rpart().\n\n#library(partykit)\nparty_model <- ctree(Balance ~ ., data = train_data[1:100,]) # перших 100 спостережень\nprint(party_model)\n\n\nModel formula:\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married + Ethnicity\n\nFitted party:\n[1] root\n|   [2] Rating <= 342\n|   |   [3] Rating <= 245: 3.360 (n = 25, err = 4597.8)\n|   |   [4] Rating > 245\n|   |   |   [5] Income <= 19.782: 416.200 (n = 10, err = 290573.6)\n|   |   |   [6] Income > 19.782: 221.889 (n = 18, err = 254079.8)\n|   [7] Rating > 342\n|   |   [8] Rating <= 599: 765.051 (n = 39, err = 1913599.9)\n|   |   [9] Rating > 599: 1380.625 (n = 8, err = 720389.9)\n\nNumber of inner nodes:    4\nNumber of terminal nodes: 5\n\n\n\nplot(party_model)\n\n\n\n\n\nparty_model <- ctree(Balance ~ ., data = train_data) \nprint(party_model)\n\n\nModel formula:\nBalance ~ Income + Limit + Rating + Cards + Age + Education + \n    Gender + Student + Married + Ethnicity\n\nFitted party:\n[1] root\n|   [2] Rating <= 353\n|   |   [3] Rating <= 278\n|   |   |   [4] Student in No\n|   |   |   |   [5] Rating <= 249: 8.516 (n = 62, err = 64369.5)\n|   |   |   |   [6] Rating > 249: 167.056 (n = 18, err = 91486.9)\n|   |   |   [7] Student in Yes: 265.000 (n = 8, err = 223014.0)\n|   |   [8] Rating > 278\n|   |   |   [9] Income <= 44.978: 453.178 (n = 45, err = 1330364.6)\n|   |   |   [10] Income > 44.978: 104.273 (n = 11, err = 45162.2)\n|   [11] Rating > 353\n|   |   [12] Rating <= 682\n|   |   |   [13] Limit <= 5310\n|   |   |   |   [14] Income <= 48.218: 717.778 (n = 27, err = 670882.7)\n|   |   |   |   [15] Income > 48.218: 352.583 (n = 12, err = 243564.9)\n|   |   |   [16] Limit > 5310\n|   |   |   |   [17] Student in No\n|   |   |   |   |   [18] Income <= 101.788\n|   |   |   |   |   |   [19] Rating <= 536\n|   |   |   |   |   |   |   [20] Income <= 63.809\n|   |   |   |   |   |   |   |   [21] Rating <= 456\n|   |   |   |   |   |   |   |   |   [22] Income <= 34.48: 934.222 (n = 18, err = 95773.1)\n|   |   |   |   |   |   |   |   |   [23] Income > 34.48: 808.000 (n = 7, err = 45006.0)\n|   |   |   |   |   |   |   |   [24] Rating > 456: 1028.533 (n = 15, err = 106875.7)\n|   |   |   |   |   |   |   [25] Income > 63.809: 734.714 (n = 14, err = 332820.9)\n|   |   |   |   |   |   [26] Rating > 536: 1173.250 (n = 12, err = 307664.2)\n|   |   |   |   |   [27] Income > 101.788: 667.545 (n = 11, err = 816958.7)\n|   |   |   |   [28] Student in Yes: 1160.778 (n = 9, err = 523847.6)\n|   |   [29] Rating > 682: 1584.909 (n = 11, err = 510442.9)\n\nNumber of inner nodes:    14\nNumber of terminal nodes: 15\n\n\n\nplot(party_model)\n\n\n\n\nОбчислимо модельовані значення середнього балансу по карті:\n\ntrain_res$PredictedPartyDef <- predict(party_model, train_data)\ntest_res$PredictedPartyDef <- predict(party_model, test_data)\n\nПереглянемо коефіцієнти детермінації:\n\nr_train <- cor(train_res$Balance, train_res$PredictedPartyDef)^2\nr_test <- cor(test_res$Balance, test_res$PredictedPartyDef)^2\n\nprint(paste(\"R_train = \", r_train, sep = \"\"))\nprint(paste(\"R_test = \", r_test, sep = \"\"))\n\n[1] \"R_train = 0.906079382030953\"\n[1] \"R_test = 0.857313458113989\""
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html",
    "href": "034-supervised-learning-neutal-networks-regression.html",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "",
    "text": "Курс: “Математичне моделювання в R”\nУ цій лекції ми скористаємося набором даних Boston: вартість житла у пригородах Бостона"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#dataset-description",
    "href": "034-supervised-learning-neutal-networks-regression.html#dataset-description",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.1 Dataset description",
    "text": "5.1 Dataset description\n\n# install.packages(\"MASS\")\n# install.packages(\"neuralnet\")\n# install.packages(\"clusterGeneration\")\n# install.packages(\"devtools\")\n# install.packages(\"caTools\")\n# install.packages(\"reshape\")\n# install.packages(\"Metrics\")\n# install.packages(\"nnet\")\n#install.packages(\"plyr\")\n#install.packages(\"boot\")\n\nmedv is TARGET!\n\nlibrary(MASS)\n?Boston\n\n\n\nBoston {MASS}R Documentation\n\n\nHousing Values in Suburbs of Boston\n\n\nDescription\n\nThe Boston data frame has 506 rows and 14 columns.\n\n\n\nUsage\n\nBoston\n\n\n\nFormat\n\nThis data frame contains the following columns:\n\n\n\ncrim\nper capita crime rate by town.\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\n\nindus\nproportion of non-retail business acres per town.\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\n\nnox\nnitrogen oxides concentration (parts per 10 million).\n\n\nrm\naverage number of rooms per dwelling.\n\n\nage\nproportion of owner-occupied units built prior to 1940.\n\n\ndis\nweighted mean of distances to five Boston employment centres.\n\n\nrad\nindex of accessibility to radial highways.\n\n\ntax\nfull-value property-tax rate per $10,000.\n\n\nptratio\npupil-teacher ratio by town.\n\n\nblack\n1000(Bk - 0.63)^2 where Bk is the proportion of blacks\nby town.\n\n\nlstat\nlower status of the population (percent).\n\n\nmedv\nmedian value of owner-occupied homes in $1000s.\n\n\n\n\n\n\nSource\n\nHarrison, D. and Rubinfeld, D.L. (1978)\nHedonic prices and the demand for clean air.\nJ. Environ. Economics and Management\n5, 81–102.\n\nBelsley D.A., Kuh, E.  and Welsch, R.E. (1980)\nRegression Diagnostics. Identifying Influential Data and Sources\nof Collinearity.\nNew York: Wiley.\n\n\n[Package MASS version 7.3-58.1 ]\n\n\nПереглянемо дані:\n\nhead(Boston)\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00632182.3100.5386.57565.24.0900129615.3396.904.9824.0\n    20.02731 07.0700.4696.42178.94.9671224217.8396.909.1421.6\n    30.02729 07.0700.4697.18561.14.9671224217.8392.834.0334.7\n    40.03237 02.1800.4586.99845.86.0622322218.7394.632.9433.4\n    50.06905 02.1800.4587.14754.26.0622322218.7396.905.3336.2\n    60.02985 02.1800.4586.43058.76.0622322218.7394.125.2128.7\n\n\n\n\n\nstr(Boston)\n\n'data.frame':   506 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ..."
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#data-visualization",
    "href": "034-supervised-learning-neutal-networks-regression.html#data-visualization",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.2 Data visualization",
    "text": "5.2 Data visualization\nLets move our dataset to special variable:\n\ndata <- Boston\nhead(data)\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00632182.3100.5386.57565.24.0900129615.3396.904.9824.0\n    20.02731 07.0700.4696.42178.94.9671224217.8396.909.1421.6\n    30.02729 07.0700.4697.18561.14.9671224217.8392.834.0334.7\n    40.03237 02.1800.4586.99845.86.0622322218.7394.632.9433.4\n    50.06905 02.1800.4587.14754.26.0622322218.7396.905.3336.2\n    60.02985 02.1800.4586.43058.76.0622322218.7394.125.2128.7\n\n\n\n\nLet’s check a correlation between parameters:\n\nsuppressMessages(library(corrplot))\ncorrplot(cor(data))\n\n\n\n\nRad and tax has highest correlation: 0.91\n\nrad - index of accessibility to radial highways.\ntax - full-value property-tax rate per $10,000.\n\n\nlibrary(ggplot2)\n\nggplot(data, aes(medv)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n#crim\nggplot(data, aes(crim)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n#zn\nggplot(data, aes(zn)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n#indus\nggplot(data, aes(indus)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n#chas \nggplot(data, aes(chas)) + \n    geom_bar(aes(fill = chas)) + \n    theme_bw()\n\n\n\n\n\n# nox\nggplot(data, aes(nox)) + \n    geom_histogram(bins = 10, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# rm\nggplot(data, aes(rm)) + \n    geom_histogram(bins = 10, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# age\nggplot(data, aes(age)) + \n    geom_histogram(bins = 15, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# dis\nggplot(data, aes(dis)) + \n    geom_histogram(bins = 15, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n#rad\nggplot(data, aes(ptratio)) + \n    geom_histogram(bins = 10, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# black\nggplot(data, aes(black)) + \n    geom_histogram(bins = 10, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# lstat\nggplot(data, aes(lstat)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# nox\nggplot(data, aes(nox)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#data-scaling",
    "href": "034-supervised-learning-neutal-networks-regression.html#data-scaling",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.3 Data scaling",
    "text": "5.3 Data scaling\nДля роботи з нейромережами хорошою практикою є нормалізація даних перед використанням. Скористаємося формулою \\((X-Xmin)/(Xmax-Xmin)\\).\nВизначимо мінімальні та максимальні значення по факторах:\n\nsuppressMessages(library(dplyr))\n\n\nnormalizeData <- function(x) {\n    return ((x - min(x)) / (max(x) - min(x)))\n}\n\n\nrevertData <- function(scaled, original) {\n    return (scaled * (max(original) - min(original)) + min(original))    \n}\n\n\ndata %>% head()\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><int><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00632182.3100.5386.57565.24.0900129615.3396.904.9824.0\n    20.02731 07.0700.4696.42178.94.9671224217.8396.909.1421.6\n    30.02729 07.0700.4697.18561.14.9671224217.8392.834.0334.7\n    40.03237 02.1800.4586.99845.86.0622322218.7394.632.9433.4\n    50.06905 02.1800.4587.14754.26.0622322218.7396.905.3336.2\n    60.02985 02.1800.4586.43058.76.0622322218.7394.125.2128.7\n\n\n\n\nНормалізуємо значення за один раз у всьому датафреймі.\nПереглянемо як змінився вигляд значень:\n\nscaled_data <- sapply(X = data, FUN = normalizeData) %>% as.data.frame() \nscaled_data %>% head()\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00000000000.180.0678152500.31481480.57750530.64160660.26920310.000000000.208015270.28723401.00000000.089679910.4222222\n    20.00023592250.000.2423020500.17283950.54799770.78269820.34896200.043478260.104961830.55319151.00000000.204470200.3688889\n    30.00023569770.000.2423020500.17283950.69438590.59938210.34896200.043478260.104961830.55319150.98973730.063465780.6600000\n    40.00029279570.000.0630498500.15020580.65855530.44181260.44854460.086956520.066793890.64893620.99427610.033388520.6311111\n    50.00070507010.000.0630498500.15020580.68710480.52832130.44854460.086956520.066793890.64893621.00000000.099337750.6933333\n    60.00026447150.000.0630498500.15020580.54972220.57466530.44854460.086956520.066793890.64893620.99299010.096026490.5266667\n\n\n\n\n\nre_data <- sapply(X = scaled_data$medv, original = data$medv, FUN = revertData) %>% as.data.frame() \nre_data %>% head()\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    124.0\n    221.6\n    334.7\n    433.4\n    536.2\n    628.7"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#traintest-split",
    "href": "034-supervised-learning-neutal-networks-regression.html#traintest-split",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.4 Train/test split",
    "text": "5.4 Train/test split\nСформуємо вибірки з пропорцією 70/30 % значень:\n\nlibrary(caTools)\nset.seed(2023)\nsplit <- sample.split(scaled_data$medv, SplitRatio = 0.7)\ntrain_data <- subset(scaled_data, split)\ntest_data <- subset(scaled_data, !split)\n\nLet’s check how target’s are distrubuted in both samples:\n\n# train\nggplot(train_data, aes(medv)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\n\n# test\nggplot(test_data, aes(medv)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#baseline-linear-regression",
    "href": "034-supervised-learning-neutal-networks-regression.html#baseline-linear-regression",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.5 Baseline (Linear regression)",
    "text": "5.5 Baseline (Linear regression)\nДля порівняння ефективності використання нейронних мереж скористаємося для початку лінійною регресією:\n\nlm_model <- lm(medv ~ ., data = train_data)\nsummary(lm_model)\n\n\nCall:\nlm(formula = medv ~ ., data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24556 -0.06405 -0.01292  0.04494  0.59898 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.49370    0.06281   7.860 4.70e-14 ***\ncrim        -0.14725    0.15139  -0.973 0.331410    \nzn           0.08336    0.03588   2.323 0.020751 *  \nindus        0.01499    0.04469   0.335 0.737541    \nchas         0.01520    0.02281   0.667 0.505425    \nnox         -0.18674    0.05028  -3.714 0.000237 ***\nrm           0.48617    0.05571   8.727  < 2e-16 ***\nage         -0.01314    0.03207  -0.410 0.682344    \ndis         -0.36140    0.05961  -6.063 3.43e-09 ***\nrad          0.14625    0.04369   3.347 0.000905 ***\ntax         -0.14304    0.05333  -2.682 0.007655 ** \nptratio     -0.21909    0.03199  -6.850 3.30e-11 ***\nblack        0.07181    0.02898   2.478 0.013692 *  \nlstat       -0.41460    0.04909  -8.446 7.94e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1058 on 353 degrees of freedom\nMultiple R-squared:  0.7566,    Adjusted R-squared:  0.7477 \nF-statistic: 84.42 on 13 and 353 DF,  p-value: < 2.2e-16\n\n\nЗдійснимо прогноз тестових значень:\n\ntest_lm_predicted_scaled <- predict(lm_model, test_data)\nhead(test_lm_predicted_scaled)\n\n10.5679243174097250.523111676025518200.295073007902457290.32360237024043300.355931690861336350.195998476728651\n\n\n\ntest_lm_predicted <- sapply(X = test_lm_predicted_scaled, original = data$medv, FUN = revertData) %>% as.data.frame()\nhead(test_lm_predicted)\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    130.55659\n    528.54003\n    2018.27829\n    2919.56211\n    3021.01693\n    3513.81993\n\n\n\n\nLet’s check errors and R^2:\n\nlibrary(modelr)\n# need for next comparison\nlinear_err <- data.frame(  \n  R2_train = round(modelr::rsquare(lm_model, data = train_data), 4),\n  R2_test = round(modelr::rsquare(lm_model, data = test_data), 4),\n  MSE_test = round(modelr::mse(lm_model, data = test_data), 4),\n  RMSE_test = round(modelr::rmse(lm_model, data = test_data), 4) \n)\n\nrownames(linear_err) <- c(\"linear\")\nlinear_err\n\n\n\nA data.frame: 1 × 4\n\n    R2_trainR2_testMSE_testRMSE_test\n    <dbl><dbl><dbl><dbl>\n\n\n    linear0.75660.66110.01170.1081"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#побудова-нейромережі-за-допомогою-neuralnet",
    "href": "034-supervised-learning-neutal-networks-regression.html#побудова-нейромережі-за-допомогою-neuralnet",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.6 Побудова нейромережі за допомогою neuralnet",
    "text": "5.6 Побудова нейромережі за допомогою neuralnet\nПідключаємо пакет neuralnet:\n\nsuppressMessages(library(neuralnet))\n\n\nAttaching package: 'neuralnet'\n\n\nThe following object is masked from 'package:dplyr':\n\n    compute\n\n\n\n\nДля побудови моделі потрібно згенерувати формулу у форматі \\(y ~ x_1 + x_2 + ... + x_n\\)\n\nn <- colnames(data)\nn <- n[!n %in% \"medv\"]\nformula <- as.formula(paste(\"medv ~\", paste(n, collapse = \" + \")))\nformula\n\nmedv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat\n\n\nБудуємо модель за допомогою фунуції neuralnet():\nhidden = c(3,4) - перший прихований шар буде мати 3 нейрони, другий 4 linear.output - вихідний показник неперервне число. Не класифікація\n\nnn_model <- neuralnet(formula = formula, data = train_data, hidden = c(3,4), linear.output = TRUE, rep = 1)\n\nВізуалізуємо модель:\n\nplot(nn_model)\n\n\ntrain_data %>% head()\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    20.00023592250.0000.2423020500.17283950.54799770.78269820.34896200.043478260.104961830.55319151.00000000.204470200.3688889\n    30.00023569770.0000.2423020500.17283950.69438590.59938210.34896200.043478260.104961830.55319150.98973730.063465780.6600000\n    40.00029279570.0000.0630498500.15020580.65855530.44181260.44854460.086956520.066793890.64893620.99427610.033388520.6311111\n    60.00026447150.0000.0630498500.15020580.54972220.57466530.44854460.086956520.066793890.64893620.99299010.096026490.5266667\n    70.00092132300.1250.2716275700.28600820.46963020.65602470.40292260.173913040.236641220.27659570.99672200.295253860.3977778\n    80.00155367190.1250.2716275700.28600820.50028740.95983520.43838720.173913040.236641220.27659571.00000000.480684330.4911111\n\n\n\n\nТакож для візуалізація можна скористатися функцією plot.nnet(), опубілкованою на відкритому ресурсі одним із користувачів мережі Інтернет:\n\nsuppressMessages(library(devtools))\n\n\nlibrary(clusterGeneration)\n\n\nsuppressMessages(source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r'))\n\n\nsuppressMessages(plot.nnet(nn_model))\n\n\n\n\nPreview neural network matrix as text:\n\nnn_model$result.matrix %>% head()\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    error 5.182582e-01\n    reached.threshold 9.322212e-03\n    steps 3.503000e+03\n    Intercept.to.1layhid1 5.249092e-01\n    crim.to.1layhid1-2.880638e+00\n    zn.to.1layhid1-9.485175e+01\n\n\n\n\nЗдійснимо прогноз для тестової вибірки та повернемо значення до базового виміру:\n\ntest_predicted_scaled <- compute(nn_model, test_data%>%select(-medv))\n\nConvert $net.result to original form:\n\ntest_nn_predicted <- sapply(X = test_predicted_scaled$net.result, original = data$medv, FUN = revertData) %>% as.data.frame()\nhead(test_nn_predicted)\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    128.74649\n    231.34932\n    318.86595\n    418.92882\n    519.89660\n    611.69719\n\n\n\n\nПорівняємо похибки та \\(R^2\\) по лінійній регресії та першій нейронній мережі:\n\nhead(test_data)\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00000000000.180.0678152500.31481480.57750530.64160660.26920310.000000000.208015270.28723401.00000000.089679910.4222222\n    50.00070507010.000.0630498500.15020580.68710480.52832130.44854460.086956520.066793890.64893621.00000000.099337750.6933333\n    200.00808678170.000.2815249300.31481480.41502200.68589080.24251380.130434780.229007630.89361700.98499670.263520970.2933333\n    290.00861718600.000.2815249300.31481480.56217670.94232750.30236700.130434780.229007630.89361700.97740680.305463580.2977778\n    300.01119626100.000.2815249300.31481480.59647440.86920700.28275240.130434780.229007630.89361700.95796560.282836640.3555556\n    350.01805667270.000.2815249300.31481480.48572520.96807420.23917650.130434780.229007630.89361700.62532150.513520970.1888889\n\n\n\n\n\nsuppressMessages(library(Metrics))\n\n\nlibrary(Metrics) # for rmse function\nneuralnet_err <- data.frame(\n  R2_train = round(cor(train_data$medv, nn_model$response[,\"medv\"])^2, 4),\n  R2_test = round(cor(test_data$medv, test_predicted_scaled$net.result)^2, 4),\n  MSE_test = round(mse(test_data$medv, test_predicted_scaled$net.result), 4),\n  RMSE_test = round(rmse(test_data$medv, test_predicted_scaled$net.result), 4)  \n)\n\nrownames(neuralnet_err) <- \"neuralnet\"\nlinear_err |> bind_rows(neuralnet_err)\n\n# Model is much better by all metrics but overfitted on train\n\n\n\nA data.frame: 2 × 4\n\n    R2_trainR2_testMSE_testRMSE_test\n    <dbl><dbl><dbl><dbl>\n\n\n    linear0.75660.66110.01170.1081\n    neuralnet1.00000.77990.00770.0876"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#neuralnet-caret",
    "href": "034-supervised-learning-neutal-networks-regression.html#neuralnet-caret",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.7 neuralnet + caret",
    "text": "5.7 neuralnet + caret\n\nsuppressMessages(library(caret))\n\n\nctrl<- trainControl(method=\"cv\", number=1, search=\"random\")\n\nsuppressMessages(nn2_model <- train(formula, \n             data = train_data, \n             trControl = ctrl,\n             metric = \"RMSE\",\n             method = \"neuralnet\"))\n\nhead(nn2_model)\n\nWarning message in cbind(intercept = 1, as.matrix(data[, model.list$variables])):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(intercept = 1, as.matrix(data[, model.list$variables])):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(intercept = 1, as.matrix(data[, model.list$variables])):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\nWarning message in cbind(1, act.temp):\n\"number of rows of result is not a multiple of vector length (arg 1)\"\n\n\n\n\n    $method\n        'neuralnet'\n    $modelInfo\n        \n    $label\n        'Neural Network'\n    $library\n        'neuralnet'\n    $loop\n        NULL\n    $type\n        'Regression'\n    $parameters\n        \nA data.frame: 3 × 3\n\n    parameterclasslabel\n    <chr><chr><chr>\n\n\n    layer1numeric#Hidden Units in Layer 1\n    layer2numeric#Hidden Units in Layer 2\n    layer3numeric#Hidden Units in Layer 3\n\n\n\n    $grid\n        function (x, y, len = NULL, search = \"grid\") \n{\n    if (search == \"grid\") {\n        out <- expand.grid(layer1 = ((1:len) * 2) - 1, layer2 = 0, \n            layer3 = 0)\n    }\n    else {\n        out <- data.frame(layer1 = sample(2:20, replace = TRUE, \n            size = len), layer2 = sample(c(0, 2:20), replace = TRUE, \n            size = len), layer3 = sample(c(0, 2:20), replace = TRUE, \n            size = len))\n    }\n    out\n}\n    $fit\n        function (x, y, wts, param, lev, last, classProbs, ...) \n{\n    colNames <- colnames(x)\n    dat <- if (is.data.frame(x)) \n        x\n    else as.data.frame(x, stringsAsFactors = TRUE)\n    dat$.outcome <- y\n    form <- as.formula(paste(\".outcome ~\", paste(colNames, collapse = \"+\")))\n    if (param$layer1 == 0) \n        stop(\"the first layer must have at least one hidden unit\")\n    if (param$layer2 == 0 & param$layer2 > 0) \n        stop(\"the second layer must have at least one hidden unit if a third layer is specified\")\n    nodes <- c(param$layer1)\n    if (param$layer2 > 0) {\n        nodes <- c(nodes, param$layer2)\n        if (param$layer3 > 0) \n            nodes <- c(nodes, param$layer3)\n    }\n    neuralnet::neuralnet(form, data = dat, hidden = nodes, ...)\n}\n    $predict\n        function (modelFit, newdata, submodels = NULL) \n{\n    newdata <- newdata[, modelFit$model.list$variables, drop = FALSE]\n    neuralnet::compute(modelFit, covariate = newdata)$net.result[, \n        1]\n}\n    $prob\n        NULL\n    $tags\n        'Neural Network'\n    $sort\n        function (x) \nx[order(x$layer1, x$layer2, x$layer3), ]\n\n\n    $modelType\n        'Regression'\n    $results\n        \nA data.frame: 3 × 9\n\n    layer1layer2layer3RMSERsquaredMAERMSESDRsquaredSDMAESD\n    <int><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1 42154.3176010.00096097444.312419NANANA\n    2117102.1100060.10775281312.098059NANANA\n    3156 72.7744240.08402124382.766978NANANA\n\n\n\n    $pred\n        NULL\n    $bestTune\n        \nA data.frame: 1 × 3\n\n    layer1layer2layer3\n    <int><dbl><dbl>\n\n\n    211710\n\n\n\n\n\n\n\ntest_predicted_scaled <- predict(nn2_model, test_data%>%select(-medv))\nhead(test_predicted_scaled)\ntrain_predicted_scaled <- predict(nn2_model, train_data%>%select(-medv))\nhead(train_predicted_scaled)\n\n10.48704326140971450.621601182176565200.296898662765632290.311093959706198300.358306907905892350.207608926759425\n\n\n20.40495390078405330.63159026552399340.61677970143541160.48528860052442770.34487348682733380.321661962365258\n\n\n\ntest_nn2_predicted <- sapply(X = test_predicted_scaled, original = data$medv, FUN = revertData) %>% as.data.frame()\nhead(test_nn2_predicted)\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    126.91695\n    532.97205\n    2018.36044\n    2918.99923\n    3021.12381\n    3514.34240\n\n\n\n\n\nneuralnet2_err <- data.frame(\n  R2_train = round(cor(train_data$medv, train_predicted_scaled)^2, 4),\n  R2_test = round(cor(test_data$medv, test_predicted_scaled)^2, 4),\n  MSE_test = round(mse(test_data$medv, test_predicted_scaled), 4),\n  RMSE_test = round(rmse(test_data$medv, test_predicted_scaled), 4)  \n)\n\nrownames(neuralnet2_err) <- \"neuralnet_caret\"\nlinear_err |> bind_rows(neuralnet_err) |> bind_rows(neuralnet2_err)\n\n\n\nA data.frame: 3 × 4\n\n    R2_trainR2_testMSE_testRMSE_test\n    <dbl><dbl><dbl><dbl>\n\n\n    linear0.75660.66110.01170.1081\n    neuralnet1.00000.77990.00770.0876\n    neuralnet_caret0.97670.82510.00630.0793"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#neural-network-with-nnet",
    "href": "034-supervised-learning-neutal-networks-regression.html#neural-network-with-nnet",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.8 Neural network with nnet",
    "text": "5.8 Neural network with nnet\nПідключаємо пакет nnet для побудови нейромережі із 2-ма прихованими шарами, для прикладу.\n\nlibrary(nnet)\n\nБудуємо модель на основі формули створеної для попередньої моделі:\n\nsize - кількість нейронів у прихованому шарі\n\n\nnnet_model <- nnet(formula, data = train_data, size = 2, maxit = 100)\n# Не забудьте поекспериментувати зі зміною кількості вузлів\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n# weights:  31\ninitial  value 17.172411 \niter  10 value 3.798860\niter  20 value 2.477108\niter  30 value 2.048115\niter  40 value 1.889895\niter  50 value 1.692867\niter  60 value 1.452958\niter  70 value 1.436145\niter  80 value 1.420405\niter  90 value 1.398269\niter 100 value 1.390185\nfinal  value 1.390185 \nstopped after 100 iterations\n\n\n\nsummary(nnet_model)\n\na 13-2-1 network with 31 weights\noptions were -\n  b->h1  i1->h1  i2->h1  i3->h1  i4->h1  i5->h1  i6->h1  i7->h1  i8->h1  i9->h1 \n   3.21   11.13    0.13    0.14   -0.03   -0.12   -2.78    0.39    0.45   -0.59 \ni10->h1 i11->h1 i12->h1 i13->h1 \n   0.46    0.36   -0.51    0.46 \n  b->h2  i1->h2  i2->h2  i3->h2  i4->h2  i5->h2  i6->h2  i7->h2  i8->h2  i9->h2 \n -10.30   -2.70   14.50    3.19    0.10   -2.68    1.65    7.43  -14.01    0.98 \ni10->h2 i11->h2 i12->h2 i13->h2 \n   8.10   -2.17   -1.77  -14.95 \n  b->o  h1->o  h2->o \n 10.26 -12.12   8.44 \n\n\nВізуалізуємо модель:\n\nplot.nnet(nnet_model)\n\n\n\n\nЗдійснимо прогноз для тестової вибірки та повернемо значення до базового виміру:\n\ntest_predicted_scaled <- predict(nnet_model, test_data)\nhead(test_predicted_scaled)\ntest_nnet_predicted <- sapply(X = test_predicted_scaled, original = data$medv, FUN = revertData) %>% as.data.frame()\n\n\n\nA matrix: 6 × 1 of type dbl\n\n    10.5002897\n    50.6030015\n    200.2798733\n    290.3302058\n    300.3558518\n    350.2345779\n\n\n\n\n\nnnet_model\n\na 13-2-1 network with 31 weights\ninputs: crim zn indus chas nox rm age dis rad tax ptratio black lstat \noutput(s): medv \noptions were -\n\n\nПорівняємо похибки по лінійній регресії та нейронних мереж:\n\nnnet_err <- data.frame(\n  R2_train = round(cor(train_data$medv, predict(nnet_model, new=train_data))^2, 4),\n  R2_test = round(cor(test_data$medv, test_predicted_scaled)^2, 4),\n  MSE_test = round(mse(test_data$medv, test_predicted_scaled), 4),\n  RMSE_test = round(rmse(test_data$medv, test_predicted_scaled), 4)  \n)\n\nrownames(nnet_err) <- \"nnet\"\nlinear_err |> bind_rows(neuralnet_err) |> bind_rows(neuralnet2_err) |> bind_rows(nnet_err)\n\n# neuralnet wins! but remember it has more layers\n\n\n\nA data.frame: 4 × 4\n\n    R2_trainR2_testMSE_testRMSE_test\n    <dbl><dbl><dbl><dbl>\n\n\n    linear0.75660.66110.01170.1081\n    neuralnet1.00000.77990.00770.0876\n    neuralnet_caret0.97670.82510.00630.0793\n    nnet0.91480.72760.01030.1013"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#final-models-compare",
    "href": "034-supervised-learning-neutal-networks-regression.html#final-models-compare",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.9 Final models compare",
    "text": "5.9 Final models compare\nПобудуємо графік розподілу пронозованих значень показників по усх моделях:\n\nhead(test_lm_predicted)\nhead(test_nn_predicted)\nhead(test_nnet_predicted)\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    130.55659\n    528.54003\n    2018.27829\n    2919.56211\n    3021.01693\n    3513.81993\n\n\n\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    128.74649\n    231.34932\n    318.86595\n    418.92882\n    519.89660\n    611.69719\n\n\n\n\n\n\nA data.frame: 6 × 1\n\n    .\n    <dbl>\n\n\n    127.51304\n    232.13507\n    317.59430\n    419.85926\n    521.01333\n    615.55600\n\n\n\n\n\ntest_medv <- subset(data, !split) |> select(medv) |> unlist()\nhead(test_medv)\n\nmedv124medv236.2medv318.2medv418.4medv521medv613.5\n\n\n\nhead(test_data)\n\n\n\nA data.frame: 6 × 14\n\n    crimzninduschasnoxrmagedisradtaxptratioblacklstatmedv\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    10.00000000000.180.0678152500.31481480.57750530.64160660.26920310.000000000.208015270.28723401.00000000.089679910.4222222\n    50.00070507010.000.0630498500.15020580.68710480.52832130.44854460.086956520.066793890.64893621.00000000.099337750.6933333\n    200.00808678170.000.2815249300.31481480.41502200.68589080.24251380.130434780.229007630.89361700.98499670.263520970.2933333\n    290.00861718600.000.2815249300.31481480.56217670.94232750.30236700.130434780.229007630.89361700.97740680.305463580.2977778\n    300.01119626100.000.2815249300.31481480.59647440.86920700.28275240.130434780.229007630.89361700.95796560.282836640.3555556\n    350.01805667270.000.2815249300.31481480.48572520.96807420.23917650.130434780.229007630.89361700.62532150.513520970.1888889\n\n\n\n\n\n#Однакові межі для усіх графіків по Y\nplot_ylim <- c(5,40)\n\npar(mfrow=c(1,3)) # три стовпці, 1 рядок\n# pch  - тип точки для графіку\nplot(test_medv, test_lm_predicted[,1], col='black', ylim=plot_ylim, main='Real vs predicted LM', pch=25, cex=0.7)\nabline(0,1,lwd=2)\nlegend('bottomright',legend='LM',pch=25,col='black', bty='n')\n\nplot(test_medv,test_nn_predicted[,1], col='red', ylim=plot_ylim, main='Real vs predicted NN',pch=18,cex=0.7)\nabline(0,1,lwd=2)\nlegend('bottomright',legend='NN',pch=18,col='red', bty='n')\n\nplot(test_medv,test_nnet_predicted[,1], col='blue', ylim=plot_ylim, main='Real vs predicted NNET',pch=18,cex=0.7)\nabline(0,1,lwd=2)\n\nlegend('bottomright',legend='NNET',pch=18, col='black', bty='n')\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windows\n\n\n\n\n\nCombine all to one chart:\n\npar(mfrow=c(1,1))\n\nplot(test_medv,test_lm_predicted[,1],col='black',main='Real vs predicted NN + LM',pch=25,cex=0.7)\npoints(test_medv,test_nn_predicted[,1],col='red',pch=19,cex=0.7)\npoints(test_medv,test_nnet_predicted[,1],col='blue',pch=18,cex=0.7)\nabline(0,1,lwd=2)\nlegend('bottomright',legend=c('lm','neuralnet','nnet'),pch=c(25,19,18),col=c('black', 'red','blue'))"
  },
  {
    "objectID": "034-supervised-learning-neutal-networks-regression.html#валідація-моделі-на-різних-розмірах-вибірки",
    "href": "034-supervised-learning-neutal-networks-regression.html#валідація-моделі-на-різних-розмірах-вибірки",
    "title": "5  Нейронні мережі. Регресія. Вартість житла",
    "section": "5.10 Валідація моделі на різних розмірах вибірки",
    "text": "5.10 Валідація моделі на різних розмірах вибірки\n\nsuppressMessages(library(boot))\nsuppressMessages(library(plyr))\nsuppressMessages(library(DMwR))\n\nset.seed(2023)\nerrors <- data.frame(TrainSize = c(1:90), RMSE1 = c(0), RMSE2 = c(0))\n\nfor(i in 1:10) {\n\n    for(j in 1:90){\n  \n      split <- c(1:(nrow(scaled_data)*j/100))\n  \n      train_data <- scaled_data[split,]\n      test_data <- scaled_data[-split,]\n  \n      nnet_model <- nnet(formula, data = train_data,  size = 3, maxit = 100, trace = F)\n  \n      nnet_predicted_prob_scaled <- predict(nnet_model, test_data)\n      nnet_predicted <- nnet_predicted_prob_scaled * (max(data$medv) - min(data$medv)) + min(data$medv)\n  \n      errors_list <- regr.eval(test_data$medv, nnet_predicted)\n  \n      errors[errors$TrainSize == j, ]$RMSE1 <- errors[errors$TrainSize == j, ]$RMSE1 + errors_list[3]\n    }\n}\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:lattice':\n\n    melanoma\n\n\n------------------------------------------------------------------------------\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n------------------------------------------------------------------------------\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:reshape':\n\n    rename, round_any\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nLoading required package: grid\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nAttaching package: 'DMwR'\n\n\nThe following object is masked from 'package:plyr':\n\n    join\n\n\nThe following object is masked from 'package:modelr':\n\n    bootstrap\n\n\n\n\n\nerrors$RMSE1 <- errors$RMSE1/10\nplot (x = errors$TrainSize, \n      y = errors$RMSE1, \n      ylim = c(min(errors$RMSE1), \n               max(errors$RMSE1)), \n      type = \"l\", \n      xlab = \"length of training set\", \n      ylab = \"mean RMSE\", \n      main = \"Variation of RMSE with length of training set\")\n\n\n\n\n\nfor(i in 1:10) {\n  \n  for(j in 1:90){\n    \n    split <- sample.split(scaled_data$medv, SplitRatio = j/100)\n    \n    train_data <- subset(scaled_data, split == TRUE)\n    test_data <- subset(scaled_data, split == FALSE)\n    \n    nnet_model <- nnet(formula, data = train_data,  size = 3, maxit = 100, trace = F)\n    \n    nnet_predicted_prob_scaled <- predict(nnet_model, test_data)\n    nnet_predicted <- nnet_predicted_prob_scaled * (max(data$medv) - min(data$medv)) + min(data$medv)\n      \n    errors_list <- regr.eval(test_data$medv, nnet_predicted)\n    \n    errors[errors$TrainSize == j, ]$RMSE2 <- errors[errors$TrainSize == j, ]$RMSE2 + errors_list[3]\n  }\n}\n\n\nerrors$RMSE2 <- errors$RMSE2/10\nplot (x = errors$TrainSize, \n      y = errors$RMSE2, \n      ylim = c(min(errors$RMSE2), \n               max(errors$RMSE2)), \n      type = \"l\", \n      xlab = \"length of training set\", \n      ylab = \"mean RMSE\", \n      main = \"Variation of RMSE with length of training set\")\n\n\n\n\n\nhead(errors)\n\n\n\nA data.frame: 6 × 3\n\n    TrainSizeRMSE1RMSE2\n    <int><dbl><dbl>\n\n\n    1122.6584522.34442\n    2232.7666722.23155\n    3330.0697722.14714\n    4434.5936522.05084\n    5529.4951622.39201\n    6627.9320722.22010\n\n\n\n\n\n\nrequire(caret)\n\n\nfolds <- createFolds(scaled_data$medv, k = 10, list = TRUE, returnTrain = FALSE)\nerrors2 <- data.frame(FoldExcluded = c(1:10), RMSE = c(0))\n\n\nfolds\n\n\n    $Fold01\n        \n16202139464748566991120127132143151186187191196227231236239252258259279280289314320335337350352360372373380385392409415419420431439445452468484486\n\n    $Fold02\n        \n5152223333435527980889798126130135160168184205230243253261263267270273304313319354366387389411422423440451454463467478480482483498504\n\n    $Fold03\n        \n41726385370718286100104128133134141147201208209212214217224246269274305325326332341348353355356362365369370377382390400404407408417418470495501\n\n    $Fold04\n        \n392868727787899395131139140144163195197204213229233248254284290291299300301303316317318327329333339342346349351363391393399406426441450472473476\n\n    $Fold05\n        \n55768594113117118125142156157159167170172179189190192194203221240262272285286310324330338344358368371388414425427429434449459475493497500503505\n\n    $Fold06\n        \n111314414245576263106108114116119121136137153171176177199202207215223226234235244257281282288292302307312334336379395413428433435444446453456485\n\n    $Fold07\n        \n171012273040586073747881839296152154158162169206210216218237242245249260266276293295311321345347357378383386405424438447448455457458460\n\n    $Fold08\n        \n824293243102105110111112123124146149165166173175181183200220222225247251264271278283287296297331340359364376394398432443461469479488490491496506\n\n    $Fold09\n        \n61819253650545961669099103107109155161164174178180182198211219250265275306328361367375381384396397402403412416437442464465466471477481502\n\n    $Fold10\n        \n231374449516465677584101115122129138145148150185188193228232238241255256268277294298308309315322323343374401410421430436462474487489492494499\n\n\n\n\n\nfor(i in 1:10) {\n  \n  for(j in 1:10){    \n    train_data <- scaled_data[-folds[[j]], ]\n    test_data <- scaled_data[folds[[j]], ]\n    \n    nnet_model <- nnet(formula, data = train_data, trace = F,  size = 3, maxit = 100)\n    \n    nnet_predicted_prob_scaled <- predict(nnet_model, test_data)\n    nnet_predicted <- nnet_predicted_prob_scaled * (max(data$medv) - min(data$medv)) + min(data$medv)\n    errors_list <- regr.eval(test_data$medv, nnet_predicted)\n    errors2[errors2$FoldExcluded == j, ]$RMSE <- errors2[errors2$FoldExcluded == j, ]$RMSE + errors_list[3]\n  }\n}\n\n\nerrors2$RMSE <- errors2$RMSE/10\nplot (x = errors2$FoldExcluded, y = errors2$RMSE, \n      ylim = c(min(errors2$RMSE)-5, \n               max(errors2$RMSE)+5), \n      type = \"l\", \n      xlab = \"length of training set\", \n      ylab = \"mean RMSE\", \n      main = \"Variation of RMSE with length of training set\")\nhead(errors2)\n\n\n\nA data.frame: 6 × 2\n\n    FoldExcludedRMSE\n    <int><dbl>\n\n\n    1123.88887\n    2224.57703\n    3324.03482\n    4425.52928\n    5524.05225\n    6624.70978"
  },
  {
    "objectID": "035-supervised-learning-logistic-regression.html",
    "href": "035-supervised-learning-logistic-regression.html",
    "title": "6  Логістична регресія. Класифікація. Титанік",
    "section": "",
    "text": "Курс: “Математичне моделювання в R”"
  },
  {
    "objectID": "035-supervised-learning-logistic-regression.html#dataset-overview",
    "href": "035-supervised-learning-logistic-regression.html#dataset-overview",
    "title": "6  Логістична регресія. Класифікація. Титанік",
    "section": "6.1 Dataset overview",
    "text": "6.1 Dataset overview\nУ даному навчальному матеріалі використано класичний приклад даних з інформацією про пасажирів корабля Титанік.\nSource: https://github.com/Geoyi/Cleaning-Titanic-Data\nDataset description:\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\nData Fields:\n\n\n\n\n\n\n\n\nVariable\nDefinition\nKey\n\n\n\n\nsurvival\nSurvival (TARGET)\n0 = No, 1 = Yes\n\n\npclass\nTicket class\n1 = 1st, 2 = 2nd, 3 = 3rd\n\n\nsex\nSex\n\n\n\nAge\nAge in years\n\n\n\nsibsp\n# of siblings / spouses aboard the Titanic\n\n\n\nparch\n# of parents / children aboard the Titanic\n\n\n\nticket\nTicket number\n\n\n\nfare\nPassenger fare\n\n\n\ncabin\nCabin number\n\n\n\nembarked\nPort of Embarkation\nC = Cherbourg, Q = Queenstown, S = Southampton\n\n\n\n\n# read local\ntitanic_data <- read.csv(\"https://raw.githubusercontent.com/Geoyi/Cleaning-Titanic-Data/master/titanic_original.csv\", na.strings = c(\"\")) # empty strings is missing data and wiil be replaced with NA\nhead(titanic_data)\n\n\n\nA data.frame: 6 × 14\n\n    pclasssurvivednamesexagesibspparchticketfarecabinembarkedboatbodyhome.dest\n    <int><int><chr><chr><dbl><int><int><chr><dbl><chr><chr><chr><int><chr>\n\n\n    111Allen, Miss. Elisabeth Walton                  female29.00000024160 211.3375B5     S2  NASt Louis, MO                   \n    211Allison, Master. Hudson Trevor                 male   0.916712113781151.5500C22 C26S11 NAMontreal, PQ / Chesterville, ON\n    310Allison, Miss. Helen Loraine                   female 2.000012113781151.5500C22 C26SNA NAMontreal, PQ / Chesterville, ON\n    410Allison, Mr. Hudson Joshua Creighton           male  30.000012113781151.5500C22 C26SNA135Montreal, PQ / Chesterville, ON\n    510Allison, Mrs. Hudson J C (Bessie Waldo Daniels)female25.000012113781151.5500C22 C26SNA NAMontreal, PQ / Chesterville, ON\n    611Anderson, Mr. Harry                            male  48.00000019952  26.5500E12    S3  NANew York, NY                   \n\n\n\n\nПереглянемо структуру даних:\n\nstr(titanic_data)\n\n'data.frame':   1310 obs. of  14 variables:\n $ pclass   : int  1 1 1 1 1 1 1 1 1 1 ...\n $ survived : int  1 1 0 0 0 1 1 0 1 0 ...\n $ name     : chr  \"Allen, Miss. Elisabeth Walton\" \"Allison, Master. Hudson Trevor\" \"Allison, Miss. Helen Loraine\" \"Allison, Mr. Hudson Joshua Creighton\" ...\n $ sex      : chr  \"female\" \"male\" \"female\" \"male\" ...\n $ age      : num  29 0.917 2 30 25 ...\n $ sibsp    : int  0 1 1 1 1 0 1 0 2 0 ...\n $ parch    : int  0 2 2 2 2 0 0 0 0 0 ...\n $ ticket   : chr  \"24160\" \"113781\" \"113781\" \"113781\" ...\n $ fare     : num  211 152 152 152 152 ...\n $ cabin    : chr  \"B5\" \"C22 C26\" \"C22 C26\" \"C22 C26\" ...\n $ embarked : chr  \"S\" \"S\" \"S\" \"S\" ...\n $ boat     : chr  \"2\" \"11\" NA NA ...\n $ body     : int  NA NA NA 135 NA NA NA NA NA 22 ...\n $ home.dest: chr  \"St Louis, MO\" \"Montreal, PQ / Chesterville, ON\" \"Montreal, PQ / Chesterville, ON\" \"Montreal, PQ / Chesterville, ON\" ...\n\n\nЗначення показників вибірки:\n\nsurvival – “клас виживання” (0 = No; 1 = Yes)\npclass – клас пасажирів (1 = 1st; 2 = 2nd; 3 = 3rd)\nname – ім’я\nsex - стать\nage - вік\nsibsp – кількість членів сім’ї на борту (братів, сестер, подружжя)\nparch – кількість дітей або батьків на борту\nticket – номер квитка\nfare – вартість\ncabin – номер(и)\nembarked – порт посадки на судно (C = Cherbourg; Q = Queenstown; S = Southampton)\nboat – борт на якому відпливав під час порятунку (якщо врятований)\nbody – ідентифікаційни номер тіла\nhome.dest – місце доставки пасажира\n\nДетальніше: http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf.\nЗакреслені поля надалі будуть видалені з датасету для моделювання."
  },
  {
    "objectID": "035-supervised-learning-logistic-regression.html#data-preprocessing",
    "href": "035-supervised-learning-logistic-regression.html#data-preprocessing",
    "title": "6  Логістична регресія. Класифікація. Титанік",
    "section": "6.2 Data preprocessing",
    "text": "6.2 Data preprocessing\n\n6.2.1 Remove unused columns\nLets remove columns: - [x] name - just string - [x] ticket - it can be useful to analyze numbers and check if - [x] boat - if boat is not missing - survived, we cannot use it - [x] body - if body is not missing - not survived, cant use it - [x] home.dest - just text\n\nsuppressMessages(library(dplyr)) # filter()\ntitanic_data <- titanic_data |>\n    select(-c(\"name\", \"ticket\", \"boat\", \"body\", \"home.dest\"))\nhead(titanic_data)\n\n\n\nA data.frame: 6 × 9\n\n    pclasssurvivedsexagesibspparchfarecabinembarked\n    <int><int><chr><dbl><int><int><dbl><chr><chr>\n\n\n    111female29.000000211.3375B5     S\n    211male   0.916712151.5500C22 C26S\n    310female 2.000012151.5500C22 C26S\n    410male  30.000012151.5500C22 C26S\n    510female25.000012151.5500C22 C26S\n    611male  48.000000 26.5500E12    S\n\n\n\n\n\n\nsummary(titanic_data)\n\n     pclass         survived         sex                 age         \n Min.   :1.000   Min.   :0.000   Length:1310        Min.   : 0.1667  \n 1st Qu.:2.000   1st Qu.:0.000   Class :character   1st Qu.:21.0000  \n Median :3.000   Median :0.000   Mode  :character   Median :28.0000  \n Mean   :2.295   Mean   :0.382                      Mean   :29.8811  \n 3rd Qu.:3.000   3rd Qu.:1.000                      3rd Qu.:39.0000  \n Max.   :3.000   Max.   :1.000                      Max.   :80.0000  \n NA's   :1       NA's   :1                          NA's   :264      \n     sibsp            parch            fare            cabin          \n Min.   :0.0000   Min.   :0.000   Min.   :  0.000   Length:1310       \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:  7.896   Class :character  \n Median :0.0000   Median :0.000   Median : 14.454   Mode  :character  \n Mean   :0.4989   Mean   :0.385   Mean   : 33.295                     \n 3rd Qu.:1.0000   3rd Qu.:0.000   3rd Qu.: 31.275                     \n Max.   :8.0000   Max.   :9.000   Max.   :512.329                     \n NA's   :1        NA's   :1       NA's   :2                           \n   embarked        \n Length:1310       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\n\n\n\n6.2.2 Check missing data\nДля оцінки кількості пропусків побудуємо матрицю та мапу:\n\nsuppressMessages(library(mice))\nmd.pattern(titanic_data)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\n\n\n\n\nA matrix: 8 × 10 of type dbl\n\n    pclasssurvivedsexsibspparchfareembarkedagecabin\n\n\n    2701111111  1   1   0\n    7731111111  1   0   1\n    231111111  0   1   1\n    2401111111  0   0   2\n    21111110  1   1   1\n    11111101  1   0   2\n    10000000  0   0   9\n    111112326410151289\n\n\n\n\n\n\n\n\nsuppressMessages(library(Amelia))\nmissmap(titanic_data, main = 'Missing data map', col = c('yellow', 'black'), legend = FALSE)\n\nLoading required package: Rcpp\n\n## \n## Amelia II: Multiple Imputation\n## (Version 1.8.0, built: 2021-05-26)\n## Copyright (C) 2005-2022 James Honaker, Gary King and Matthew Blackwell\n## Refer to http://gking.harvard.edu/amelia/ for more information\n## \n\n\n\n\n\n\nOne more way to check where missing data is present with saplly():\n\nsapply(titanic_data, function(x) sum(is.na(x)))\n\npclass1survived1sex1age264sibsp1parch1fare2cabin1015embarked3\n\n\n\n# Lets check where survived is NA, target should be finite\ntitanic_data %>% filter(is.na(survived))\n# looks like its wrong line in data\n\n\n\nA data.frame: 1 × 9\n\n    pclasssurvivedsexagesibspparchfarecabinembarked\n    <int><int><chr><dbl><int><int><dbl><chr><chr>\n\n\n    NANANANANANANANANA\n\n\n\n\n\n# lets remove this row\ntitanic_data <- titanic_data %>% filter(!is.na(survived))\nsapply(titanic_data, function(x) sum(is.na(x)))\n\npclass0survived0sex0age263sibsp0parch0fare1cabin1014embarked2\n\n\n\nWe will replace all missing data on the next stages for: fare, cabin, age, embarked.\n\n\n\n6.2.3 Visual analysis\n\nlibrary(ggplot2) # for plots\nlibrary(gmodels) # for CrossTable\n\nSurvived\n\nCrossTable(titanic_data$survived)\nggplot(titanic_data, aes(survived)) + geom_bar(aes(fill = factor(survived))) + theme_bw()\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1309 \n\n \n          |         0 |         1 | \n          |-----------|-----------|\n          |       809 |       500 | \n          |     0.618 |     0.382 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n\n\nКрос-таблиця survived vs pclass\n\nCrossTable(titanic_data$survived, titanic_data$pclass)\nggplot(titanic_data, aes(pclass)) + geom_bar(aes(fill = factor(pclass)))+ theme_bw()\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  1309 \n\n \n                      | titanic_data$pclass \ntitanic_data$survived |         1 |         2 |         3 | Row Total | \n----------------------|-----------|-----------|-----------|-----------|\n                    0 |       123 |       158 |       528 |       809 | \n                      |    29.411 |     1.017 |    18.411 |           | \n                      |     0.152 |     0.195 |     0.653 |     0.618 | \n                      |     0.381 |     0.570 |     0.745 |           | \n                      |     0.094 |     0.121 |     0.403 |           | \n----------------------|-----------|-----------|-----------|-----------|\n                    1 |       200 |       119 |       181 |       500 | \n                      |    47.587 |     1.645 |    29.788 |           | \n                      |     0.400 |     0.238 |     0.362 |     0.382 | \n                      |     0.619 |     0.430 |     0.255 |           | \n                      |     0.153 |     0.091 |     0.138 |           | \n----------------------|-----------|-----------|-----------|-----------|\n         Column Total |       323 |       277 |       709 |      1309 | \n                      |     0.247 |     0.212 |     0.542 |           | \n----------------------|-----------|-----------|-----------|-----------|\n\n \n\n\n\n\n\nsex\n\nggplot(titanic_data, aes(x = sex)) + geom_bar(aes(fill = factor(sex))) + theme_bw()\n\n\n\n\nAge*\n\nggplot(titanic_data, aes(age)) + geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + theme_bw()\n\nWarning message:\n\"Removed 263 rows containing non-finite values (stat_bin).\"\n\n\n\n\n\nfare\n\nggplot(titanic_data, aes(fare)) + geom_histogram(bins = 15, alpha = 0.5, fill = 'blue', color='black')+ theme_bw()\n\nWarning message:\n\"Removed 1 rows containing non-finite values (stat_bin).\"\n\n\n\n\n\nsibsp\n\nggplot(titanic_data, aes(sibsp)) + geom_bar()+ theme_bw()\n\n\n\n\nparch\n\nggplot(titanic_data, aes(parch)) + geom_bar()+ theme_bw()\n\n\n\n\n\nggplot(titanic_data, aes(embarked)) + geom_bar(aes(fill = factor(embarked)))+ theme_bw()\n\n# it looks like S the most popular value in embarked, so it could be good idea to replace 1 missing with S"
  },
  {
    "objectID": "035-supervised-learning-logistic-regression.html#splitting-data",
    "href": "035-supervised-learning-logistic-regression.html#splitting-data",
    "title": "6  Логістична регресія. Класифікація. Титанік",
    "section": "6.3 Splitting data",
    "text": "6.3 Splitting data\nLest split out dataset for test and train\n\nset.seed(1) # seed\nlibrary(caTools)\nsplit <- sample.split(titanic_data$survived, SplitRatio = 0.7)\ntitanic_train <- subset(titanic_data, split == TRUE)\ntitanic_test <- subset(titanic_data, split == FALSE)\n\nlibrary(gmodels)\nCrossTable(titanic_train$survived)\nCrossTable(titanic_test$survived) # its splitted by the same proportions\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  916 \n\n \n          |         0 |         1 | \n          |-----------|-----------|\n          |       566 |       350 | \n          |     0.618 |     0.382 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  393 \n\n \n          |         0 |         1 | \n          |-----------|-----------|\n          |       243 |       150 | \n          |     0.618 |     0.382 | \n          |-----------|-----------|"
  },
  {
    "objectID": "035-supervised-learning-logistic-regression.html#missing-replacement",
    "href": "035-supervised-learning-logistic-regression.html#missing-replacement",
    "title": "6  Логістична регресія. Класифікація. Титанік",
    "section": "6.4 Missing replacement",
    "text": "6.4 Missing replacement\nMissing data replacement should have base points for each parameter.\nSo, we will fix data for replacement from train and implement it to both train and test.\n\nhead(titanic_train)\n\n\n\nA data.frame: 6 × 9\n\n    pclasssurvivedsexagesibspparchfarecabinembarked\n    <int><int><chr><dbl><int><int><dbl><chr><chr>\n\n\n    111female29.000000211.3375B5     S\n    211male   0.916712151.5500C22 C26S\n    310female 2.000012151.5500C22 C26S\n    410male  30.000012151.5500C22 C26S\n    510female25.000012151.5500C22 C26S\n    611male  48.000000 26.5500E12    S\n\n\n\n\n\n# check missing\nsapply(titanic_train, function(x) sum(is.na(x)))\nsapply(titanic_test, function(x) sum(is.na(x)))\n\npclass0survived0sex0age182sibsp0parch0fare1cabin703embarked1\n\n\npclass0survived0sex0age81sibsp0parch0fare0cabin311embarked1\n\n\nСтворюємо dummy-змінну, що буде вказути на наявність запису про каюту у пасажира 1/0:\n\n# train\ntitanic_train <- titanic_train %>%\n    mutate(hascabin = ifelse(!is.na(cabin), 1, 0)) %>%\n    select(-cabin) # remove cabin column\n\ntitanic_test <- titanic_test %>%\n    mutate(hascabin = ifelse(!is.na(cabin), 1, 0)) %>%\n    select(-cabin) # remove cabin column\n\nЗаповнюємо дані пропусків вартості квитка fare. Замінюємо NA на середнє значення вартості квитка:\n\navg_fare <- round(mean(titanic_data$fare, na.rm = TRUE),4)\navg_fare\n\n# lets study new function from tidyr replace NA\nlibrary(tidyr)\ntitanic_train <- titanic_train %>% \n    mutate(fare = replace_na(fare, avg_fare))\n\n# there are no missing in test$fare\n\nany(is.na(titanic_train$fare)) # check for missing\nany(is.na(titanic_test$fare))\n\n33.2955\n\n\nFALSE\n\n\nFALSE\n\n\nЗаповнюємо пропуски для порту посадки (embarked). Варіантом для заміни оберемо найпопулярнішйи варінт значення, той який зустрічається частіше за інші. Скористаємося показником МОДА. З попереднього аналізу відомо, що таким показником є S. Проте варто написати універсальну функцію, що буде сама відбирати значення:\n\ngetmode <- function(v) {\n  uniqv <- unique(v) # select all unique values\n  uniqv[which.max(tabulate(match(v, uniqv)))] # select value that included most times\n}\n\n\n# get the moda\nembarked_moda <- getmode(titanic_data$embarked)\nembarked_moda # its `S`\n\n'S'\n\n\n\ntitanic_train <- titanic_train %>% \n    mutate(embarked = replace_na(embarked, embarked_moda))\n\ntitanic_test <- titanic_test %>% \n    mutate(embarked = replace_na(embarked, embarked_moda))\n\nany(is.na(titanic_train$embarked)) # check for missing\nany(is.na(titanic_test$embarked))\n\nFALSE\n\n\nFALSE\n\n\nДля заміни пропусків по віку (age) використаємо показник “Клас пасажира” (pclass). Обчислимо середній вік пасажирів кожного класу і замінимо пропуски з урахуванням цієї інформації. Тобто для усіх пасажирів 1-го класу замінимо пропуски середнім по пасажирах середнього класу і т.д.\n\n# lets build boxplot for average age in each class\n\nage_plot <- ggplot(titanic_train, aes(pclass, age)) \nage_plot <- age_plot + geom_boxplot(aes(group = pclass, fill = factor(pclass), alpha = 0.4)) # alpha == opacity of chart elements\nage_plot <- age_plot + scale_y_continuous(breaks = seq(min(0), max(80), by = 2)) + theme_bw()\nage_plot\n\nWarning message:\n\"Removed 182 rows containing non-finite values (stat_boxplot).\"\n\n\n\n\n\nСередні значення віку для кожного класу пасажирів становлять:\n\npclass_ages <- titanic_train %>%\n    group_by(pclass) %>%\n    summarise(mean_age = floor(mean(age, na.rm =T)))\npclass_ages\n\n\n\nA tibble: 3 × 2\n\n    pclassmean_age\n    <int><dbl>\n\n\n    138\n    229\n    324\n\n\n\n\n\ntitanic_train <- titanic_train %>% \n    mutate(age = ifelse(is.na(age), pclass_ages$mean_age[pclass], age))\n           \nany(is.na(titanic_train$age)) # check for missing\nhead(titanic_train)\n\nFALSE\n\n\n\n\nA data.frame: 6 × 9\n\n    pclasssurvivedsexagesibspparchfareembarkedhascabin\n    <int><int><chr><dbl><int><int><dbl><chr><dbl>\n\n\n    111female29.000000211.3375S1\n    211male   0.916712151.5500S1\n    310female 2.000012151.5500S1\n    410male  30.000012151.5500S1\n    510female25.000012151.5500S1\n    611male  48.000000 26.5500S1\n\n\n\n\n\ntitanic_test <- titanic_test %>% \n       mutate(age = ifelse(is.na(age), pclass_ages$mean_age[pclass], age))\nany(is.na(titanic_test$age)) # check for missing\n\nFALSE\n\n\n\nmd.pattern(titanic_train) # OK, no missing\n\n /\\     /\\\n{  `---'  }\n{  O   O  }\n==>  V <==  No need for mice. This data set is completely observed.\n \\  \\|/  /\n  `-----'\n\n\n\n\n\nA matrix: 2 × 10 of type dbl\n\n    pclasssurvivedsexagesibspparchfareembarkedhascabin\n\n\n    9161111111110\n    0000000000\n\n\n\n\n\n\n\n\nmd.pattern(titanic_test) # OK, no missing\n\n /\\     /\\\n{  `---'  }\n{  O   O  }\n==>  V <==  No need for mice. This data set is completely observed.\n \\  \\|/  /\n  `-----'\n\n\n\n\n\nA matrix: 2 × 10 of type dbl\n\n    pclasssurvivedsexagesibspparchfareembarkedhascabin\n\n\n    3931111111110\n    0000000000"
  },
  {
    "objectID": "035-supervised-learning-logistic-regression.html#model-accuracy-check",
    "href": "035-supervised-learning-logistic-regression.html#model-accuracy-check",
    "title": "6  Логістична регресія. Класифікація. Титанік",
    "section": "7.1 Model accuracy check",
    "text": "7.1 Model accuracy check\nLets create new datasets for train and test results storing. We will use them\n\ntrain_results <- titanic_train %>% \n    select(survived) %>%\n    mutate(predicted = predict(def_glm, type = 'response', newdata = titanic_train),\n          residuals = residuals(def_glm, type = \"response\"))\n\ntest_results <- titanic_test %>%\n    select(survived) %>%\n    mutate(predicted = predict(def_glm, type = 'response', newdata = titanic_test))\n\nhead(test_results)\n\n\n\nA data.frame: 6 × 2\n\n    survivedpredicted\n    <fct><dbl>\n\n\n    710.7839711\n    1100.4233024\n    1210.9688239\n    1310.9717471\n    1700.6718810\n    2000.5925107\n\n\n\n\nThe next stage is converting out predicted probabilities into event classes 1/0. By default you can do this using next condition predicted == 1 if prob >= 0.5 and 0 if prob < 0.5.\nIn this case 0.5 is cut-off line or classification splitter. So, lets do this for both train and test.\n\ntrain_results <- train_results %>%\n    mutate(survived_05 = as.factor(ifelse(predicted >= 0.5, 1 , 0)))\n\ntest_results <- test_results %>%\n    mutate(survived_05 = as.factor(ifelse(predicted >= 0.5, 1 , 0)))\n\nhead(test_results)\n\n\n\nA data.frame: 6 × 3\n\n    survivedpredictedsurvived_05\n    <fct><dbl><fct>\n\n\n    710.78397111\n    1100.42330240\n    1210.96882391\n    1310.97174711\n    1700.67188101\n    2000.59251071\n\n\n\n\nДля демонстрації переглянемо графік реальних та прогнозованих/модельованих значень survived.\n\nplot_train_data <- train_results[0:80,]\n\nggplot(plot_train_data, aes(x = c(1:nrow(plot_train_data)), y = as.numeric(survived)-1)) +\n  geom_segment(aes(xend = c(1:nrow(plot_train_data)), yend = predicted), alpha = .2) +\n  geom_point(aes(color = residuals), size = 2) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 1, size = 2) +\n  geom_hline(yintercept=0.5, linetype=\"dashed\", color = \"red\") +\n  theme_bw()\n\nWarning message:\n\"`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\"\n\n\n\n\n\n\nplot_test_data <- test_results[0:80,]\n\nggplot(plot_test_data, aes(x = c(1:nrow(plot_test_data)), y = as.numeric(survived)-1)) +\n  geom_segment(aes(xend = c(1:nrow(plot_test_data)), yend = predicted), alpha = .2) +\n  # Увага, для тестової вибірки є зміни, за точку відліку враховується вихідна змінна\n  geom_point(aes(color =  as.numeric(survived)), size = 2) +\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n  guides(color = FALSE) +\n  geom_point(aes(y = predicted), shape = 1, size = 2) +\n  geom_hline(yintercept=0.5, linetype=\"dashed\", color = \"red\") +\n  theme_bw()\n\nYour code contains a unicode char which cannot be displayed in your\ncurrent locale and R will silently convert it to an escaped form when the\nR kernel executes this code. This can lead to subtle errors if you use\nsuch chars to do comparisons. For more information, please see\nhttps://github.com/IRkernel/repr/wiki/Problems-with-unicode-on-windowsWarning message:\n\"`guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\"\n\n\n\n\n\nПобудуємо confusion matrix для тренувальної вибірки:\n\nprint(\"Train CM:\")\ntable(train_results$survived, train_results$survived_05)\n\n[1] \"Train CM:\"\n\n\n   \n      0   1\n  0 488  78\n  1 104 246\n\n\n\nprint(\"Test CM:\")\ntable(test_results$survived, test_results$survived_05)\n\n[1] \"Test CM:\"\n\n\n   \n      0   1\n  0 206  37\n  1  46 104\n\n\nБільш детальну статистику по confusion matrix можна отримати скориставшись методом confusionMatrix() з пакету caret.\n\n# train\ncaret::confusionMatrix(train_results$survived, train_results$survived_05, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 488  78\n         1 104 246\n                                         \n               Accuracy : 0.8013         \n                 95% CI : (0.774, 0.8267)\n    No Information Rate : 0.6463         \n    P-Value [Acc > NIR] : < 2e-16        \n                                         \n                  Kappa : 0.5732         \n                                         \n Mcnemar's Test P-Value : 0.06386        \n                                         \n            Sensitivity : 0.7593         \n            Specificity : 0.8243         \n         Pos Pred Value : 0.7029         \n         Neg Pred Value : 0.8622         \n             Prevalence : 0.3537         \n         Detection Rate : 0.2686         \n   Detection Prevalence : 0.3821         \n      Balanced Accuracy : 0.7918         \n                                         \n       'Positive' Class : 1              \n                                         \n\n\n\n# test\ncaret::confusionMatrix(test_results$survived, test_results$survived_05, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 206  37\n         1  46 104\n                                          \n               Accuracy : 0.7888          \n                 95% CI : (0.7451, 0.8281)\n    No Information Rate : 0.6412          \n    P-Value [Acc > NIR] : 1.522e-10       \n                                          \n                  Kappa : 0.5474          \n                                          \n Mcnemar's Test P-Value : 0.3799          \n                                          \n            Sensitivity : 0.7376          \n            Specificity : 0.8175          \n         Pos Pred Value : 0.6933          \n         Neg Pred Value : 0.8477          \n             Prevalence : 0.3588          \n         Detection Rate : 0.2646          \n   Detection Prevalence : 0.3817          \n      Balanced Accuracy : 0.7775          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nOne of most often qustions is “What cutoff line should be selected?”. You can have 2 answers:\n\nYou can select optimal cutoff using best classification criteria: while you best classify both 1 and 0 events\nYou can select cutoff than help solve your business tasks, for, example, you need to select only 20% most risky clients, so, set your cutoff to 0.8. Or you need concrete count of clients, for example, 1000 and it can be at 0.94 line sometimes if you have big database.\n\nLets try to find optimal cutoff with InformationValue package and special method:\n\nsuppressMessages(library(InformationValue))\n\nopt_cutoff <- optimalCutoff(train_results$survived, train_results$predicted)\nopt_cutoff\n\n\nAttaching package: 'InformationValue'\n\n\nThe following objects are masked from 'package:caret':\n\n    confusionMatrix, precision, sensitivity, specificity\n\n\n\n\n0.556674085642972\n\n\nLest compare confusion matrix for previous 0.5 cutoff and current opt_cutoff\n\n# cutoff = 0.5\ncaret::confusionMatrix(test_results$survived, test_results$survived_05, positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 206  37\n         1  46 104\n                                          \n               Accuracy : 0.7888          \n                 95% CI : (0.7451, 0.8281)\n    No Information Rate : 0.6412          \n    P-Value [Acc > NIR] : 1.522e-10       \n                                          \n                  Kappa : 0.5474          \n                                          \n Mcnemar's Test P-Value : 0.3799          \n                                          \n            Sensitivity : 0.7376          \n            Specificity : 0.8175          \n         Pos Pred Value : 0.6933          \n         Neg Pred Value : 0.8477          \n             Prevalence : 0.3588          \n         Detection Rate : 0.2646          \n   Detection Prevalence : 0.3817          \n      Balanced Accuracy : 0.7775          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\n\n# opt_cutoff\n\ntest_results <- test_results %>%\n    mutate(survived_opt = ifelse(predicted >= opt_cutoff, 1, 0)) # calculate optima cutoff\n\ncaret::confusionMatrix(test_results$survived, factor(test_results$survived_opt), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 215  28\n         1  53  97\n                                          \n               Accuracy : 0.7939          \n                 95% CI : (0.7505, 0.8328)\n    No Information Rate : 0.6819          \n    P-Value [Acc > NIR] : 5.041e-07       \n                                          \n                  Kappa : 0.5489          \n                                          \n Mcnemar's Test P-Value : 0.007661        \n                                          \n            Sensitivity : 0.7760          \n            Specificity : 0.8022          \n         Pos Pred Value : 0.6467          \n         Neg Pred Value : 0.8848          \n             Prevalence : 0.3181          \n         Detection Rate : 0.2468          \n   Detection Prevalence : 0.3817          \n      Balanced Accuracy : 0.7891          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nAccuracy : 0.7888 -> Accuracy : 0.7939 increased\nBalanced Accuracy : 0.7775 -> Balanced Accuracy : 0.7891 inreased\nIn this case if you need classificate more clients as survived decrease cutoff, for less survived increase it.\nAnd the last one characteristics we will use is ROC-curve:\n\n# for train\nplotROC(train_results$survived, train_results$predicted) # from InformationValue package\nauroc <- round(AUROC(train_results$survived, train_results$predicted), 4)\nauroc\ngini <- 2*auroc - 1\ngini\n\n#if ROC is fully filled and AUROC close to 1 on train, its very big possibility that our model is overfitted\n\n0.8573\n\n\n0.7146\n\n\n\n\n\n\n# for test\nplotROC(test_results$survived, test_results$predicted) # from InformationValue package\nauroc <- round(AUROC(test_results$survived, test_results$predicted), 4)\nauroc\ngini <- 2*auroc - 1\ngini\n\n#if ROC is fully filled and AUROC close to 1 on test, its very big possibility that you test sample contains\n\n0.8142\n\n\n0.6284\n\n\n\n\n\nYou should made your conclusions about test set, because test results shows how model predicting events on data it not know before. Test data is very close to prediction data but did not have target variable, so you can check accuracy on prediction set after some time cheking you experiment on real-world situation."
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html",
    "href": "036-supervised-learning-desicion-trees-classification.html",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "",
    "text": "Курс: “Математичне моделювання в R”\nУ даній частині навчального процесу потрібно побудувати математичні моделі класифікації клієнтів на основі алгоритму дерева рішень та перевірити їх на тестовій вибірці."
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#dataset-description",
    "href": "036-supervised-learning-desicion-trees-classification.html#dataset-description",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.1 Dataset description",
    "text": "7.1 Dataset description\nAbstract\nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\nData Set Information:\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (‘yes’) or not (‘no’) subscribed.\nThere are four datasets: 1. bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014] 2. bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs. 3. bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). 4. bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs).\nThe smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM).\nThe classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\nAttribute Information\nInput variables: bank client data:\n\n\n\n\n\n\n\n\n\n\nNo\nTitle\nDescription\nData Type\nValues\n\n\n\n\n1\nage\n\nnumeric\n\n\n\n2\njob\ntype of job\ncategorical\n‘admin.’, ‘blue-collar’, ‘entrepreneur’, ‘housemaid’, ‘management’, ‘retired’, ‘self-employed’, ‘services’, ‘student’, ‘technician’, ‘unemployed’, ‘unknown’\n\n\n3\nmarital\nmarital status\ncategorical\n‘divorced’,‘married’,‘single’,‘unknown’; note: ‘divorced’ means divorced or widowed\n\n\n4\neducation\n\ncategorical\n‘basic.4y’,‘basic.6y’,‘basic.9y’,‘high.school’,‘illiterate’,‘professional.course’,‘university.degree’,‘unknown’\n\n\n5\ndefault\nhas credit in default?\ncategorical\n‘no’,‘yes’,‘unknown’\n\n\n6\nhousing\nhas housing loan?\ncategorical\n‘no’,‘yes’,‘unknown’\n\n\n7\nloan\nhas personal loan?\ncategorical\n‘no’,‘yes’,‘unknown’\n\n\n\nInput variables: related with the last contact of the current campaign:\n\n\n\n\n\n\n\n\n\n\nNo\nTitle\nDescription\nData Type\nValues\n\n\n\n\n8\ncontact\ncontact communication type\ncategorical\n‘cellular’,‘telephone’\n\n\n9\nmonth\nlast contact month of year\ncategorical\n‘jan’, ‘feb’, ‘mar’, …, ‘nov’, ‘dec’\n\n\n10\nday_of_week\nlast contact day of the week\ncategorical\n‘mon’,‘tue’,‘wed’,‘thu’,‘fri’\n\n\n11\nduration\nlast contact duration, in seconds\nnumeric\n\n\n\n\nduration - Important note: this attribute highly affects the output target (e.g., if duration=0 then y=‘no’). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nInput variables: other attributes:\n\n\n\n\n\n\n\n\n\n\nNo\nTitle\nDescription\nData Type\nValues\n\n\n\n\n12\ncampaign\nnumber of contacts performed during this campaign and for this client\nnumeric\nincludes last contact\n\n\n13\npdays\nnumber of days that passed by after the client was last contacted from a previous campaign\nnumeric\n999 mean client was not previously contacted\n\n\n14\nprevious\nnumber of contacts performed before this campaign and for this client\nnumeric\n\n\n\n15\npoutcome\noutcome of the previous marketing campaign\ncategorical\n‘failure’,‘nonexistent’,‘success’\n\n\n\nInput variables: social and economic context attributes\n\n\n\n\n\n\n\n\n\n\nNo\nTitle\nDescription\nData Type\nValues\n\n\n\n\n16\nemp.var.rate\nemployment variation rate - quarterly indicator\nnumeric\n\n\n\n17\ncons.price.idx\nconsumer price index - monthly indicator\nnumeric\n\n\n\n18\ncons.conf.idx\nconsumer confidence index - monthly indicator\n\nnumeric\n\n\n19\neuribor3m\neuribor 3 month rate - daily indicator\nnumeric\n\n\n\n20\nnr.employed\nnumber of employees - quarterly indicator\nnumeric\n\n\n\n\nOutput variable (desired target):\n\n\n\n\n\n\n\n\n\n\nNo\nTitle\nDescription\nData Type\nValues\n\n\n\n\n21\ny\nhas the client subscribed a term deposit?\nbinary\n‘yes’,‘no’\n\n\n\nSource: https://archive.ics.uci.edu/ml/datasets/bank+marketing"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#data-load-and-preview",
    "href": "036-supervised-learning-desicion-trees-classification.html#data-load-and-preview",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.2 Data load and preview",
    "text": "7.2 Data load and preview\nДля початку завантажимо дані у змінну data:\n\ndata <- read.csv(\"https://raw.githubusercontent.com/kleban/r-course-eng/main/data/banking.csv\", \n                 na.strings = c(\"\", \" \", \"NA\", \"NULL\"), # fix missing as NA if present\n                 stringsAsFactors = TRUE) # set strings as factor, we need this for some algorithms\n#use + unknown with na.strings if you want to play with missing\n#data <- read.csv(\"data/banking.csv\", na.strings = c(\"\", \" \", \"NA\", \"NULL\", \"unknown\"))\n\nПереглянемо структуру вибірки даних з str():\n\nstr(data)\n\n'data.frame':   11162 obs. of  17 variables:\n $ age      : int  59 56 41 55 54 42 56 60 37 28 ...\n $ job      : Factor w/ 12 levels \"admin.\",\"blue-collar\",..: 1 1 10 8 1 5 5 6 10 8 ...\n $ marital  : Factor w/ 3 levels \"divorced\",\"married\",..: 2 2 2 2 2 3 2 1 2 3 ...\n $ education: Factor w/ 4 levels \"primary\",\"secondary\",..: 2 2 2 2 3 3 3 2 2 2 ...\n $ default  : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ balance  : int  2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 2 1 2 2 2 2 2 ...\n $ loan     : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 2 2 1 1 1 ...\n $ contact  : Factor w/ 3 levels \"cellular\",\"telephone\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ day      : int  5 5 5 5 5 5 6 6 6 6 ...\n $ month    : Factor w/ 12 levels \"apr\",\"aug\",\"dec\",..: 9 9 9 9 9 9 9 9 9 9 ...\n $ duration : int  1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int  1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int  0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : Factor w/ 4 levels \"failure\",\"other\",..: 4 4 4 4 4 4 4 4 4 4 ...\n $ deposit  : Factor w/ 2 levels \"no\",\"yes\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nПереглянемо вигляд перших рядків даних з head():\n\nhead(data)\n\n\n\nA data.frame: 6 × 17\n\n    agejobmaritaleducationdefaultbalancehousingloancontactdaymonthdurationcampaignpdayspreviouspoutcomedeposit\n    <int><fct><fct><fct><fct><int><fct><fct><fct><int><fct><int><int><int><int><fct><fct>\n\n\n    159admin.    marriedsecondaryno2343yesno unknown5may10421-10unknownyes\n    256admin.    marriedsecondaryno  45no no unknown5may14671-10unknownyes\n    341technicianmarriedsecondaryno1270yesno unknown5may13891-10unknownyes\n    455services  marriedsecondaryno2476yesno unknown5may 5791-10unknownyes\n    554admin.    marriedtertiary no 184no no unknown5may 6732-10unknownyes\n    642managementsingle tertiary no   0yesyesunknown5may 5622-10unknownyes\n\n\n\n\nОписова статистика факторів:\n\nsummary(data)\n\n      age                 job           marital         education   \n Min.   :18.00   management :2566   divorced:1293   primary  :1500  \n 1st Qu.:32.00   blue-collar:1944   married :6351   secondary:5476  \n Median :39.00   technician :1823   single  :3518   tertiary :3689  \n Mean   :41.23   admin.     :1334                   unknown  : 497  \n 3rd Qu.:49.00   services   : 923                                   \n Max.   :95.00   retired    : 778                                   \n                 (Other)    :1794                                   \n default        balance      housing     loan           contact    \n no :10994   Min.   :-6847   no :5881   no :9702   cellular :8042  \n yes:  168   1st Qu.:  122   yes:5281   yes:1460   telephone: 774  \n             Median :  550                         unknown  :2346  \n             Mean   : 1529                                         \n             3rd Qu.: 1708                                         \n             Max.   :81204                                         \n                                                                   \n      day            month         duration       campaign     \n Min.   : 1.00   may    :2824   Min.   :   2   Min.   : 1.000  \n 1st Qu.: 8.00   aug    :1519   1st Qu.: 138   1st Qu.: 1.000  \n Median :15.00   jul    :1514   Median : 255   Median : 2.000  \n Mean   :15.66   jun    :1222   Mean   : 372   Mean   : 2.508  \n 3rd Qu.:22.00   nov    : 943   3rd Qu.: 496   3rd Qu.: 3.000  \n Max.   :31.00   apr    : 923   Max.   :3881   Max.   :63.000  \n                 (Other):2217                                  \n     pdays           previous          poutcome    deposit   \n Min.   : -1.00   Min.   : 0.0000   failure:1228   no :5873  \n 1st Qu.: -1.00   1st Qu.: 0.0000   other  : 537   yes:5289  \n Median : -1.00   Median : 0.0000   success:1071             \n Mean   : 51.33   Mean   : 0.8326   unknown:8326             \n 3rd Qu.: 20.75   3rd Qu.: 1.0000                            \n Max.   :854.00   Max.   :58.0000                            \n                                                             \n\n\nПеревіримо вибірку на наявність пропусків:\n\nsuppressMessages(library(mice))\nmd.pattern(data) # OK\n\n /\\     /\\\n{  `---'  }\n{  O   O  }\n==>  V <==  No need for mice. This data set is completely observed.\n \\  \\|/  /\n  `-----'\n\n\n\n\n\nA matrix: 2 × 18 of type dbl\n\n    agejobmaritaleducationdefaultbalancehousingloancontactdaymonthdurationcampaignpdayspreviouspoutcomedeposit\n\n\n    11162111111111111111110\n    000000000000000000\n\n\n\n\n\n\n\n\nanyNA(data)\n\nFALSE"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#data-visualization",
    "href": "036-supervised-learning-desicion-trees-classification.html#data-visualization",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.3 Data visualization",
    "text": "7.3 Data visualization\nВік клієнта (age):\n\nlibrary(ggplot2)\n\nggplot(data, aes(age)) + \n    geom_histogram(bins = 20, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nРобота клієнта (job):\n\nggplot(data, aes(job)) + \n    geom_bar(aes(fill = job)) + \n    theme_bw()\n\n\n\n\n\nlibrary(gmodels)\nCrossTable(data$job, data$deposit)\n# more loyal to deposits are management, retired, student, unemployed ))\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n              | data$deposit \n     data$job |        no |       yes | Row Total | \n--------------|-----------|-----------|-----------|\n       admin. |       703 |       631 |      1334 | \n              |     0.002 |     0.002 |           | \n              |     0.527 |     0.473 |     0.120 | \n              |     0.120 |     0.119 |           | \n              |     0.063 |     0.057 |           | \n--------------|-----------|-----------|-----------|\n  blue-collar |      1236 |       708 |      1944 | \n              |    44.415 |    49.320 |           | \n              |     0.636 |     0.364 |     0.174 | \n              |     0.210 |     0.134 |           | \n              |     0.111 |     0.063 |           | \n--------------|-----------|-----------|-----------|\n entrepreneur |       205 |       123 |       328 | \n              |     6.090 |     6.762 |           | \n              |     0.625 |     0.375 |     0.029 | \n              |     0.035 |     0.023 |           | \n              |     0.018 |     0.011 |           | \n--------------|-----------|-----------|-----------|\n    housemaid |       165 |       109 |       274 | \n              |     3.010 |     3.343 |           | \n              |     0.602 |     0.398 |     0.025 | \n              |     0.028 |     0.021 |           | \n              |     0.015 |     0.010 |           | \n--------------|-----------|-----------|-----------|\n   management |      1265 |      1301 |      2566 | \n              |     5.367 |     5.960 |           | \n              |     0.493 |     0.507 |     0.230 | \n              |     0.215 |     0.246 |           | \n              |     0.113 |     0.117 |           | \n--------------|-----------|-----------|-----------|\n      retired |       262 |       516 |       778 | \n              |    53.042 |    58.899 |           | \n              |     0.337 |     0.663 |     0.070 | \n              |     0.045 |     0.098 |           | \n              |     0.023 |     0.046 |           | \n--------------|-----------|-----------|-----------|\nself-employed |       218 |       187 |       405 | \n              |     0.113 |     0.125 |           | \n              |     0.538 |     0.462 |     0.036 | \n              |     0.037 |     0.035 |           | \n              |     0.020 |     0.017 |           | \n--------------|-----------|-----------|-----------|\n     services |       554 |       369 |       923 | \n              |     9.621 |    10.683 |           | \n              |     0.600 |     0.400 |     0.083 | \n              |     0.094 |     0.070 |           | \n              |     0.050 |     0.033 |           | \n--------------|-----------|-----------|-----------|\n      student |        91 |       269 |       360 | \n              |    51.136 |    56.782 |           | \n              |     0.253 |     0.747 |     0.032 | \n              |     0.015 |     0.051 |           | \n              |     0.008 |     0.024 |           | \n--------------|-----------|-----------|-----------|\n   technician |       983 |       840 |      1823 | \n              |     0.591 |     0.656 |           | \n              |     0.539 |     0.461 |     0.163 | \n              |     0.167 |     0.159 |           | \n              |     0.088 |     0.075 |           | \n--------------|-----------|-----------|-----------|\n   unemployed |       155 |       202 |       357 | \n              |     5.741 |     6.375 |           | \n              |     0.434 |     0.566 |     0.032 | \n              |     0.026 |     0.038 |           | \n              |     0.014 |     0.018 |           | \n--------------|-----------|-----------|-----------|\n      unknown |        36 |        34 |        70 | \n              |     0.019 |     0.021 |           | \n              |     0.514 |     0.486 |     0.006 | \n              |     0.006 |     0.006 |           | \n              |     0.003 |     0.003 |           | \n--------------|-----------|-----------|-----------|\n Column Total |      5873 |      5289 |     11162 | \n              |     0.526 |     0.474 |           | \n--------------|-----------|-----------|-----------|\n\n \n\n\nСімейний статус (marital):\n\nggplot(data, aes(marital)) + \n    geom_bar(aes(fill = marital)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$marital, data$deposit)\n# married are not very loyal to deposits\n# but singles is more loyal\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n             | data$deposit \ndata$marital |        no |       yes | Row Total | \n-------------|-----------|-----------|-----------|\n    divorced |       671 |       622 |      1293 | \n             |     0.128 |     0.142 |           | \n             |     0.519 |     0.481 |     0.116 | \n             |     0.114 |     0.118 |           | \n             |     0.060 |     0.056 |           | \n-------------|-----------|-----------|-----------|\n     married |      3596 |      2755 |      6351 | \n             |    19.361 |    21.499 |           | \n             |     0.566 |     0.434 |     0.569 | \n             |     0.612 |     0.521 |           | \n             |     0.322 |     0.247 |           | \n-------------|-----------|-----------|-----------|\n      single |      1606 |      1912 |      3518 | \n             |    32.436 |    36.018 |           | \n             |     0.457 |     0.543 |     0.315 | \n             |     0.273 |     0.362 |           | \n             |     0.144 |     0.171 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |      5873 |      5289 |     11162 | \n             |     0.526 |     0.474 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nОсвіта (education):\n\nggplot(data, aes(education)) + \n    geom_bar(aes(fill = education)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$education, data$deposit)\n# people with tertiary education is more loyal than other groups\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n               | data$deposit \ndata$education |        no |       yes | Row Total | \n---------------|-----------|-----------|-----------|\n       primary |       909 |       591 |      1500 | \n               |    18.172 |    20.179 |           | \n               |     0.606 |     0.394 |     0.134 | \n               |     0.155 |     0.112 |           | \n               |     0.081 |     0.053 |           | \n---------------|-----------|-----------|-----------|\n     secondary |      3026 |      2450 |      5476 | \n               |     7.272 |     8.075 |           | \n               |     0.553 |     0.447 |     0.491 | \n               |     0.515 |     0.463 |           | \n               |     0.271 |     0.219 |           | \n---------------|-----------|-----------|-----------|\n      tertiary |      1693 |      1996 |      3689 | \n               |    31.688 |    35.187 |           | \n               |     0.459 |     0.541 |     0.330 | \n               |     0.288 |     0.377 |           | \n               |     0.152 |     0.179 |           | \n---------------|-----------|-----------|-----------|\n       unknown |       245 |       252 |       497 | \n               |     1.041 |     1.156 |           | \n               |     0.493 |     0.507 |     0.045 | \n               |     0.042 |     0.048 |           | \n               |     0.022 |     0.023 |           | \n---------------|-----------|-----------|-----------|\n  Column Total |      5873 |      5289 |     11162 | \n               |     0.526 |     0.474 |           | \n---------------|-----------|-----------|-----------|\n\n \n\n\nДефолт (default):\n\nggplot(data, aes(default)) + \n    geom_bar(aes(fill = default)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$default, data$deposit)\n# defaults not very loyal to deposits, but why? ))))))\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n             | data$deposit \ndata$default |        no |       yes | Row Total | \n-------------|-----------|-----------|-----------|\n          no |      5757 |      5237 |     10994 | \n             |     0.132 |     0.146 |           | \n             |     0.524 |     0.476 |     0.985 | \n             |     0.980 |     0.990 |           | \n             |     0.516 |     0.469 |           | \n-------------|-----------|-----------|-----------|\n         yes |       116 |        52 |       168 | \n             |     8.621 |     9.573 |           | \n             |     0.690 |     0.310 |     0.015 | \n             |     0.020 |     0.010 |           | \n             |     0.010 |     0.005 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |      5873 |      5289 |     11162 | \n             |     0.526 |     0.474 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nБаланс (balance):\n\nggplot(data, aes(balance)) + \n    geom_histogram(bins = 30, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n# looks like balance data has outliers\n\n\n\n\nНаявність кредиту на житло (housing):\n\nggplot(data, aes(housing)) + \n    geom_bar(aes(fill = housing)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$housing, data$deposit)\n# people without housing load logicaly more often can do deposits\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n             | data$deposit \ndata$housing |        no |       yes | Row Total | \n-------------|-----------|-----------|-----------|\n          no |      2527 |      3354 |      5881 | \n             |   104.023 |   115.509 |           | \n             |     0.430 |     0.570 |     0.527 | \n             |     0.430 |     0.634 |           | \n             |     0.226 |     0.300 |           | \n-------------|-----------|-----------|-----------|\n         yes |      3346 |      1935 |      5281 | \n             |   115.842 |   128.633 |           | \n             |     0.634 |     0.366 |     0.473 | \n             |     0.570 |     0.366 |           | \n             |     0.300 |     0.173 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |      5873 |      5289 |     11162 | \n             |     0.526 |     0.474 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nНаявність позики (loan):\n\nggplot(data, aes(loan)) + \n    geom_bar(aes(fill = loan)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$loan, data$deposit)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n             | data$deposit \n   data$loan |        no |       yes | Row Total | \n-------------|-----------|-----------|-----------|\n          no |      4897 |      4805 |      9702 | \n             |     8.459 |     9.393 |           | \n             |     0.505 |     0.495 |     0.869 | \n             |     0.834 |     0.908 |           | \n             |     0.439 |     0.430 |           | \n-------------|-----------|-----------|-----------|\n         yes |       976 |       484 |      1460 | \n             |    56.214 |    62.421 |           | \n             |     0.668 |     0.332 |     0.131 | \n             |     0.166 |     0.092 |           | \n             |     0.087 |     0.043 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |      5873 |      5289 |     11162 | \n             |     0.526 |     0.474 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\n# Тип комунікації (contact):\n\nggplot(data, aes(contact)) + \n    geom_bar(aes(fill = contact)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$contact, data$deposit)\n# cellular communication channel looks like the best way to increase deposits count\n# people with cellular devices has more money? \n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n             | data$deposit \ndata$contact |        no |       yes | Row Total | \n-------------|-----------|-----------|-----------|\n    cellular |      3673 |      4369 |      8042 | \n             |    73.685 |    81.821 |           | \n             |     0.457 |     0.543 |     0.720 | \n             |     0.625 |     0.826 |           | \n             |     0.329 |     0.391 |           | \n-------------|-----------|-----------|-----------|\n   telephone |       384 |       390 |       774 | \n             |     1.327 |     1.474 |           | \n             |     0.496 |     0.504 |     0.069 | \n             |     0.065 |     0.074 |           | \n             |     0.034 |     0.035 |           | \n-------------|-----------|-----------|-----------|\n     unknown |      1816 |       530 |      2346 | \n             |   274.060 |   304.321 |           | \n             |     0.774 |     0.226 |     0.210 | \n             |     0.309 |     0.100 |           | \n             |     0.163 |     0.047 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |      5873 |      5289 |     11162 | \n             |     0.526 |     0.474 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nДень місяця (day):\n\nggplot(data, aes(day)) + \n    geom_histogram(bins = 25, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nМісяць (month):\n\nggplot(data, aes(month)) + \n    geom_bar(aes(fill = month)) + \n    theme_bw()\n\n\n\n\n\n# So, lets replace our month with ordered factor for correct visualization\nsuppressMessages(library(dplyr))\ndata <- data |>\n    mutate(month = factor(month, levels=c(\"jan\",\"feb\",\"mar\",\n               \"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\n              \"oct\",\"nov\",\"dec\"),ordered=TRUE))\n\n\nggplot(data, aes(month)) + \n    geom_bar(aes(fill = month)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$month, data$deposit)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n             | data$deposit \n  data$month |        no |       yes | Row Total | \n-------------|-----------|-----------|-----------|\n         jan |       202 |       142 |       344 | \n             |     2.437 |     2.706 |           | \n             |     0.587 |     0.413 |     0.031 | \n             |     0.034 |     0.027 |           | \n             |     0.018 |     0.013 |           | \n-------------|-----------|-----------|-----------|\n         feb |       335 |       441 |       776 | \n             |    13.159 |    14.612 |           | \n             |     0.432 |     0.568 |     0.070 | \n             |     0.057 |     0.083 |           | \n             |     0.030 |     0.040 |           | \n-------------|-----------|-----------|-----------|\n         mar |        28 |       248 |       276 | \n             |    94.619 |   105.067 |           | \n             |     0.101 |     0.899 |     0.025 | \n             |     0.005 |     0.047 |           | \n             |     0.003 |     0.022 |           | \n-------------|-----------|-----------|-----------|\n         apr |       346 |       577 |       923 | \n             |    40.155 |    44.588 |           | \n             |     0.375 |     0.625 |     0.083 | \n             |     0.059 |     0.109 |           | \n             |     0.031 |     0.052 |           | \n-------------|-----------|-----------|-----------|\n         may |      1899 |       925 |      2824 | \n             |   114.862 |   127.545 |           | \n             |     0.672 |     0.328 |     0.253 | \n             |     0.323 |     0.175 |           | \n             |     0.170 |     0.083 |           | \n-------------|-----------|-----------|-----------|\n         jun |       676 |       546 |      1222 | \n             |     1.697 |     1.884 |           | \n             |     0.553 |     0.447 |     0.109 | \n             |     0.115 |     0.103 |           | \n             |     0.061 |     0.049 |           | \n-------------|-----------|-----------|-----------|\n         jul |       887 |       627 |      1514 | \n             |    10.257 |    11.390 |           | \n             |     0.586 |     0.414 |     0.136 | \n             |     0.151 |     0.119 |           | \n             |     0.079 |     0.056 |           | \n-------------|-----------|-----------|-----------|\n         aug |       831 |       688 |      1519 | \n             |     1.262 |     1.402 |           | \n             |     0.547 |     0.453 |     0.136 | \n             |     0.141 |     0.130 |           | \n             |     0.074 |     0.062 |           | \n-------------|-----------|-----------|-----------|\n         sep |        50 |       269 |       319 | \n             |    82.740 |    91.876 |           | \n             |     0.157 |     0.843 |     0.029 | \n             |     0.009 |     0.051 |           | \n             |     0.004 |     0.024 |           | \n-------------|-----------|-----------|-----------|\n         oct |        69 |       323 |       392 | \n             |    91.338 |   101.423 |           | \n             |     0.176 |     0.824 |     0.035 | \n             |     0.012 |     0.061 |           | \n             |     0.006 |     0.029 |           | \n-------------|-----------|-----------|-----------|\n         nov |       540 |       403 |       943 | \n             |     3.872 |     4.300 |           | \n             |     0.573 |     0.427 |     0.084 | \n             |     0.092 |     0.076 |           | \n             |     0.048 |     0.036 |           | \n-------------|-----------|-----------|-----------|\n         dec |        10 |       100 |       110 | \n             |    39.605 |    43.979 |           | \n             |     0.091 |     0.909 |     0.010 | \n             |     0.002 |     0.019 |           | \n             |     0.001 |     0.009 |           | \n-------------|-----------|-----------|-----------|\nColumn Total |      5873 |      5289 |     11162 | \n             |     0.526 |     0.474 |           | \n-------------|-----------|-----------|-----------|\n\n \n\n\nТривалість останнього контакту (duration):\n\nggplot(data, aes(duration)) + \n    geom_histogram(bins = 100, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nКількість контактів протягом поточної кампанії (campaign):\n\nggplot(data, aes(campaign)) + \n    geom_histogram(bins = 30, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nКількість днів від попередньої акції (pday):\n\nggplot(data, aes(pdays)) + \n    geom_histogram(bins = 20, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nКількість контактів до початку поточної кампанії (previous):\n\nggplot(data, aes(previous)) + \n    geom_histogram(bins = 50, alpha = 0.5, fill = 'blue', color='black')  + \n    theme_bw()\n\n\n\n\nРезультат попередньої кампанії (poutcome):\n\nggplot(data, aes(poutcome)) + \n    geom_bar(aes(fill = poutcome)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$poutcome, data$deposit)\n# people with previous success status also loyal for new propositions\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n              | data$deposit \ndata$poutcome |        no |       yes | Row Total | \n--------------|-----------|-----------|-----------|\n      failure |       610 |       618 |      1228 | \n              |     2.020 |     2.243 |           | \n              |     0.497 |     0.503 |     0.110 | \n              |     0.104 |     0.117 |           | \n              |     0.055 |     0.055 |           | \n--------------|-----------|-----------|-----------|\n        other |       230 |       307 |       537 | \n              |     9.773 |    10.852 |           | \n              |     0.428 |     0.572 |     0.048 | \n              |     0.039 |     0.058 |           | \n              |     0.021 |     0.028 |           | \n--------------|-----------|-----------|-----------|\n      success |        93 |       978 |      1071 | \n              |   392.866 |   436.245 |           | \n              |     0.087 |     0.913 |     0.096 | \n              |     0.016 |     0.185 |           | \n              |     0.008 |     0.088 |           | \n--------------|-----------|-----------|-----------|\n      unknown |      4940 |      3386 |      8326 | \n              |    71.378 |    79.259 |           | \n              |     0.593 |     0.407 |     0.746 | \n              |     0.841 |     0.640 |           | \n              |     0.443 |     0.303 |           | \n--------------|-----------|-----------|-----------|\n Column Total |      5873 |      5289 |     11162 | \n              |     0.526 |     0.474 |           | \n--------------|-----------|-----------|-----------|\n\n \n\n\nРезультат укладання або відсутність укладання договору (deposit):\n\nggplot(data, aes(deposit)) + \n    geom_bar(aes(fill = deposit)) + \n    theme_bw()\n\n\n\n\n\nCrossTable(data$deposit)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  11162 \n\n \n          |        no |       yes | \n          |-----------|-----------|\n          |      5873 |      5289 | \n          |     0.526 |     0.474 | \n          |-----------|-----------|"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#data-preprocessing",
    "href": "036-supervised-learning-desicion-trees-classification.html#data-preprocessing",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.4 Data preprocessing",
    "text": "7.4 Data preprocessing\nПеретворимо значення deposit до 0 і 1:\n\ndata$deposit <- ifelse(data$deposit == \"yes\", 1, 0)\n\nВидалимо duration, адже цей параметр чітко вказує на факт укладання угоди, такі дані називаються leak:\n\ndata$duration <- NULL\n\nСтворимо новий параметр pdays_flag, який вказує чи був контакт з клієнтом раніше:\n\ndata$pdays_flag <- ifelse(data$pdays > 0, 1, 0)\n\n\nhead(data)\n\n\n\nA data.frame: 6 × 17\n\n    agejobmaritaleducationdefaultbalancehousingloancontactdaymonthcampaignpdayspreviouspoutcomedepositpdays_flag\n    <int><fct><fct><fct><fct><int><fct><fct><fct><int><ord><int><int><int><fct><dbl><dbl>\n\n\n    159admin.    marriedsecondaryno2343yesno unknown5may1-10unknown10\n    256admin.    marriedsecondaryno  45no no unknown5may1-10unknown10\n    341technicianmarriedsecondaryno1270yesno unknown5may1-10unknown10\n    455services  marriedsecondaryno2476yesno unknown5may1-10unknown10\n    554admin.    marriedtertiary no 184no no unknown5may2-10unknown10\n    642managementsingle tertiary no   0yesyesunknown5may2-10unknown10\n\n\n\n\nСтворимо новий параметр poutcome_success, який вказує чи була попередня кампанія з цим клієнтом “успішною для банку”:\n\ndata$poutcoume_success <- ifelse(data$poutcome == \"success\", 1, 0)"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#traintest-split",
    "href": "036-supervised-learning-desicion-trees-classification.html#traintest-split",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.5 Train/test split",
    "text": "7.5 Train/test split\nЗадаємо seed для генератора випадкових чисел\nTrain 65%, test 35%\n\nset.seed(111) \nsuppressMessages(library(caret))\nindex = createDataPartition(data$deposit, p = 0.65, list = FALSE)\ntrain_data = data[index, ]\ntest_data = data[-index, ]\n\n\nCrossTable(train_data$deposit)\nCrossTable(test_data$deposit)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  7256 \n\n \n          |         0 |         1 | \n          |-----------|-----------|\n          |      3816 |      3440 | \n          |     0.526 |     0.474 | \n          |-----------|-----------|\n\n\n\n \n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  3906 \n\n \n          |         0 |         1 | \n          |-----------|-----------|\n          |      2057 |      1849 | \n          |     0.527 |     0.473 | \n          |-----------|-----------|"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#decision-trees-with-rpart",
    "href": "036-supervised-learning-desicion-trees-classification.html#decision-trees-with-rpart",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.6 Decision trees with rpart()",
    "text": "7.6 Decision trees with rpart()\nДля побудови дерев рішень у R є ряд пакетів та алгоритмів. Розглянемо пакет rpart.\n\n#install.packages(\"rpart\")\nlibrary(rpart)\nrpart_model <- rpart(deposit ~ ., train_data)\n\nВиведемо опис моделі:\n\nrpart_model\n\nn= 7256 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 7256 1809.12900 0.4740904  \n   2) poutcome=failure,other,unknown 6563 1605.86300 0.4270913  \n     4) contact=unknown 1543  272.24240 0.2287751 *\n     5) contact=cellular,telephone 5020 1254.28300 0.4880478  \n      10) housing=yes 2139  507.93360 0.3880318 *\n      11) housing=no 2881  709.06630 0.5623048  \n        22) balance< 105.5 636  150.38990 0.3836478 *\n        23) balance>=105.5 2245  532.62540 0.6129176 *\n   3) poutcome=success 693   51.47475 0.9191919 *\n\n\nДуже детальний опис:\n\nsummary(rpart_model)\n\nCall:\nrpart(formula = deposit ~ ., data = train_data)\n  n= 7256 \n\n          CP nsplit rel error    xerror        xstd\n1 0.08390285      0 1.0000000 1.0004337 0.001224021\n2 0.04385421      1 0.9160972 0.9165627 0.005207633\n3 0.02060824      2 0.8722429 0.8731088 0.006530786\n4 0.01439973      3 0.8516347 0.8528095 0.007275812\n5 0.01000000      4 0.8372350 0.8459897 0.007777416\n\nVariable importance\n         poutcome poutcoume_success           contact           housing \n               33                33                17                 8 \n          balance               job             pdays             month \n                6                 1                 1                 1 \n\nNode number 1: 7256 observations,    complexity param=0.08390285\n  mean=0.4740904, MSE=0.2493287 \n  left son=2 (6563 obs) right son=3 (693 obs)\n  Primary splits:\n      poutcome          splits as  LLRL,      improve=0.08390285, (0 missing)\n      poutcoume_success < 0.5   to the left,  improve=0.08390285, (0 missing)\n      contact           splits as  RRL,       improve=0.06524266, (0 missing)\n      pdays             < 9.5   to the left,  improve=0.04926922, (0 missing)\n      previous          < 0.5   to the left,  improve=0.04820004, (0 missing)\n  Surrogate splits:\n      poutcoume_success < 0.5   to the left,  agree=1.000, adj=1.000, (0 split)\n      age               < 91    to the left,  agree=0.905, adj=0.003, (0 split)\n\nNode number 2: 6563 observations,    complexity param=0.04385421\n  mean=0.4270913, MSE=0.2446843 \n  left son=4 (1543 obs) right son=5 (5020 obs)\n  Primary splits:\n      contact splits as  RRL,          improve=0.04940515, (0 missing)\n      housing splits as  RL,           improve=0.03656129, (0 missing)\n      age     < 60.5  to the left,     improve=0.02289822, (0 missing)\n      job     splits as  LLLLLRLLRLRL, improve=0.02007437, (0 missing)\n      balance < 798   to the left,     improve=0.01917691, (0 missing)\n  Surrogate splits:\n      campaign < 24.5  to the right, agree=0.766, adj=0.003, (0 split)\n\nNode number 3: 693 observations\n  mean=0.9191919, MSE=0.07427813 \n\nNode number 4: 1543 observations\n  mean=0.2287751, MSE=0.1764371 \n\nNode number 5: 5020 observations,    complexity param=0.02060824\n  mean=0.4880478, MSE=0.2498571 \n  left son=10 (2139 obs) right son=11 (2881 obs)\n  Primary splits:\n      housing splits as  RL,           improve=0.02972452, (0 missing)\n      balance < 799.5 to the left,     improve=0.02236410, (0 missing)\n      job     splits as  LLLLLRLLRLRL, improve=0.02093907, (0 missing)\n      age     < 59.5  to the left,     improve=0.02067340, (0 missing)\n      loan    splits as  RL,           improve=0.01407420, (0 missing)\n  Surrogate splits:\n      job      splits as  LLRRRRRLRRRR, agree=0.630, adj=0.132, (0 split)\n      pdays    < 165.5 to the right,    agree=0.623, adj=0.115, (0 split)\n      month    splits as  LLLLLRRRRRRR, agree=0.615, adj=0.096, (0 split)\n      poutcome splits as  LR-R,         agree=0.604, adj=0.071, (0 split)\n      previous < 0.5   to the right,    agree=0.599, adj=0.059, (0 split)\n\nNode number 10: 2139 observations\n  mean=0.3880318, MSE=0.2374631 \n\nNode number 11: 2881 observations,    complexity param=0.01439973\n  mean=0.5623048, MSE=0.2461181 \n  left son=22 (636 obs) right son=23 (2245 obs)\n  Primary splits:\n      balance < 105.5 to the left,     improve=0.03673982, (0 missing)\n      loan    splits as  RL,           improve=0.03447328, (0 missing)\n      month   splits as  RRRRRRLLLLLL, improve=0.03024729, (0 missing)\n      job     splits as  LLLLLRLLRLRL, improve=0.02005774, (0 missing)\n      age     < 60.5  to the left,     improve=0.01886179, (0 missing)\n  Surrogate splits:\n      default splits as  RL, agree=0.787, adj=0.035, (0 split)\n\nNode number 22: 636 observations\n  mean=0.3836478, MSE=0.2364622 \n\nNode number 23: 2245 observations\n  mean=0.6129176, MSE=0.2372496 \n\n\n\nВізуалізуємо дерево рішень:\n\n# install.packages(c(\"rattle\", \"RColorBrewer\"))\n\nsuppressMessages(library(rattle))\nsuppressMessages(library(RColorBrewer))\nfancyRpartPlot(rpart_model)\n\n# now you can see how model model works\n\n\n\n\nСтворимо два дата-фрейм для для запису результатів моделювання на тестовій вибірці. Одразу додамо у набори даних реальні значення результатів маркетингової акції deposit та модельовані значення\nДані тренувальної вибірки будуть використовуватися для визначення оптимальної cutoff лінії, а тестової для порівняння моделей між собою.\n\ntrain_results <- data.frame(No = c(1:nrow(train_data)), \n                            deposit = train_data$deposit, \n                            RPartPredicted = predict(rpart_model, train_data))\n\ntest_results <- data.frame(No = c(1:nrow(test_data)),\n                           deposit = test_data$deposit, \n                           RPartPredicted = predict(rpart_model, test_data))\n\nhead(test_results)\n\n\n\nA data.frame: 6 × 3\n\n    NodepositRPartPredicted\n    <int><dbl><dbl>\n\n\n    1110.2287751\n    2210.2287751\n    4310.2287751\n    7410.2287751\n    8510.2287751\n    9610.2287751\n\n\n\n\nВизначимо оптимальну лінію розподілу на 0 і 1 для тренувальної вибірки за допомогою пакету InformationValue:\n\nsuppressMessages(library(InformationValue))\noptCutOff <- optimalCutoff(train_results$deposit, train_results$RPartPredicted)\noptCutOff\n\n0.389191919191919\n\n\nПобудуємо ROC-криву для тестової вибірки:\n\nplotROC(test_results$deposit, test_results$RPartPredicted)\n\n\n\n\nСформуємо набір класів 0 і 1 для тестового набору даних:\n\ntest_results$RPartPredicted_Class <- ifelse(test_results$RPartPredicted > optCutOff, 1, 0)\n\nConfusion matrix:\n\ncm <- caret::confusionMatrix(factor(test_results$deposit), \n                      factor(test_results$RPartPredicted_Class), \n                      positive = \"1\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1504  553\n         1  793 1056\n                                          \n               Accuracy : 0.6554          \n                 95% CI : (0.6403, 0.6703)\n    No Information Rate : 0.5881          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.3043          \n                                          \n Mcnemar's Test P-Value : 7.297e-11       \n                                          \n            Sensitivity : 0.6563          \n            Specificity : 0.6548          \n         Pos Pred Value : 0.5711          \n         Neg Pred Value : 0.7312          \n             Prevalence : 0.4119          \n         Detection Rate : 0.2704          \n   Detection Prevalence : 0.4734          \n      Balanced Accuracy : 0.6555          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nПереглянемо збалансовану точність класифіції:\n\nBAcc <- cm$byClass[[\"Balanced Accuracy\"]]\nBAcc \n\n0.655537676754586"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#desicion-trees-with-partykit",
    "href": "036-supervised-learning-desicion-trees-classification.html#desicion-trees-with-partykit",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.7 Desicion trees with PartyKit",
    "text": "7.7 Desicion trees with PartyKit\nПобудуємо дерево рішень за допоомгою пакету partykit:\n\n#install.packages(\"partykit\")\nsuppressMessages(library(partykit))\nhead(train_data)\nparty_model <- ctree(deposit ~ ., data = train_data)\n\n\n\nA data.frame: 6 × 18\n\n    agejobmaritaleducationdefaultbalancehousingloancontactdaymonthcampaignpdayspreviouspoutcomedepositpdays_flagpoutcoume_success\n    <int><fct><fct><fct><fct><int><fct><fct><fct><int><ord><int><int><int><fct><dbl><dbl><dbl>\n\n\n    341technician marriedsecondaryno1270yesno unknown5may1-10unknown100\n    554admin.     marriedtertiary no 184no no unknown5may2-10unknown100\n    642management single tertiary no   0yesyesunknown5may2-10unknown100\n    1138admin.     single secondaryno 100yesno unknown7may1-10unknown100\n    1230blue-collarmarriedsecondaryno 309yesno unknown7may2-10unknown100\n    1446blue-collarsingle tertiary no 460yesno unknown7may2-10unknown100\n\n\n\n\nВиведемо текстовий опис моделі:\n\nparty_model\n# Looks like this model is more complex\n\n\nModel formula:\ndeposit ~ age + job + marital + education + default + balance + \n    housing + loan + contact + day + month + campaign + pdays + \n    previous + poutcome + pdays_flag + poutcoume_success\n\nFitted party:\n[1] root\n|   [2] poutcome in failure, other, unknown\n|   |   [3] contact in cellular, telephone\n|   |   |   [4] housing in no\n|   |   |   |   [5] loan in no\n|   |   |   |   |   [6] campaign <= 3\n|   |   |   |   |   |   [7] job in admin., management, retired, student, unemployed\n|   |   |   |   |   |   |   [8] day <= 17: 0.733 (n = 715, err = 140.0)\n|   |   |   |   |   |   |   [9] day > 17\n|   |   |   |   |   |   |   |   [10] job in admin., management, retired, unemployed: 0.583 (n = 434, err = 105.5)\n|   |   |   |   |   |   |   |   [11] job in student: 0.843 (n = 51, err = 6.7)\n|   |   |   |   |   |   [12] job in blue-collar, entrepreneur, housemaid, self-employed, services, technician, unknown\n|   |   |   |   |   |   |   [13] campaign <= 1: 0.612 (n = 415, err = 98.5)\n|   |   |   |   |   |   |   [14] campaign > 1\n|   |   |   |   |   |   |   |   [15] balance <= 106\n|   |   |   |   |   |   |   |   |   [16] age <= 57: 0.234 (n = 94, err = 16.9)\n|   |   |   |   |   |   |   |   |   [17] age > 57: 0.700 (n = 10, err = 2.1)\n|   |   |   |   |   |   |   |   [18] balance > 106: 0.561 (n = 321, err = 79.1)\n|   |   |   |   |   [19] campaign > 3\n|   |   |   |   |   |   [20] marital in divorced, single: 0.579 (n = 195, err = 47.5)\n|   |   |   |   |   |   [21] marital in married\n|   |   |   |   |   |   |   [22] campaign <= 5: 0.474 (n = 171, err = 42.6)\n|   |   |   |   |   |   |   [23] campaign > 5: 0.299 (n = 154, err = 32.3)\n|   |   |   |   [24] loan in yes: 0.302 (n = 321, err = 67.7)\n|   |   |   [25] housing in yes\n|   |   |   |   [26] campaign <= 3\n|   |   |   |   |   [27] marital in divorced, single: 0.455 (n = 759, err = 188.2)\n|   |   |   |   |   [28] marital in married: 0.372 (n = 985, err = 230.0)\n|   |   |   |   [29] campaign > 3: 0.301 (n = 395, err = 83.1)\n|   |   [30] contact in unknown\n|   |   |   [31] month <= sep\n|   |   |   |   [32] marital in divorced, single: 0.275 (n = 619, err = 123.3)\n|   |   |   |   [33] marital in married: 0.183 (n = 903, err = 134.9)\n|   |   |   [34] month > sep: 0.857 (n = 21, err = 2.6)\n|   [35] poutcome in success\n|   |   [36] contact in cellular: 0.925 (n = 637, err = 44.4)\n|   |   [37] contact in telephone, unknown: 0.857 (n = 56, err = 6.9)\n\nNumber of inner nodes:    18\nNumber of terminal nodes: 19\n\n\nВізуалізуємо побудоване дерево рішень:\n\nplot(party_model)\n\n\n\n\nКонвернтуємо ctree() до rpart():\n\nst <- as.simpleparty(party_model)\nplot(st)\n\n\n\n\nДодамо прогнозовані показники до раніше створених дата-фрейму для збору результатів:\n\ntrain_results$PartyPredicted <- predict(party_model, train_data)\ntest_results$PartyPredicted <- predict(party_model, test_data)\nhead(test_results)\n\n\n\nA data.frame: 6 × 5\n\n    NodepositRPartPredictedRPartPredicted_ClassPartyPredicted\n    <int><dbl><dbl><dbl><dbl>\n\n\n    1110.228775100.1827243\n    2210.228775100.1827243\n    4310.228775100.1827243\n    7410.228775100.1827243\n    8510.228775100.2746365\n    9610.228775100.1827243\n\n\n\n\nВизначимо оптимальну лінію розділення на класи 0 і 1:\n\noptCutOff <- optimalCutoff(train_results$deposit, train_results$PartyPredicted)\noptCutOff\n\n0.474646781789639\n\n\nROC-крива та AUROC:\n\nplotROC(test_results$deposit, test_results$PartyPredicted)\n\n\n\n\nРозділимо результати прогнозування на класи:\n\ntest_results$PartyPredicted_Class <- ifelse(test_results$PartyPredicted > optCutOff, 1, 0)\n\nConfusion matrix:\n\ncm <- caret::confusionMatrix(factor(test_results$deposit), \n                             factor(test_results$PartyPredicted_Class), \n                             positive = \"1\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1535  522\n         1  803 1046\n                                          \n               Accuracy : 0.6608          \n                 95% CI : (0.6457, 0.6756)\n    No Information Rate : 0.5986          \n    P-Value [Acc > NIR] : 6.520e-16       \n                                          \n                  Kappa : 0.3144          \n                                          \n Mcnemar's Test P-Value : 1.446e-14       \n                                          \n            Sensitivity : 0.6671          \n            Specificity : 0.6565          \n         Pos Pred Value : 0.5657          \n         Neg Pred Value : 0.7462          \n             Prevalence : 0.4014          \n         Detection Rate : 0.2678          \n   Detection Prevalence : 0.4734          \n      Balanced Accuracy : 0.6618          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nОцінимо збалансовану точність класифікації:\n\nBAcc # value for previous model\n\n0.655537676754586\n\n\n\nBAcc1 <- cm$byClass[[\"Balanced Accuracy\"]]\nBAcc1\n\n0.661817945741171"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#desision-tree-with-c50",
    "href": "036-supervised-learning-desicion-trees-classification.html#desision-tree-with-c50",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.8 Desision Tree with c50",
    "text": "7.8 Desision Tree with c50\nСкористаємося алгоритмом C50 для побудови дерева рішень. Для початку потрібно виіхдний показник перетворити у категоріальний (factor):\n\n# lets make anew temporary dataset for modeling with target output as factor\n\n\ntrain_data_tmp <- train_data %>%\n    mutate(deposit = factor(train_data$deposit, levels = c(0,1)))\n\ntest_data_tmp <- test_data %>%\n    mutate(deposit = factor(test_data$deposit, levels = c(0,1)))\n\nПобудуємо модель:\n\nlibrary(C50)\nc5_model <- C5.0(deposit ~ ., data = train_data_tmp)\n\nПереглянемо модель:\n\nsummary(c5_model)\n# its hard to check the nodes\n\n\nCall:\nC5.0.formula(formula = deposit ~ ., data = train_data_tmp)\n\n\nC5.0 [Release 2.07 GPL Edition]     Mon Oct 03 18:54:18 2022\n-------------------------------\n\nClass specified by attribute `outcome'\n\nRead 7256 cases (18 attributes) from undefined.data\n\nDecision tree:\n\npoutcoume_success > 0: 1 (693/56)\npoutcoume_success <= 0:\n:...pdays > 374: 1 (75/10)\n    pdays <= 374:\n    :...age > 60: 1 (292/66)\n        age <= 60:\n        :...contact = unknown:\n            :...poutcome = success: 0 (0)\n            :   poutcome in {failure,other}: 1 (3)\n            :   poutcome = unknown:\n            :   :...month in [oct-dec]: 1 (20/2)\n            :       month in [jan-sep]:\n            :       :...month in [jan-apr]: 1 (10/1)\n            :           month in [may-sep]:\n            :           :...marital = married: 0 (893/157)\n            :               marital in {divorced,single}:\n            :               :...default = yes:\n            :                   :...marital = divorced: 0 (7/2)\n            :                   :   marital = single:\n            :                   :   :...day <= 16: 1 (6)\n            :                   :       day > 16: 0 (7/2)\n            :                   default = no:\n            :                   :...day <= 29: 0 (564/139)\n            :                       day > 29:\n            :                       :...campaign > 7: 1 (4)\n            :                           campaign <= 7:\n            :                           :...campaign <= 1: 1 (5/1)\n            :                               campaign > 1: 0 (19/7)\n            contact in {cellular,telephone}:\n            :...month = dec: 1 (32/3)\n                month in [jan-nov]:\n                :...housing = yes:\n                    :...month = nov:\n                    :   :...day <= 16: 1 (23/7)\n                    :   :   day > 16: 0 (264/65)\n                    :   month in [jan-oct]:\n                    :   :...month in [sep-oct]: 1 (47/7)\n                    :       month in [jan-aug]:\n                    :       :...contact = telephone: 0 (110/17)\n                    :           contact = cellular:\n                    :           :...marital in {divorced,\n                    :               :           married}: 0 (1115/408)\n                    :               marital = single:\n                    :               :...pdays_flag > 0:\n                    :                   :...education in {primary,\n                    :                   :   :             unknown}: 0 (13/3)\n                    :                   :   education = secondary:\n                    :                   :   :...loan = no: 0 (69/19)\n                    :                   :   :   loan = yes: 1 (8/2)\n                    :                   :   education = tertiary:\n                    :                   :   :...poutcome = failure: 1 (55/24)\n                    :                   :       poutcome in {other,success,\n                    :                   :                    unknown}: 0 (17/3)\n                    :                   pdays_flag <= 0:\n                    :                   :...job in {entrepreneur,management,\n                    :                       :       retired,\n                    :                       :       services}: 0 (129/55)\n                    :                       job in {housemaid,self-employed,\n                    :                       :       student,unemployed,\n                    :                       :       unknown}: 1 (35/13)\n                    :                       job = admin.:\n                    :                       :...campaign <= 2: 1 (42/20)\n                    :                       :   campaign > 2:\n                    :                       :   :...loan = no: 0 (12)\n                    :                       :       loan = yes: 1 (4/1)\n                    :                       job = technician: [S1]\n                    :                       job = blue-collar:\n                    :                       :...loan = yes: 1 (10)\n                    :                           loan = no: [S2]\n                    housing = no:\n                    :...loan = yes:\n                        :...month = jan: 0 (20/1)\n                        :   month in [feb-nov]:\n                        :   :...age > 37: 0 (170/39)\n                        :       age <= 37:\n                        :       :...job in {admin.,housemaid,management,\n                        :           :       retired,self-employed,student,\n                        :           :       unknown}: 0 (58/20)\n                        :           job = unemployed: 1 (1)\n                        :           job = blue-collar:\n                        :           :...education in {primary,\n                        :           :   :             tertiary}: 0 (3)\n                        :           :   education in {secondary,\n                        :           :                 unknown}: 1 (20/5)\n                        :           job = entrepreneur:\n                        :           :...balance <= 935: 0 (5/1)\n                        :           :   balance > 935: 1 (2)\n                        :           job = services:\n                        :           :...day <= 9: 0 (6)\n                        :           :   day > 9: 1 (6/1)\n                        :           job = technician:\n                        :           :...age <= 29: 1 (11/3)\n                        :               age > 29: 0 (16/3)\n                        loan = no:\n                        :...balance <= 105:\n                            :...day <= 5:\n                            :   :...month in [jan-feb]: 0 (22/8)\n                            :   :   month in [mar-nov]: 1 (28/4)\n                            :   day > 5:\n                            :   :...month in [jan-apr]:\n                            :       :...month in [feb-apr]: 1 (42/10)\n                            :       :   month = jan:\n                            :       :   :...day <= 20: 1 (4)\n                            :       :       day > 20: 0 (32/9)\n                            :       month in [may-nov]:\n                            :       :...month in [may-aug]: 0 (283/72)\n                            :           month in [sep-nov]:\n                            :           :...day > 21: 1 (11)\n                            :               day <= 21:\n                            :               :...balance <= 86: 0 (45/16)\n                            :                   balance > 86: 1 (3)\n                            balance > 105:\n                            :...month in [jan-jun]:\n                                :...month in [mar-jun]:\n                                :   :...pdays <= 293: 1 (470/88)\n                                :   :   pdays > 293:\n                                :   :   :...marital = divorced: 1 (4)\n                                :   :       marital in {married,\n                                :   :                   single}: 0 (20/8)\n                                :   month in [jan-feb]:\n                                :   :...day > 27: 0 (67/17)\n                                :       day <= 27:\n                                :       :...day > 9: 1 (71/6)\n                                :           day <= 9: [S3]\n                                month in [jul-nov]:\n                                :...pdays_flag > 0:\n                                    :...month in [jul-oct]: 1 (101/15)\n                                    :   month = nov:\n                                    :   :...campaign > 2: 0 (4)\n                                    :       campaign <= 2:\n                                    :       :...day <= 17: 1 (20/2)\n                                    :           day > 17:\n                                    :           :...pdays <= 106: 1 (5/1)\n                                    :               pdays > 106: 0 (8)\n                                    pdays_flag <= 0:\n                                    :...age <= 29: 1 (92/14)\n                                        age > 29:\n                                        :...marital = divorced:\n                                            :...balance > 710: 1 (44/9)\n                                            :   balance <= 710: [S4]\n                                            marital in {married,single}:\n                                            :...campaign <= 1: [S5]\n                                                campaign > 1:\n                                                :...day <= 10:\n                                                    :...campaign > 7: 0 (5)\n                                                    :   campaign <= 7: [S6]\n                                                    day > 10: [S7]\n\nSubTree [S1]\n\neducation in {primary,unknown}: 0 (2)\neducation = tertiary:\n:...month in [jan-jul]: 1 (25/9)\n:   month = aug: 0 (4)\neducation = secondary:\n:...month = jan: 0 (3)\n    month in [feb-aug]:\n    :...month in [feb-apr]: 1 (16/3)\n        month in [may-aug]: 0 (25/10)\n\nSubTree [S2]\n\neducation = unknown: 0 (0)\neducation = tertiary: 1 (1)\neducation = primary:\n:...age <= 27: 1 (3)\n:   age > 27:\n:   :...age <= 46: 0 (13/4)\n:       age > 46: 1 (2)\neducation = secondary:\n:...age <= 23: 1 (4)\n    age > 23:\n    :...month in [jan-may]: 0 (24/5)\n        month in [jun-aug]:\n        :...day <= 14: 0 (6/2)\n            day > 14: 1 (6)\n\nSubTree [S3]\n\neducation = unknown: 0 (7/2)\neducation = primary:\n:...marital in {divorced,married}: 0 (10/2)\n:   marital = single: 1 (3)\neducation = tertiary:\n:...pdays <= 192: 1 (59/24)\n:   pdays > 192: 0 (5)\neducation = secondary:\n:...marital = divorced: 0 (3)\n    marital = married:\n    :...balance <= 1381: 0 (16/4)\n    :   balance > 1381: 1 (8/1)\n    marital = single:\n    :...day <= 7: 0 (27/5)\n        day > 7: 1 (7/1)\n\nSubTree [S4]\n\njob in {admin.,self-employed,unemployed}: 1 (9/1)\njob in {blue-collar,entrepreneur,housemaid,management,retired,services,student,\n:       unknown}: 0 (19/7)\njob = technician:\n:...age <= 54: 0 (11/2)\n    age > 54: 1 (3)\n\nSubTree [S5]\n\ncontact = telephone: 1 (16/1)\ncontact = cellular:\n:...education in {primary,unknown}: 0 (28/8)\n    education in {secondary,tertiary}: 1 (136/56)\n\nSubTree [S6]\n\njob in {admin.,retired,self-employed,student}: 1 (12/1)\njob in {entrepreneur,housemaid,unemployed,unknown}: 0 (7/1)\njob = blue-collar:\n:...campaign <= 5: 0 (8/1)\n:   campaign > 5: 1 (2)\njob = services:\n:...balance <= 755: 0 (3)\n:   balance > 755: 1 (4)\njob = technician:\n:...contact = cellular: 1 (19/4)\n:   contact = telephone: 0 (2)\njob = management:\n:...contact = telephone: 1 (2)\n    contact = cellular:\n    :...age <= 45: 1 (20/7)\n        age > 45: 0 (7/1)\n\nSubTree [S7]\n\njob in {admin.,housemaid,management,self-employed,services,student,\n:       unknown}: 0 (215/68)\njob = unemployed: 1 (10/3)\njob = blue-collar:\n:...marital = married: 0 (40/16)\n:   marital = single: 1 (3)\njob = entrepreneur:\n:...balance <= 1679: 0 (5)\n:   balance > 1679: 1 (4/1)\njob = retired:\n:...contact = telephone: 1 (3)\n:   contact = cellular:\n:   :...day <= 16: 1 (5)\n:       day > 16:\n:       :...age <= 53: 1 (2)\n:           age > 53: 0 (12/1)\njob = technician:\n:...education = primary: 0 (0)\n    education = unknown: 1 (1)\n    education = tertiary:\n    :...age <= 39: 0 (21/6)\n    :   age > 39: 1 (7/1)\n    education = secondary:\n    :...age <= 34: 0 (16)\n        age > 34:\n        :...marital = married: 0 (30/8)\n            marital = single:\n            :...day <= 21: 1 (8)\n                day > 21: 0 (5/1)\n\n\nEvaluation on training data (7256 cases):\n\n        Decision Tree   \n      ----------------  \n      Size      Errors  \n\n       124 1709(23.6%)   <<\n\n\n       (a)   (b)    <-classified as\n      ----  ----\n      3332   484    (a): class 0\n      1225  2215    (b): class 1\n\n\n    Attribute usage:\n\n    100.00% poutcoume_success\n     90.45% pdays\n     89.42% age\n     85.39% contact\n     85.35% month\n     63.75% housing\n     54.99% marital\n     37.22% loan\n     30.71% balance\n     30.04% day\n     22.19% poutcome\n     20.70% pdays_flag\n     13.91% job\n     10.76% campaign\n      9.87% education\n      8.43% default\n\n\nTime: 0.1 secs\n\n\nЗдійснимо прогноз значень:\n\ntrain_results$C5Predicted <- predict(c5_model, train_data_tmp)\ntest_results$C5Predicted <- predict(c5_model, test_data_tmp)\n\nROC-крива та AUROC:\n\nplotROC(as.numeric(test_results$deposit), as.numeric(test_results$C5Predicted))\n# you can see that current algorithm is not very good for this data, partykit is much better\n\n\n\n\nConfusion Matrix:\n\ncm <- caret::confusionMatrix(factor(test_results$deposit), \n                             test_results$C5Predicted, \n                             positive = \"1\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1662  395\n         1  777 1072\n                                          \n               Accuracy : 0.6999          \n                 95% CI : (0.6853, 0.7143)\n    No Information Rate : 0.6244          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.3918          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.7307          \n            Specificity : 0.6814          \n         Pos Pred Value : 0.5798          \n         Neg Pred Value : 0.8080          \n             Prevalence : 0.3756          \n         Detection Rate : 0.2744          \n   Detection Prevalence : 0.4734          \n      Balanced Accuracy : 0.7061          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nЗбалансована точність моделі:\n\nBAcc # rpart\nBAcc1 # partykit\n\n0.655537676754586\n\n\n0.661817945741171\n\n\n\nBAcc2 <- cm$byClass[[\"Balanced Accuracy\"]]\nBAcc2\n# but balanced accuracy is the best. So this model better classify both good and bad events\n\n0.706084913609872"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#randomforest",
    "href": "036-supervised-learning-desicion-trees-classification.html#randomforest",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.9 RandomForest",
    "text": "7.9 RandomForest\nYou can use random forest with default or special training parameters.\n\nhead(train_results)\n\n\n\nA data.frame: 6 × 5\n\n    NodepositRPartPredictedPartyPredictedC5Predicted\n    <int><dbl><dbl><dbl><fct>\n\n\n    3110.22877510.18272430\n    5210.22877510.18272430\n    6310.22877510.27463650\n    11410.22877510.27463650\n    12510.22877510.18272430\n    14610.22877510.27463650\n\n\n\n\n\nhead(train_data)\n\n\n\nA data.frame: 6 × 18\n\n    agejobmaritaleducationdefaultbalancehousingloancontactdaymonthcampaignpdayspreviouspoutcomedepositpdays_flagpoutcoume_success\n    <int><fct><fct><fct><fct><int><fct><fct><fct><int><ord><int><int><int><fct><dbl><dbl><dbl>\n\n\n    341technician marriedsecondaryno1270yesno unknown5may1-10unknown100\n    554admin.     marriedtertiary no 184no no unknown5may2-10unknown100\n    642management single tertiary no   0yesyesunknown5may2-10unknown100\n    1138admin.     single secondaryno 100yesno unknown7may1-10unknown100\n    1230blue-collarmarriedsecondaryno 309yesno unknown7may2-10unknown100\n    1446blue-collarsingle tertiary no 460yesno unknown7may2-10unknown100\n\n\n\n\n\n#install.packages(\"randomForest\")\n\n\ntable(train_data$deposit)\n\n\n   0    1 \n3816 3440 \n\n\n\nsuppressMessages(library(randomForest))\n\nrf_model <- randomForest(deposit ~ ., \n                         data=train_data, \n                         ntree=200, \n                         mtry=2, \n                         importance=TRUE) #Should importance of predictors be assessed?\n\nWarning message in randomForest.default(m, y, ...):\n\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n\n\nntree - Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.\nmtry - Number of variables randomly sampled as candidates at each split.\n\nrf_model\n\n\nCall:\n randomForest(formula = deposit ~ ., data = train_data, ntree = 200,      mtry = 2, importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 200\nNo. of variables tried at each split: 2\n\n          Mean of squared residuals: 0.1909128\n                    % Var explained: 23.43\n\n\nМожемо провести аналіз важливості параметрів у залежності від критерію зменшення точності або зменшення джині:\n\nvarImpPlot(rf_model)\n\n\n\n\n\nMeanDecreaseAccuracy: gives a rough estimate of the loss in prediction performance when that particular variable is omitted from the training set. Caveat: if two variables are somewhat redundant, then omitting one of them may not lead to massive gains in prediction performance, but would make the second variable more important.\nMeanDecreaseGini: GINI is a measure of node impurity. Think of it like this, if you use this feature to split the data, how pure will the nodes be? Highest purity means that each node contains only elements of a single class. Assessing the decrease in GINI when that feature is omitted leads to an understanding of how important that feature is to split the data correctly.\n\n\ntrain_results$RF <- predict(rf_model, train_data)\ntest_results$RF <- predict(rf_model, test_data)\n\n\noptCutOff <- optimalCutoff(train_results$deposit, train_results$RF)\noptCutOff\n\n0.426968327910145\n\n\n\ntest_results$RF_Class = ifelse(test_results$RF > optCutOff, 1, 0)\n\nROC-крива та AUROC:\n\nplotROC(as.numeric(test_results$deposit), as.numeric(test_results$RF))\n\n\n\n\n\n# Balanced accuracy is much better the before!\ncm <- caret::confusionMatrix(factor(test_results$deposit), \n                             factor(test_results$RF_Class), \n                             positive = \"1\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1411  646\n         1  506 1343\n                                          \n               Accuracy : 0.7051          \n                 95% CI : (0.6905, 0.7193)\n    No Information Rate : 0.5092          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.4107          \n                                          \n Mcnemar's Test P-Value : 4.216e-05       \n                                          \n            Sensitivity : 0.6752          \n            Specificity : 0.7360          \n         Pos Pred Value : 0.7263          \n         Neg Pred Value : 0.6860          \n             Prevalence : 0.5092          \n         Detection Rate : 0.3438          \n   Detection Prevalence : 0.4734          \n      Balanced Accuracy : 0.7056          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nBalanced accuracy is hte best for now"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#xgboost",
    "href": "036-supervised-learning-desicion-trees-classification.html#xgboost",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.10 xgBoost",
    "text": "7.10 xgBoost\nOur next step is testing gradient boosting with xgboost algorithm.\n\nsuppressMessages(library(xgboost))\n\nFor complex algorithm like random forest or xgboost model training is the most important stage.\nXGBoost only works with numeric vectors. Therefore, you need to convert all other forms of data into numeric vectors.\n\ntrain_labels <- train_data$deposit\ntest_labels <- test_data$deposit\n\nxgb_train_data <- xgb.DMatrix(data = model.matrix(deposit~., data = train_data),\n                              label = train_labels)\nxgb_test_data <- xgb.DMatrix(data = model.matrix(deposit~., data = test_data),\n                              label = test_labels)\nxgb_test_data\n\nxgb.DMatrix  dim: 3906 x 44  info: label  colnames: yes\n\n\nWe will train decision tree model using the following parameters:\n\nobjective = \"binary:logistic\": we will train a binary classification model ;\nmax.depth = 2: the trees won’t be deep, because our case is very simple ;\nnthread = 2: the number of CPU threads we are going to use;\nnrounds = 2: there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.\n\n\nxgb_model <- xgboost(data = xgb_train_data, \n                     label = train_labels, \n                     max.depth = 2, \n                     #eta = 1, \n                     nthread = 2, \n                     nrounds = 2, \n                     objective = \"binary:logistic\")\nxgb_model\n\nWarning message in xgb.get.DMatrix(data, label, missing, weight, nthread = merged$nthread):\n\"xgboost: label will be ignored.\"\n\n\n[1] train-logloss:0.659975 \n[2] train-logloss:0.635883 \n\n\n##### xgb.Booster\nraw: 5.8 Kb \ncall:\n  xgb.train(params = params, data = dtrain, nrounds = nrounds, \n    watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, \n    early_stopping_rounds = early_stopping_rounds, maximize = maximize, \n    save_period = save_period, save_name = save_name, xgb_model = xgb_model, \n    callbacks = callbacks, max.depth = 2, nthread = 2, objective = \"binary:logistic\")\nparams (as set within xgb.train):\n  max_depth = \"2\", nthread = \"2\", objective = \"binary:logistic\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n  cb.evaluation.log()\n# of features: 44 \nniter: 2\nnfeatures : 44 \nevaluation_log:\n iter train_logloss\n    1     0.6599753\n    2     0.6358826\n\n\n\n# predict data\ntrain_results$XGB <- predict(xgb_model, xgb_train_data)\ntest_results$XGB <- predict(xgb_model, xgb_test_data)\n\nhead(test_results)\n\n\n\nA data.frame: 6 × 10\n\n    NodepositRPartPredictedRPartPredicted_ClassPartyPredictedPartyPredicted_ClassC5PredictedRFRF_ClassXGB\n    <int><dbl><dbl><dbl><dbl><dbl><fct><dbl><dbl><dbl>\n\n\n    1110.228775100.1827243000.237751300.3903929\n    2210.228775100.1827243000.245691000.3903929\n    4310.228775100.1827243000.217355700.3903929\n    7410.228775100.1827243000.237029500.3903929\n    8510.228775100.2746365000.345949800.3903929\n    9610.228775100.1827243000.187250900.3903929\n\n\n\n\nOptimal cutoff:\n\noptCutOff <- optimalCutoff(train_results$deposit, train_results$XGB)\noptCutOff\n\n0.473113248348236\n\n\n\n# evaluate classification class\ntest_results$XGB_Class = ifelse(test_results$XGB > optCutOff, 1, 0)\n\n\nplotROC(as.numeric(test_results$deposit), as.numeric(test_results$XGB))\n\n\n\n\n\n# Balanced accuracy is not better, random forest wins for now!\ncm <- caret::confusionMatrix(factor(test_results$deposit), \n                             factor(test_results$XGB_Class), \n                             positive = \"1\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1777  280\n         1 1065  784\n                                          \n               Accuracy : 0.6557          \n                 95% CI : (0.6405, 0.6706)\n    No Information Rate : 0.7276          \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.2942          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.7368          \n            Specificity : 0.6253          \n         Pos Pred Value : 0.4240          \n         Neg Pred Value : 0.8639          \n             Prevalence : 0.2724          \n         Detection Rate : 0.2007          \n   Detection Prevalence : 0.4734          \n      Balanced Accuracy : 0.6811          \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "036-supervised-learning-desicion-trees-classification.html#lightgbm",
    "href": "036-supervised-learning-desicion-trees-classification.html#lightgbm",
    "title": "7  Дерева рішень. Класифікація. Депозити",
    "section": "7.11 lightgbm",
    "text": "7.11 lightgbm\nLight gbm is one of most useful package for machine learning. It has one super power: speed of calculations. While you using very big datasets randomForest and xgBoost work slow, but lightgbm works better.\nFor this algorithm we should convert our data to special matrices too. So, lets install packages for example:\n\n# ALERT sometimes you need to unistall Matrix in RSTudio and install it again\nsuppressMessages(library(Matrix))                      \nsuppressMessages(library(lightgbm))\n\nLets use binning technique for data preprocessing\n\nsuppressMessages(library(scorecard))\n\n\nvars_list <- train_data %>%\n  select(-deposit) %>%\n  names()\nvars_list\n\n\n'age''job''marital''education''default''balance''housing''loan''contact''day''month''campaign''pdays''previous''poutcome''pdays_flag''poutcoume_success'\n\n\n\nbin_class <- woebin(train_data, \n                    y = \"deposit\", \n                    x = vars_list, \n                    positive = 1, # the value in deposit that indicates event                   \n                    bin_num_limit = 20)\n# bin_class - to check bins\n\n[INFO] creating woe binning ... \n\n\n\ntrain_woe <- woebin_ply(train_data, bin_class)\ntest_woe <- woebin_ply(test_data, bin_class)\nhead(train_woe)\n\n[INFO] converting into woe values ... \n[INFO] converting into woe values ... \n\n\n\n\nA data.table: 6 × 18\n\n    depositage_woejob_woemarital_woeeducation_woedefault_woebalance_woehousing_woeloan_woecontact_woeday_woemonth_woecampaign_woepdays_woeprevious_woepoutcome_woepdays_flag_woepoutcoume_success_woe\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    1-0.1919797 0.18206313-0.1591497-0.11140050 0.34944916-0.4636881 0.08789815-1.11036-0.2931007-0.6621753 0.22554244-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    1-0.1919797-0.02410209-0.1591497 0.26973530-0.34928490 0.4042223 0.08789815-1.11036-0.2931007-0.6621753-0.03160046-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    1-0.1919797 0.17010635 0.2785068 0.26973530-0.34928490-0.4636881-0.61989139-1.11036-0.2931007-0.6621753-0.03160046-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    1-0.1919797-0.02410209 0.2785068-0.11140050-0.34928490-0.4636881 0.08789815-1.11036-0.2931007-0.6621753 0.22554244-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    1-0.1919797-0.47176755-0.1591497-0.11140050-0.05026394-0.4636881 0.08789815-1.11036-0.2931007-0.6621753-0.03160046-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    1-0.1919797-0.47176755 0.2785068 0.26973530-0.05026394-0.4636881 0.08789815-1.11036-0.2931007-0.6621753-0.03160046-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n\n\n\n\n\nvars_list <- train_woe %>%\n  select(-deposit) %>%\n  names() \nvars_list\n\n\n'age_woe''job_woe''marital_woe''education_woe''default_woe''balance_woe''housing_woe''loan_woe''contact_woe''day_woe''month_woe''campaign_woe''pdays_woe''previous_woe''poutcome_woe''pdays_flag_woe''poutcoume_success_woe'\n\n\n\nhead(test_woe %>% select(vars_list))\n\nNote: Using an external vector in selections is ambiguous.\ni Use `all_of(vars_list)` instead of `vars_list` to silence this message.\ni See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis message is displayed once per session.\n\n\n\n\nA data.table: 6 × 17\n\n    age_woejob_woemarital_woeeducation_woedefault_woebalance_woehousing_woeloan_woecontact_woeday_woemonth_woecampaign_woepdays_woeprevious_woepoutcome_woepdays_flag_woepoutcoume_success_woe\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    -0.1919797-0.02410209-0.15914969-0.11140050 0.34944916-0.4636881 0.08789815-1.11036-0.2931007-0.66217530.2255424-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    -0.1919797-0.02410209-0.15914969-0.11140050-0.34928490 0.4042223 0.08789815-1.11036-0.2931007-0.66217530.2255424-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    -0.1919797-0.26465641-0.15914969-0.11140050 0.34944916-0.4636881 0.08789815-1.11036-0.2931007-0.66217530.2255424-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    -0.1919797 0.17010635-0.15914969 0.26973530 0.34944916-0.4636881-0.61989139-1.11036-0.2931007-0.66217530.2255424-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n     1.2441251 0.73838226 0.01571154-0.11140050-0.05026394-0.4636881 0.08789815-1.11036-0.2931007-0.66217530.2255424-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n    -0.1919797 0.18206313-0.15914969-0.11140050-0.34928490-0.4636881 0.08789815-1.11036-0.2931007-0.66217530.2255424-0.2605003-0.2597224-0.2592721-0.2597224-0.1899974\n\n\n\n\n\ntrain_sparse = Matrix(as.matrix(train_woe %>% select(vars_list)), sparse=TRUE)\ntest_sparse = Matrix(as.matrix(test_woe %>% select(vars_list)), sparse=TRUE)\n\n\nlgb.train = lgb.Dataset(data = train_sparse, label = train_woe$deposit, free_raw_data = FALSE)\nlgb.test = lgb.Dataset(data = test_sparse, label = test_woe$deposit, free_raw_data = FALSE)\n\n\nlgb.grid = list(objective = \"binary\",\n                metric = \"auc\",\n                #save_binary = T,\n                max_bin = 32,\n                num_leaves = 33)\n\n\nlgb.train.cv = lgb.train(params = lgb.grid,\n                         data = lgb.train,                         \n                         nrounds = 15,                         \n                         early_stopping_round = 300,\n                         #categorical_feature = categoricals.vec,\n                         valids = list(test = lgb.test),\n                         verbose = 1) \n\n[LightGBM] [Info] Number of positive: 3440, number of negative: 3816\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020781 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 7256, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.474090 -> initscore=-0.103731\n[LightGBM] [Info] Start training from score -0.103731\n[1] \"[1]:  test's auc:0.759236\"\n[1] \"[2]:  test's auc:0.760231\"\n[1] \"[3]:  test's auc:0.763489\"\n[1] \"[4]:  test's auc:0.763607\"\n[1] \"[5]:  test's auc:0.763681\"\n[1] \"[6]:  test's auc:0.763814\"\n[1] \"[7]:  test's auc:0.762936\"\n[1] \"[8]:  test's auc:0.765679\"\n[1] \"[9]:  test's auc:0.766939\"\n[1] \"[10]:  test's auc:0.767712\"\n[1] \"[11]:  test's auc:0.768105\"\n[1] \"[12]:  test's auc:0.768932\"\n[1] \"[13]:  test's auc:0.769262\"\n[1] \"[14]:  test's auc:0.769862\"\n[1] \"[15]:  test's auc:0.770532\"\n\n\n\n# predict data\ntrain_results$LGBM <- predict(lgb.train.cv, train_sparse)\ntest_results$LGBM <- predict(lgb.train.cv, test_sparse)\n\nhead(test_results)\n\n\n\nA data.frame: 6 × 12\n\n    NodepositRPartPredictedRPartPredicted_ClassPartyPredictedPartyPredicted_ClassC5PredictedRFRF_ClassXGBXGB_ClassLGBM\n    <int><dbl><dbl><dbl><dbl><dbl><fct><dbl><dbl><dbl><dbl><dbl>\n\n\n    1110.228775100.1827243000.237751300.390392900.2076138\n    2210.228775100.1827243000.245691000.390392900.1978679\n    4310.228775100.1827243000.217355700.390392900.2076138\n    7410.228775100.1827243000.237029500.390392900.2076138\n    8510.228775100.2746365000.345949800.390392900.2508506\n    9610.228775100.1827243000.187250900.390392900.1978679\n\n\n\n\n\n# Optimal cutoff:\n\noptCutOff <- optimalCutoff(train_results$deposit, train_results$LGBM)\noptCutOff\n\n0.489317324236863\n\n\n\n# evaluate classification class\ntest_results$LGBM_Class = ifelse(test_results$LGBM > optCutOff, 1, 0)\nplotROC(as.numeric(test_results$deposit), as.numeric(test_results$LGBM))\n\n\n\n\n\n# Balanced accuracy is not better, random forest wins for now!\ncm <- caret::confusionMatrix(factor(test_results$deposit), \n                             factor(test_results$LGBM_Class), \n                             positive = \"1\")\ncm\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1667  390\n         1  741 1108\n                                          \n               Accuracy : 0.7104          \n                 95% CI : (0.6959, 0.7246)\n    No Information Rate : 0.6165          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.4136          \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.7397          \n            Specificity : 0.6923          \n         Pos Pred Value : 0.5992          \n         Neg Pred Value : 0.8104          \n             Prevalence : 0.3835          \n         Detection Rate : 0.2837          \n   Detection Prevalence : 0.4734          \n      Balanced Accuracy : 0.7160          \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "037-supervised-learning-neural-network-classification.html",
    "href": "037-supervised-learning-neural-network-classification.html",
    "title": "8  Нейронні мережі. Deep Learning. Класифікація. Кредитний скоринг",
    "section": "",
    "text": "Курс: “Математичне моделювання в R”"
  },
  {
    "objectID": "037-supervised-learning-neural-network-classification.html#набір-даних",
    "href": "037-supervised-learning-neural-network-classification.html#набір-даних",
    "title": "8  Нейронні мережі. Deep Learning. Класифікація. Кредитний скоринг",
    "section": "8.1 Набір даних",
    "text": "8.1 Набір даних\nДжерело: https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling.\nЗавантажимо файли з даними:\n\ncustomers <- read.csv(\"data/customers.csv\")\nproducts <- read.csv(\"data/products.csv\")\ncards <- read.csv(\"data/cards.csv\")\n\nЦей датасет містить такі стовпці:\n\nRowNumber - номер рядка.\nCustomerId - ідентифікатор клієнта.\nSurname - прізвище клієнта.\nCreditScore - кредитний рейтинг клієнта.\nGeography - регіон.\nGender - стать.\nAge - вік.\nTenure - час обслуговування цього клієнта в банку.\nIsActiveMember - активний клієнт, виконує операції.\nEstimatedSalary - заробітна плата.\nExited - залишив/не залишив банк.\n\nІнформація про клієнтів:\n\nstr(customers)\n\n'data.frame':   10000 obs. of  11 variables:\n $ RowNumber      : int  1 2 3 4 5 6 7 8 9 10 ...\n $ CustomerId     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...\n $ Surname        : chr  \"Hargrave\" \"Hill\" \"Onio\" \"Boni\" ...\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : chr  \"France\" \"Spain\" \"France\" \"France\" ...\n $ Gender         : chr  \"Female\" \"Female\" \"Female\" \"Female\" ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n\n\n\nhead(customers)\n\n\n\nA data.frame: 6 × 11\n\n    RowNumberCustomerIdSurnameCreditScoreGeographyGenderAgeTenureIsActiveMemberEstimatedSalaryExited\n    <int><int><chr><int><chr><chr><int><int><int><dbl><int>\n\n\n    1115634602Hargrave619FranceFemale4221101348.881\n    2215647311Hill    608Spain Female4111112542.580\n    3315619304Onio    502FranceFemale4280113931.571\n    4415701354Boni    699FranceFemale3910 93826.630\n    5515737888Mitchell850Spain Female4321 79084.100\n    6615574012Chu     645Spain Male  4480149756.711"
  },
  {
    "objectID": "037-supervised-learning-neural-network-classification.html#оглядовий-аналіз-даних",
    "href": "037-supervised-learning-neural-network-classification.html#оглядовий-аналіз-даних",
    "title": "8  Нейронні мережі. Deep Learning. Класифікація. Кредитний скоринг",
    "section": "8.2 Оглядовий аналіз даних",
    "text": "8.2 Оглядовий аналіз даних\n\n8.2.1 Таблиця customers\nОглянемо дані колекції customers візуально та за допомогою крос-таблиць:\n\nlibrary(gmodels)\nlibrary(ggplot2)\n\nКредитний рейтинг (CreditScore):\n\nggplot(customers, aes(x=CreditScore, fill=factor(Exited))) + \n  geom_histogram(binwidth = 10, alpha=0.7) + theme_bw() \n\n\n\n\nРегіон/країна (Geography):\n\nggplot(customers, aes(x=Geography, fill=factor(Exited))) + \n  geom_bar(position = \"stack\") + theme_bw()\n\n\n\n\n\nCrossTable(customers$Geography, customers$Exited)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  10000 \n\n \n                    | customers$Exited \ncustomers$Geography |         0 |         1 | Row Total | \n--------------------|-----------|-----------|-----------|\n             France |      4204 |       810 |      5014 | \n                    |    11.188 |    43.736 |           | \n                    |     0.838 |     0.162 |     0.501 | \n                    |     0.528 |     0.398 |           | \n                    |     0.420 |     0.081 |           | \n--------------------|-----------|-----------|-----------|\n            Germany |      1695 |       814 |      2509 | \n                    |    45.927 |   179.537 |           | \n                    |     0.676 |     0.324 |     0.251 | \n                    |     0.213 |     0.400 |           | \n                    |     0.170 |     0.081 |           | \n--------------------|-----------|-----------|-----------|\n              Spain |      2064 |       413 |      2477 | \n                    |     4.251 |    16.617 |           | \n                    |     0.833 |     0.167 |     0.248 | \n                    |     0.259 |     0.203 |           | \n                    |     0.206 |     0.041 |           | \n--------------------|-----------|-----------|-----------|\n       Column Total |      7963 |      2037 |     10000 | \n                    |     0.796 |     0.204 |           | \n--------------------|-----------|-----------|-----------|\n\n \n\n\nСтать (Gender):\n\nggplot(customers, aes(x=Gender, fill=factor(Exited))) + \n  geom_bar(position = \"stack\") + theme_bw()\n\n\n\n\n\nCrossTable(customers$Gender, customers$Exited)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  10000 \n\n \n                 | customers$Exited \ncustomers$Gender |         0 |         1 | Row Total | \n-----------------|-----------|-----------|-----------|\n          Female |      3404 |      1139 |      4543 | \n                 |    12.611 |    49.298 |           | \n                 |     0.749 |     0.251 |     0.454 | \n                 |     0.427 |     0.559 |           | \n                 |     0.340 |     0.114 |           | \n-----------------|-----------|-----------|-----------|\n            Male |      4559 |       898 |      5457 | \n                 |    10.499 |    41.041 |           | \n                 |     0.835 |     0.165 |     0.546 | \n                 |     0.573 |     0.441 |           | \n                 |     0.456 |     0.090 |           | \n-----------------|-----------|-----------|-----------|\n    Column Total |      7963 |      2037 |     10000 | \n                 |     0.796 |     0.204 |           | \n-----------------|-----------|-----------|-----------|\n\n \n\n\nВік (Age):\n\nggplot(customers, aes(x=Age, fill=factor(Exited))) + \n  geom_histogram(binwidth = 1, alpha=0.7) + theme_bw() \n\n\n\n\nЧас обслуговування клієнта (Tenure):\n\nggplot(customers, aes(x=Tenure, fill=factor(Exited))) + \n  geom_histogram(binwidth = 1, alpha=0.7) + theme_bw() \n\n\n\n\nАктивність (IsActiveMember):\n\nCrossTable(customers$IsActiveMember, customers$Exited)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n| Chi-square contribution |\n|           N / Row Total |\n|           N / Col Total |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  10000 \n\n \n                         | customers$Exited \ncustomers$IsActiveMember |         0 |         1 | Row Total | \n-------------------------|-----------|-----------|-----------|\n                       0 |      3547 |      1302 |      4849 | \n                         |    25.577 |    99.984 |           | \n                         |     0.731 |     0.269 |     0.485 | \n                         |     0.445 |     0.639 |           | \n                         |     0.355 |     0.130 |           | \n-------------------------|-----------|-----------|-----------|\n                       1 |      4416 |       735 |      5151 | \n                         |    24.077 |    94.122 |           | \n                         |     0.857 |     0.143 |     0.515 | \n                         |     0.555 |     0.361 |           | \n                         |     0.442 |     0.073 |           | \n-------------------------|-----------|-----------|-----------|\n            Column Total |      7963 |      2037 |     10000 | \n                         |     0.796 |     0.204 |           | \n-------------------------|-----------|-----------|-----------|\n\n \n\n\nЗаробітна плата (EstimatedSalary):\n\nggplot(customers, aes(x=EstimatedSalary, fill=factor(Exited))) + \n  geom_histogram(binwidth = 1000, alpha=0.7) + theme_bw() \n\n\n\n\nExited:\n\nCrossTable(customers$Exited)\n\n\n \n   Cell Contents\n|-------------------------|\n|                       N |\n|         N / Table Total |\n|-------------------------|\n\n \nTotal Observations in Table:  10000 \n\n \n          |         0 |         1 | \n          |-----------|-----------|\n          |      7963 |      2037 | \n          |     0.796 |     0.204 | \n          |-----------|-----------|\n\n\n\n \n\n\n\n\n\n8.2.2 Таблиця cards\nІнформація про карти клієнта:\n\nstr(cards)\n\n'data.frame':   13545 obs. of  4 variables:\n $ CustomerId  : int  15634602 15647311 15647311 15619304 15619304 15701354 15701354 15737888 15737888 15574012 ...\n $ CardNo      : int  684618 357092 802678 888594 987103 507476 928960 370210 935036 581042 ...\n $ IsCreditCard: int  1 0 0 1 0 0 0 1 0 1 ...\n $ Balance     : num  5 41922 41913 79846 79859 ...\n\n\n\nhead(cards)\n\n\n\nA data.frame: 6 × 4\n\n    CustomerIdCardNoIsCreditCardBalance\n    <int><int><int><dbl>\n\n\n    1156346026846181    5.00\n    215647311357092041921.93\n    315647311802678041912.93\n    415619304888594179846.40\n    515619304987103079859.40\n    6157013545074760   28.00\n\n\n\n\n\nsummary(cards)\n\n   CustomerId           CardNo        IsCreditCard       Balance      \n Min.   :15565701   Min.   :100231   Min.   :0.0000   Min.   :     0  \n 1st Qu.:15628272   1st Qu.:323587   1st Qu.:0.0000   1st Qu.:    24  \n Median :15691011   Median :545073   Median :0.0000   Median : 52991  \n Mean   :15690848   Mean   :548414   Mean   :0.4688   Mean   : 51096  \n 3rd Qu.:15752816   3rd Qu.:774943   3rd Qu.:1.0000   3rd Qu.: 77276  \n Max.   :15815690   Max.   :999985   Max.   :1.0000   Max.   :221549  \n\n\n\n\n\n8.2.3 Таблиця products\nІнформація про продукти клієнта:\n\nstr(products)\n\n'data.frame':   13862 obs. of  2 variables:\n $ CustomerId : int  15634602 15647311 15619304 15619304 15619304 15701354 15701354 15737888 15574012 15574012 ...\n $ ProductName: chr  \"PROD_1\" \"PROD_1\" \"PROD_1\" \"PROD_2\" ...\n\n\n\nhead(products)\n\n\n\nA data.frame: 6 × 2\n\n    CustomerIdProductName\n    <int><chr>\n\n\n    115634602PROD_1\n    215647311PROD_1\n    315619304PROD_1\n    415619304PROD_2\n    515619304PROD_3\n    615701354PROD_1"
  },
  {
    "objectID": "037-supervised-learning-neural-network-classification.html#feature-engeniering",
    "href": "037-supervised-learning-neural-network-classification.html#feature-engeniering",
    "title": "8  Нейронні мережі. Deep Learning. Класифікація. Кредитний скоринг",
    "section": "8.3 Feature engeniering",
    "text": "8.3 Feature engeniering\nСформуємо додаткові змінні на основі наявних даних:\n\nBalance - сума по усіх картах клієнат.\nNumOfProducts - кількість продуктів банку, які використовує клієнт.\nHasCreditCard - dummy-змінна, наявність кредитної карти у клієнта.\n\n\n8.3.1 Показник NumOfProducts\nВарто звернути увагу, що генерувати нові фічі можна із використанням можлиовстей агрегації та фільтрування даних (наприклад, методи пакету dplyr) або за допомогою простих алгоритмічних структур (цикли, розгалуження).\nДля R використання циклів для подібних задач є не досить хорошим рішенням, адже код виглядає складно і виконується повільно. Проте ми напишемо для требування і пояснення приклади з використанням обох підходів.\nІмперативний підхід до програмування:\n\n# Створюємо пусту змінну\ncustomers$NumOfProducts <- c(0)\n\nfor(i in 1:nrow(customers))\n{\n  id <- customers$CustomerId[i]\n  prods <- subset(products, CustomerId == id)\n  customers$NumOfProducts[i] <- nrow(prods)\n}\n\nhead(customers, 4)\n\n\n\nA data.frame: 4 × 12\n\n    RowNumberCustomerIdSurnameCreditScoreGeographyGenderAgeTenureIsActiveMemberEstimatedSalaryExitedNumOfProducts\n    <int><int><chr><int><chr><chr><int><int><int><dbl><int><dbl>\n\n\n    1115634602Hargrave619FranceFemale4221101348.8811\n    2215647311Hill    608Spain Female4111112542.5801\n    3315619304Onio    502FranceFemale4280113931.5713\n    4415701354Boni    699FranceFemale3910 93826.6302\n\n\n\n\nДекларативний приклад коду:\n\nsuppressMessages(library(dplyr))\n\ncustomers_tmp <- customers |>\n    left_join(products |> \n              group_by(CustomerId) |> \n              mutate(NumOfProducts = n()) |>\n              select(CustomerId, NumOfProducts) |> distinct(), by = \"CustomerId\")\n\nhead(customers_tmp, 4)\n\n# P.S. Це набагато швидше\n\n\n\nA data.frame: 4 × 13\n\n    RowNumberCustomerIdSurnameCreditScoreGeographyGenderAgeTenureIsActiveMemberEstimatedSalaryExitedNumOfProducts.xNumOfProducts.y\n    <int><int><chr><int><chr><chr><int><int><int><dbl><int><dbl><int>\n\n\n    1115634602Hargrave619FranceFemale4221101348.88111\n    2215647311Hill    608Spain Female4111112542.58011\n    3315619304Onio    502FranceFemale4280113931.57133\n    4415701354Boni    699FranceFemale3910 93826.63022\n\n\n\n\n\n\n\n8.3.2 Показник HasCreditCard\n\ncustomers <- customers |>\n    left_join(cards |> \n              group_by(CustomerId) |> \n              summarise(HasCreditCard = ifelse(sum(IsCreditCard) == 0, 0, 1)) |>\n              select(CustomerId, HasCreditCard) |> distinct(), by = \"CustomerId\")\n\nhead(customers, 4)\n\n\n\nA data.frame: 4 × 13\n\n    RowNumberCustomerIdSurnameCreditScoreGeographyGenderAgeTenureIsActiveMemberEstimatedSalaryExitedNumOfProductsHasCreditCard\n    <int><int><chr><int><chr><chr><int><int><int><dbl><int><dbl><dbl>\n\n\n    1115634602Hargrave619FranceFemale4221101348.88111\n    2215647311Hill    608Spain Female4111112542.58010\n    3315619304Onio    502FranceFemale4280113931.57131\n    4415701354Boni    699FranceFemale3910 93826.63020\n\n\n\n\n\n\n\n8.3.3 Показник Balance\n\ncustomers <- customers |>\n    left_join(cards |> \n              group_by(CustomerId) |> \n              mutate(Balance = round(sum(Balance))) |>\n              select(CustomerId, Balance) |> distinct(), by = \"CustomerId\")\n\nhead(customers, 4)\n\n\n\nA data.frame: 4 × 14\n\n    RowNumberCustomerIdSurnameCreditScoreGeographyGenderAgeTenureIsActiveMemberEstimatedSalaryExitedNumOfProductsHasCreditCardBalance\n    <int><int><chr><int><chr><chr><int><int><int><dbl><int><dbl><dbl><dbl>\n\n\n    1115634602Hargrave619FranceFemale4221101348.88111     5\n    2215647311Hill    608Spain Female4111112542.58010 83835\n    3315619304Onio    502FranceFemale4280113931.57131159706\n    4415701354Boni    699FranceFemale3910 93826.63020    47\n\n\n\n\nЗвичано код можна поєднати в 1 запит.\n\n\n8.3.4 Нормалізація даних\nВидалимо зайві покзники, що заважатимуть будувати моделі:\n\ncustomers <- customers |>\n    dplyr::select(-c(RowNumber, CustomerId, Surname))\n\nВарто звернути увагу, що інженереія нових фіч завершена не до кінця і у нас ще присутні параметри, що мають нечислові значення Geography + Gender. Спробуйте замінити їх, наприклад, на dummy-змінні або використати бінінг.\nДля скорочення часу на вивчення матеріали ми скористаємося звичайним приведенням даних до числового типу:\n\ncustomers$Geography <- as.numeric(as.factor(customers$Geography))\ncustomers$Gender <- as.numeric(as.factor(customers$Gender))\n\n\nstr(customers)\n\n'data.frame':   10000 obs. of  11 variables:\n $ CreditScore    : int  619 608 502 699 850 645 822 376 501 684 ...\n $ Geography      : num  1 3 1 1 3 3 1 2 1 1 ...\n $ Gender         : num  1 1 1 1 1 2 2 1 2 2 ...\n $ Age            : int  42 41 42 39 43 44 50 29 44 27 ...\n $ Tenure         : int  2 1 8 1 2 8 7 4 4 2 ...\n $ IsActiveMember : int  1 1 0 0 1 0 1 0 1 1 ...\n $ EstimatedSalary: num  101349 112543 113932 93827 79084 ...\n $ Exited         : int  1 0 1 0 0 1 0 1 0 0 ...\n $ NumOfProducts  : num  1 1 3 2 1 2 2 4 2 1 ...\n $ HasCreditCard  : num  1 0 1 0 1 1 1 1 0 1 ...\n $ Balance        : num  5 83835 159706 47 125571 ...\n\n\nВикористовуючи функцію scale() нормалізуємо дані:\n\nscaled <- scale(customers[-8], center = TRUE)\nscaled <- cbind(scaled, customers[8])\nhead(scaled,)\n\n\n\nA data.frame: 6 × 11\n\n    CreditScoreGeographyGenderAgeTenureIsActiveMemberEstimatedSalaryNumOfProductsHasCreditCardBalanceExited\n    <dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><dbl><int>\n\n\n    1-0.32620511-0.9018411-1.09593270.293502747-1.041708 0.970194 0.0218854-0.5420116 0.6521559-1.22401031\n    2-0.44001395 1.5149916-1.09593270.198153924-1.387468 0.970194 0.2165229-0.5420116-1.5332062 0.11795070\n    3-1.53671734-0.9018411-1.09593270.293502747 1.032856-1.030619 0.2406749 2.2648841 0.6521559 1.33250301\n    4 0.50149556-0.9018411-1.09593270.007456278-1.387468-1.030619-0.1089125 0.8614363-1.5332062-1.22333800\n    5 2.06378057 1.5149916-1.09593270.388851570-1.041708 0.970194-0.3652575-0.5420116 0.6521559 0.78606570\n    6-0.05720239 1.5149916 0.91237350.484200392 1.032856-1.030619 0.8636071 0.8614363 0.6521559 0.59742601"
  },
  {
    "objectID": "037-supervised-learning-neural-network-classification.html#формування-вибірок",
    "href": "037-supervised-learning-neural-network-classification.html#формування-вибірок",
    "title": "8  Нейронні мережі. Deep Learning. Класифікація. Кредитний скоринг",
    "section": "8.4 Формування вибірок",
    "text": "8.4 Формування вибірок\nРозділимо вибірку на тестову та тренувальну за допомогою пакету caTools та функції sample.split():\n\nlibrary(caTools)\nset.seed(2022)\nsplit <- sample.split(scaled$Exited, SplitRatio = 0.7)\ntrain_data <- subset(scaled, split == TRUE)\ntest_data <- subset(scaled, split == FALSE)"
  },
  {
    "objectID": "037-supervised-learning-neural-network-classification.html#побудова-deep-learning-моделі",
    "href": "037-supervised-learning-neural-network-classification.html#побудова-deep-learning-моделі",
    "title": "8  Нейронні мережі. Deep Learning. Класифікація. Кредитний скоринг",
    "section": "8.5 Побудова Deep Learning моделі",
    "text": "8.5 Побудова Deep Learning моделі\nВикосритаємо можливості пакету h2o для побудови моделі на основі deep learning. Підключимо пакет.\n\n# install.packages(\"h2o\")\n\n\nsuppressMessages(library(h2o))\n\n\n\n\n\n\n\nWarning\n\n\n\nУвага! Для запуску пакету потрібна віртуальна машина Java на ПК (JVM). Завантажити актуальну версію Java можна з сайту https://www.java.com/en/download/.\n\n\nЗапустимо двигун h2o:\n\n# Увага, це специфічні налаштуваняя для ПК на якому налагоджувався проєкт\nSys.setenv(JAVA_HOME = \"C:/Program Files/Java/jdk-19/\")\nprint(Sys.getenv(\"JAVA_HOME\"))\n\n[1] \"C:/Program Files/Java/jdk-19/\"\n\n\n\nh2o.init()\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    D:\\Temp\\RtmpWcfQIa\\file86d022e97546/h2o_yura_started_from_r.out\n    D:\\Temp\\RtmpWcfQIa\\file86d01affe0a/h2o_yura_started_from_r.err\n\n\nStarting H2O JVM and connecting: . Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         4 seconds 650 milliseconds \n    H2O cluster timezone:       Europe/Kiev \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.38.0.1 \n    H2O cluster version age:    14 days, 4 hours and 46 minutes  \n    H2O cluster name:           H2O_started_from_R_yura_svm422 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.95 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.1.3 (2022-03-10) \n\n\n\nПобудуємо математичну модель:\n\nh2o_model <- h2o.deeplearning(y = 'Exited', \n                              training_frame = as.h2o(train_data),\n                              activation = \"Rectifier\",\n                              hidden = c(6,6),\n                              epochs = 100)\n\n  |======================================================================| 100%\n  |======================================================================| 100%\n\n\nВарто перегляднути набір параметрів, що може приймати функція h2o.deeplearning(), адже вона досить складна:\n\n# help(h2o.deeplearning) # можна переглянути параметри методу\n\nЗдійснимо прогноз на тестовій вибірці (h2o.predict()), а також класифікуємо значення за cutOff = 0.5:\n\nh2o_predict_prob <- h2o.predict(h2o_model, newdata = as.h2o(test_data[-(ncol(test_data))]))\n\n  |======================================================================| 100%\n  |======================================================================| 100%\n\n\n\nh2o_predict_class <- ifelse(h2o_predict_prob > 0.5, 1 , 0)\nh2o_predict_class <- as.vector(h2o_predict_class)\n\nПобудуємо матрицю Confusion Matrix:\n\nsuppressMessages(library(caret))\ncaret::confusionMatrix(factor(test_data$Exited), factor(h2o_predict_class), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2285  104\n         1  343  268\n                                          \n               Accuracy : 0.851           \n                 95% CI : (0.8377, 0.8636)\n    No Information Rate : 0.876           \n    P-Value [Acc > NIR] : 1               \n                                          \n                  Kappa : 0.4624          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.72043         \n            Specificity : 0.86948         \n         Pos Pred Value : 0.43863         \n         Neg Pred Value : 0.95647         \n             Prevalence : 0.12400         \n         Detection Rate : 0.08933         \n   Detection Prevalence : 0.20367         \n      Balanced Accuracy : 0.79496         \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nПобудуємо ROC-криву:\n\nlibrary(InformationValue)\nInformationValue::plotROC(test_data$Exited, as.vector(h2o_predict_prob))\n\n\nAttaching package: 'InformationValue'\n\n\nThe following objects are masked from 'package:caret':\n\n    confusionMatrix, precision, sensitivity, specificity\n\n\n\n\n\n\n\nПобудуємо модель з більшою кількістю прихованих шарів та нейронів:\n\nh2o_model2 <- h2o.deeplearning(y = 'Exited', \n                              training_frame = as.h2o(train_data),\n                              activation = \"Rectifier\",\n                              hidden = c(10,10),\n                              epochs = 100)\n\n  |======================================================================| 100%\n  |======================================================================| 100%\n\n\nЗдійснимо прогноз:\n\nh2o_predict_prob2 <- h2o.predict(h2o_model2, newdata = as.h2o(test_data[-(ncol(test_data))]))\n\n  |======================================================================| 100%\n  |======================================================================| 100%\n\n\n\nh2o_predict_class2 <- ifelse(h2o_predict_prob2 > 0.5, 1 , 0)\nh2o_predict_class2 <- as.vector(h2o_predict_class2)\n\nConfiusion Matrix:\n\ncaret::confusionMatrix(factor(test_data$Exited), factor(h2o_predict_class2), positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 2302   87\n         1  343  268\n                                         \n               Accuracy : 0.8567         \n                 95% CI : (0.8436, 0.869)\n    No Information Rate : 0.8817         \n    P-Value [Acc > NIR] : 1              \n                                         \n                  Kappa : 0.4765         \n                                         \n Mcnemar's Test P-Value : <2e-16         \n                                         \n            Sensitivity : 0.75493        \n            Specificity : 0.87032        \n         Pos Pred Value : 0.43863        \n         Neg Pred Value : 0.96358        \n             Prevalence : 0.11833        \n         Detection Rate : 0.08933        \n   Detection Prevalence : 0.20367        \n      Balanced Accuracy : 0.81263        \n                                         \n       'Positive' Class : 1              \n                                         \n\n\nROC-крива:\n\nInformationValue::plotROC(test_data$Exited, as.vector(h2o_predict_prob2))\n\n\n\n\nЗупинимо двигун h2o:\n\nh2o.shutdown(prompt = F)"
  }
]